{\rtf1\ansi\ansicpg1252\deff0\deflang1033{\fonttbl{\f0\fmodern Consolas;}{\f1\fmodern\fcharset0 Consolas;}{\f2\fnil\fcharset129 Courier New;}}
{\colortbl ;\red20\green20\blue20;\red142\green142\blue142;\red16\green158\blue98;\red148\green146\blue12;\red170\green51\blue170;\red56\green136\blue159;\red170\green64\blue64;\red85\green85\blue85;\red170\green85\blue85;\red136\green136\blue136;\red9\green118\blue72;\red18\green124\blue155;}
\viewkind4\uc1\pard\f0\fs20     \u9484?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9488?\cf1\highlight2 
\par \cf0\highlight0     \u9474?                 \cf3\f1\bullet  MobaXterm Personal Edition v21.4 \bullet\cf0\f0                  \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?               \cf4 (SSH client, X server and network tools)\cf0                \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?                                                                      \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474? \u8594? SSH session to \cf5 root@192.168.1.210                        \cf0           \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?\f1    \bullet  Direct SSH      :  \cf3\f0 v\cf0                                              \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?\f1    \bullet  SSH compression :  \cf3\f0 v\cf0                                              \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?\f1    \bullet  SSH-browser     :  \cf3\f0 v\cf0                                              \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?\f1    \bullet  X11-forwarding  :  \cf3\f0 v\cf0   (remote display is forwarded through SSH)  \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?                                                                      \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474? \u8594? For more info, ctrl+click on \cf6\ul help\cf0\ulnone  or visit our \cf6\ul website\cf0\ulnone .            \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9492?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9496?\cf1\highlight2 
\par 
\par \cf0\highlight0 Welcome to Ubuntu 18.04.5 LTS (GNU/Linux 4.15.0-159-generic x86_64)\cf1\highlight2 
\par 
\par \cf0\highlight0  * Documentation:  https://help.ubuntu.com\cf1\highlight2 
\par \cf0\highlight0  * Management:     https://landscape.canonical.com\cf1\highlight2 
\par \cf0\highlight0  * Support:        https://ubuntu.com/advantage\cf1\highlight2 
\par 
\par \cf0\highlight0   System information as of Tue Dec 21 09:47:47 UTC 2021\cf1\highlight2 
\par 
\par \cf0\highlight0   System load:  1.07              Processes:             334\cf1\highlight2 
\par \cf0\highlight0   Usage of /:   8.1% of 72.83GB   Users logged in:       0\cf1\highlight2 
\par \cf0\highlight0   Memory usage: 2%                IP address for ens160: 192.168.1.210\cf1\highlight2 
\par \cf0\highlight0   Swap usage:   0%\cf1\highlight2 
\par 
\par 
\par \cf0\highlight0 169 packages can be updated.\cf1\highlight2 
\par \cf0\highlight0 124 updates are security updates.\cf1\highlight2 
\par 
\par 
\par \cf0\highlight0 Last login: Thu Oct  7 06:21:48 2021\cf1\highlight2 
\par \cf0\highlight0 root@master:~# vi /etc/hostname\cf1\highlight2 
\par \cf0\highlight0 root@master:~# vi /etc/hosts\cf1\highlight2 
\par \cf0\highlight0 root@master:~# init 6\cf1\highlight2 
\par 
\par \cf7\highlight0 Remote side unexpectedly closed network connection\cf1\highlight2 
\par 
\par \cf8\highlight0\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\cf1\highlight2 
\par 
\par \cf7\highlight0 Session stopped\cf1\highlight2 
\par \cf0\highlight0     - Press \cf5 <return>\cf0  to exit tab\cf1\highlight2 
\par \cf0\highlight0     - Press \cf5 R\cf0  to restart session\cf1\highlight2 
\par \cf0\highlight0     - Press \cf5 S\cf0  to save terminal output to file\cf1\highlight2 
\par 
\par \cf7\highlight0 Network error: Connection refused\cf1\highlight2 
\par 
\par \cf8\highlight0\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\cf1\highlight2 
\par 
\par \cf7\highlight0 Session stopped\cf1\highlight2 
\par \cf0\highlight0     - Press \cf5 <return>\cf0  to exit tab\cf1\highlight2 
\par \cf0\highlight0     - Press \cf5 R\cf0  to restart session\cf1\highlight2 
\par \cf0\highlight0     - Press \cf5 S\cf0  to save terminal output to file\cf1\highlight2 
\par \cf0\highlight0     \u9484?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9488?\cf1\highlight2 
\par \cf0\highlight0     \u9474?                 \cf3\f1\bullet  MobaXterm Personal Edition v21.4 \bullet\cf0\f0                  \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?               \cf4 (SSH client, X server and network tools)\cf0                \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?                                                                      \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474? \u8594? SSH session to \cf5 root@192.168.1.210                        \cf0           \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?\f1    \bullet  Direct SSH      :  \cf3\f0 v\cf0                                              \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?\f1    \bullet  SSH compression :  \cf3\f0 v\cf0                                              \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?\f1    \bullet  SSH-browser     :  \cf3\f0 v\cf0                                              \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?\f1    \bullet  X11-forwarding  :  \cf3\f0 v\cf0   (remote display is forwarded through SSH)  \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?                                                                      \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474? \u8594? For more info, ctrl+click on \cf6\ul help\cf0\ulnone  or visit our \cf6\ul website\cf0\ulnone .            \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9492?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9496?\cf1\highlight2 
\par 
\par \cf0\highlight0 Welcome to Ubuntu 18.04.5 LTS (GNU/Linux 4.15.0-159-generic x86_64)\cf1\highlight2 
\par 
\par \cf0\highlight0  * Documentation:  https://help.ubuntu.com\cf1\highlight2 
\par \cf0\highlight0  * Management:     https://landscape.canonical.com\cf1\highlight2 
\par \cf0\highlight0  * Support:        https://ubuntu.com/advantage\cf1\highlight2 
\par 
\par \cf0\highlight0   System information as of Tue Dec 21 09:53:38 UTC 2021\cf1\highlight2 
\par 
\par \cf0\highlight0   System load:  0.62              Processes:             329\cf1\highlight2 
\par \cf0\highlight0   Usage of /:   8.2% of 72.83GB   Users logged in:       0\cf1\highlight2 
\par \cf0\highlight0   Memory usage: 1%                IP address for ens160: 192.168.1.210\cf1\highlight2 
\par \cf0\highlight0   Swap usage:   0%\cf1\highlight2 
\par 
\par 
\par \cf0\highlight0 169 packages can be updated.\cf1\highlight2 
\par \cf0\highlight0 124 updates are security updates.\cf1\highlight2 
\par 
\par \cf0\highlight0 New release '20.04.3 LTS' available.\cf1\highlight2 
\par \cf0\highlight0 Run 'do-release-upgrade' to upgrade to it.\cf1\highlight2 
\par 
\par 
\par \cf0\highlight0 Last login: Tue Dec 21 09:47:50 2021 from 192.168.1.1\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# sudo apt -y update && sudo apt -y install make && sudo apt -y install gcc && sudo apt -y install jq\cf1\highlight2 
\par \cf0\highlight0 Hit:1 http://in.archive.ubuntu.com/ubuntu bionic InRelease\cf1\highlight2 
\par \cf0\highlight0 Hit:2 http://in.archive.ubuntu.com/ubuntu bionic-updates InRelease\cf1\highlight2 
\par \cf0\highlight0 Hit:3 http://in.archive.ubuntu.com/ubuntu bionic-backports InRelease\cf1\highlight2 
\par \cf0\highlight0 Hit:4 http://in.archive.ubuntu.com/ubuntu bionic-security InRelease\cf1\highlight2 
\par \cf0\highlight0 Reading package lists... Done\cf1\highlight2 
\par \cf0\highlight0 Building dependency tree\cf1\highlight2 
\par \cf0\highlight0 Reading state information... Done\cf1\highlight2 
\par \cf0\highlight0 160 packages can be upgraded. Run 'apt list --upgradable' to see them.\cf1\highlight2 
\par \cf0\highlight0 Reading package lists... Done\cf1\highlight2 
\par \cf0\highlight0 Building dependency tree\cf1\highlight2 
\par \cf0\highlight0 Reading state information... Done\cf1\highlight2 
\par \cf0\highlight0 Suggested packages:\cf1\highlight2 
\par \cf0\highlight0   make-doc\cf1\highlight2 
\par \cf0\highlight0 The following NEW packages will be installed:\cf1\highlight2 
\par \cf0\highlight0   make\cf1\highlight2 
\par \cf0\highlight0 0 upgraded, 1 newly installed, 0 to remove and 160 not upgraded.\cf1\highlight2 
\par \cf0\highlight0 Need to get 154 kB of archives.\cf1\highlight2 
\par \cf0\highlight0 After this operation, 381 kB of additional disk space will be used.\cf1\highlight2 
\par \cf0\highlight0 Get:1 http://in.archive.ubuntu.com/ubuntu bionic/main amd64 make amd64 4.1-9.1ubuntu1 [154 kB]\cf1\highlight2 
\par \cf0\highlight0 Fetched 154 kB in 0s (691 kB/s)\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package make.\cf1\highlight2 
\par \cf0\highlight0 (Reading database ... 67138 files and directories currently installed.)\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../make_4.1-9.1ubuntu1_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking make (4.1-9.1ubuntu1) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up make (4.1-9.1ubuntu1) ...\cf1\highlight2 
\par \cf0\highlight0 Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\cf1\highlight2 
\par \cf0\highlight0 Reading package lists... Done\cf1\highlight2 
\par \cf0\highlight0 Building dependency tree\cf1\highlight2 
\par \cf0\highlight0 Reading state information... Done\cf1\highlight2 
\par \cf0\highlight0 The following additional packages will be installed:\cf1\highlight2 
\par \cf0\highlight0   binutils binutils-common binutils-x86-64-linux-gnu cpp cpp-7 gcc-7 gcc-7-base libasan4 libatomic1 libbinutils libc-dev-bin libc6 libc6-dev libcc1-0 libcilkrts5\cf1\highlight2 
\par \cf0\highlight0   libgcc-7-dev libgomp1 libisl19 libitm1 liblsan0 libmpc3 libmpx2 libquadmath0 libtsan0 libubsan0 linux-libc-dev manpages-dev\cf1\highlight2 
\par \cf0\highlight0 Suggested packages:\cf1\highlight2 
\par \cf0\highlight0   binutils-doc cpp-doc gcc-7-locales gcc-multilib autoconf automake libtool flex bison gdb gcc-doc gcc-7-multilib gcc-7-doc libgcc1-dbg libgomp1-dbg libitm1-dbg\cf1\highlight2 
\par \cf0\highlight0   libatomic1-dbg libasan4-dbg liblsan0-dbg libtsan0-dbg libubsan0-dbg libcilkrts5-dbg libmpx2-dbg libquadmath0-dbg glibc-doc\cf1\highlight2 
\par \cf0\highlight0 The following NEW packages will be installed:\cf1\highlight2 
\par \cf0\highlight0   binutils binutils-common binutils-x86-64-linux-gnu cpp cpp-7 gcc gcc-7 gcc-7-base libasan4 libatomic1 libbinutils libc-dev-bin libc6-dev libcc1-0 libcilkrts5\cf1\highlight2 
\par \cf0\highlight0   libgcc-7-dev libgomp1 libisl19 libitm1 liblsan0 libmpc3 libmpx2 libquadmath0 libtsan0 libubsan0 linux-libc-dev manpages-dev\cf1\highlight2 
\par \cf0\highlight0 The following packages will be upgraded:\cf1\highlight2 
\par \cf0\highlight0   libc6\cf1\highlight2 
\par \cf0\highlight0 1 upgraded, 27 newly installed, 0 to remove and 159 not upgraded.\cf1\highlight2 
\par \cf0\highlight0 Need to get 33.5 MB of archives.\cf1\highlight2 
\par \cf0\highlight0 After this operation, 118 MB of additional disk space will be used.\cf1\highlight2 
\par \cf0\highlight0 Get:1 http://in.archive.ubuntu.com/ubuntu bionic-updates/main amd64 libc6 amd64 2.27-3ubuntu1.4 [2,832 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:2 http://in.archive.ubuntu.com/ubuntu bionic-updates/main amd64 binutils-common amd64 2.30-21ubuntu1~18.04.7 [197 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:3 http://in.archive.ubuntu.com/ubuntu bionic-updates/main amd64 libbinutils amd64 2.30-21ubuntu1~18.04.7 [489 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:4 http://in.archive.ubuntu.com/ubuntu bionic-updates/main amd64 binutils-x86-64-linux-gnu amd64 2.30-21ubuntu1~18.04.7 [1,839 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:5 http://in.archive.ubuntu.com/ubuntu bionic-updates/main amd64 binutils amd64 2.30-21ubuntu1~18.04.7 [3,388 B]\cf1\highlight2 
\par \cf0\highlight0 Get:6 http://in.archive.ubuntu.com/ubuntu bionic-updates/main amd64 gcc-7-base amd64 7.5.0-3ubuntu1~18.04 [18.3 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:7 http://in.archive.ubuntu.com/ubuntu bionic/main amd64 libisl19 amd64 0.19-1 [551 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:8 http://in.archive.ubuntu.com/ubuntu bionic/main amd64 libmpc3 amd64 1.1.0-1 [40.8 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:9 http://in.archive.ubuntu.com/ubuntu bionic-updates/main amd64 cpp-7 amd64 7.5.0-3ubuntu1~18.04 [8,591 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:10 http://in.archive.ubuntu.com/ubuntu bionic-updates/main amd64 cpp amd64 4:7.4.0-1ubuntu2.3 [27.7 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:11 http://in.archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcc1-0 amd64 8.4.0-1ubuntu1~18.04 [39.4 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:12 http://in.archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgomp1 amd64 8.4.0-1ubuntu1~18.04 [76.5 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:13 http://in.archive.ubuntu.com/ubuntu bionic-updates/main amd64 libitm1 amd64 8.4.0-1ubuntu1~18.04 [27.9 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:14 http://in.archive.ubuntu.com/ubuntu bionic-updates/main amd64 libatomic1 amd64 8.4.0-1ubuntu1~18.04 [9,192 B]\cf1\highlight2 
\par \cf0\highlight0 Get:15 http://in.archive.ubuntu.com/ubuntu bionic-updates/main amd64 libasan4 amd64 7.5.0-3ubuntu1~18.04 [358 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:16 http://in.archive.ubuntu.com/ubuntu bionic-updates/main amd64 liblsan0 amd64 8.4.0-1ubuntu1~18.04 [133 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:17 http://in.archive.ubuntu.com/ubuntu bionic-updates/main amd64 libtsan0 amd64 8.4.0-1ubuntu1~18.04 [288 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:18 http://in.archive.ubuntu.com/ubuntu bionic-updates/main amd64 libubsan0 amd64 7.5.0-3ubuntu1~18.04 [126 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:19 http://in.archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcilkrts5 amd64 7.5.0-3ubuntu1~18.04 [42.5 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:20 http://in.archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmpx2 amd64 8.4.0-1ubuntu1~18.04 [11.6 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:21 http://in.archive.ubuntu.com/ubuntu bionic-updates/main amd64 libquadmath0 amd64 8.4.0-1ubuntu1~18.04 [134 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:22 http://in.archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgcc-7-dev amd64 7.5.0-3ubuntu1~18.04 [2,378 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:23 http://in.archive.ubuntu.com/ubuntu bionic-updates/main amd64 gcc-7 amd64 7.5.0-3ubuntu1~18.04 [9,381 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:24 http://in.archive.ubuntu.com/ubuntu bionic-updates/main amd64 gcc amd64 4:7.4.0-1ubuntu2.3 [5,184 B]\cf1\highlight2 
\par \cf0\highlight0 Get:25 http://in.archive.ubuntu.com/ubuntu bionic-updates/main amd64 libc-dev-bin amd64 2.27-3ubuntu1.4 [71.8 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:26 http://in.archive.ubuntu.com/ubuntu bionic-updates/main amd64 linux-libc-dev amd64 4.15.0-163.171 [994 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:27 http://in.archive.ubuntu.com/ubuntu bionic-updates/main amd64 libc6-dev amd64 2.27-3ubuntu1.4 [2,585 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:28 http://in.archive.ubuntu.com/ubuntu bionic/main amd64 manpages-dev all 4.15-1 [2,217 kB]\cf1\highlight2 
\par \cf0\highlight0 Fetched 33.5 MB in 4s (9,156 kB/s)\cf1\highlight2 
\par \cf0\highlight0 Preconfiguring packages ...\cf1\highlight2 
\par \cf0\highlight0 (Reading database ... 67154 files and directories currently installed.)\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../libc6_2.27-3ubuntu1.4_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking libc6:amd64 (2.27-3ubuntu1.4) over (2.27-3ubuntu1.2) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up libc6:amd64 (2.27-3ubuntu1.4) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package binutils-common:amd64.\cf1\highlight2 
\par \cf0\highlight0 (Reading database ... 67154 files and directories currently installed.)\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../00-binutils-common_2.30-21ubuntu1~18.04.7_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking binutils-common:amd64 (2.30-21ubuntu1~18.04.7) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package libbinutils:amd64.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../01-libbinutils_2.30-21ubuntu1~18.04.7_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking libbinutils:amd64 (2.30-21ubuntu1~18.04.7) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package binutils-x86-64-linux-gnu.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../02-binutils-x86-64-linux-gnu_2.30-21ubuntu1~18.04.7_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking binutils-x86-64-linux-gnu (2.30-21ubuntu1~18.04.7) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package binutils.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../03-binutils_2.30-21ubuntu1~18.04.7_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking binutils (2.30-21ubuntu1~18.04.7) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package gcc-7-base:amd64.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../04-gcc-7-base_7.5.0-3ubuntu1~18.04_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking gcc-7-base:amd64 (7.5.0-3ubuntu1~18.04) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package libisl19:amd64.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../05-libisl19_0.19-1_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking libisl19:amd64 (0.19-1) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package libmpc3:amd64.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../06-libmpc3_1.1.0-1_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking libmpc3:amd64 (1.1.0-1) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package cpp-7.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../07-cpp-7_7.5.0-3ubuntu1~18.04_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking cpp-7 (7.5.0-3ubuntu1~18.04) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package cpp.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../08-cpp_4%3a7.4.0-1ubuntu2.3_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking cpp (4:7.4.0-1ubuntu2.3) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package libcc1-0:amd64.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../09-libcc1-0_8.4.0-1ubuntu1~18.04_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking libcc1-0:amd64 (8.4.0-1ubuntu1~18.04) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package libgomp1:amd64.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../10-libgomp1_8.4.0-1ubuntu1~18.04_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking libgomp1:amd64 (8.4.0-1ubuntu1~18.04) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package libitm1:amd64.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../11-libitm1_8.4.0-1ubuntu1~18.04_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking libitm1:amd64 (8.4.0-1ubuntu1~18.04) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package libatomic1:amd64.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../12-libatomic1_8.4.0-1ubuntu1~18.04_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking libatomic1:amd64 (8.4.0-1ubuntu1~18.04) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package libasan4:amd64.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../13-libasan4_7.5.0-3ubuntu1~18.04_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking libasan4:amd64 (7.5.0-3ubuntu1~18.04) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package liblsan0:amd64.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../14-liblsan0_8.4.0-1ubuntu1~18.04_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking liblsan0:amd64 (8.4.0-1ubuntu1~18.04) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package libtsan0:amd64.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../15-libtsan0_8.4.0-1ubuntu1~18.04_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking libtsan0:amd64 (8.4.0-1ubuntu1~18.04) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package libubsan0:amd64.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../16-libubsan0_7.5.0-3ubuntu1~18.04_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking libubsan0:amd64 (7.5.0-3ubuntu1~18.04) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package libcilkrts5:amd64.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../17-libcilkrts5_7.5.0-3ubuntu1~18.04_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking libcilkrts5:amd64 (7.5.0-3ubuntu1~18.04) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package libmpx2:amd64.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../18-libmpx2_8.4.0-1ubuntu1~18.04_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking libmpx2:amd64 (8.4.0-1ubuntu1~18.04) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package libquadmath0:amd64.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../19-libquadmath0_8.4.0-1ubuntu1~18.04_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking libquadmath0:amd64 (8.4.0-1ubuntu1~18.04) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package libgcc-7-dev:amd64.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../20-libgcc-7-dev_7.5.0-3ubuntu1~18.04_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking libgcc-7-dev:amd64 (7.5.0-3ubuntu1~18.04) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package gcc-7.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../21-gcc-7_7.5.0-3ubuntu1~18.04_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking gcc-7 (7.5.0-3ubuntu1~18.04) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package gcc.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../22-gcc_4%3a7.4.0-1ubuntu2.3_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking gcc (4:7.4.0-1ubuntu2.3) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package libc-dev-bin.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../23-libc-dev-bin_2.27-3ubuntu1.4_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking libc-dev-bin (2.27-3ubuntu1.4) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package linux-libc-dev:amd64.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../24-linux-libc-dev_4.15.0-163.171_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking linux-libc-dev:amd64 (4.15.0-163.171) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package libc6-dev:amd64.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../25-libc6-dev_2.27-3ubuntu1.4_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking libc6-dev:amd64 (2.27-3ubuntu1.4) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package manpages-dev.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../26-manpages-dev_4.15-1_all.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking manpages-dev (4.15-1) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up libquadmath0:amd64 (8.4.0-1ubuntu1~18.04) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up libgomp1:amd64 (8.4.0-1ubuntu1~18.04) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up libatomic1:amd64 (8.4.0-1ubuntu1~18.04) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up libcc1-0:amd64 (8.4.0-1ubuntu1~18.04) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up libtsan0:amd64 (8.4.0-1ubuntu1~18.04) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up linux-libc-dev:amd64 (4.15.0-163.171) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up liblsan0:amd64 (8.4.0-1ubuntu1~18.04) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up gcc-7-base:amd64 (7.5.0-3ubuntu1~18.04) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up binutils-common:amd64 (2.30-21ubuntu1~18.04.7) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up libmpx2:amd64 (8.4.0-1ubuntu1~18.04) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up libmpc3:amd64 (1.1.0-1) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up libc-dev-bin (2.27-3ubuntu1.4) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up manpages-dev (4.15-1) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up libc6-dev:amd64 (2.27-3ubuntu1.4) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up libitm1:amd64 (8.4.0-1ubuntu1~18.04) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up libisl19:amd64 (0.19-1) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up libasan4:amd64 (7.5.0-3ubuntu1~18.04) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up libbinutils:amd64 (2.30-21ubuntu1~18.04.7) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up libcilkrts5:amd64 (7.5.0-3ubuntu1~18.04) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up libubsan0:amd64 (7.5.0-3ubuntu1~18.04) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up libgcc-7-dev:amd64 (7.5.0-3ubuntu1~18.04) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up cpp-7 (7.5.0-3ubuntu1~18.04) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up binutils-x86-64-linux-gnu (2.30-21ubuntu1~18.04.7) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up cpp (4:7.4.0-1ubuntu2.3) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up binutils (2.30-21ubuntu1~18.04.7) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up gcc-7 (7.5.0-3ubuntu1~18.04) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up gcc (4:7.4.0-1ubuntu2.3) ...\cf1\highlight2 
\par \cf0\highlight0 Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\cf1\highlight2 
\par \cf0\highlight0 Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\cf1\highlight2 
\par \cf0\highlight0 Reading package lists... Done\cf1\highlight2 
\par \cf0\highlight0 Building dependency tree\cf1\highlight2 
\par \cf0\highlight0 Reading state information... Done\cf1\highlight2 
\par \cf0\highlight0 The following additional packages will be installed:\cf1\highlight2 
\par \cf0\highlight0   libjq1 libonig4\cf1\highlight2 
\par \cf0\highlight0 The following NEW packages will be installed:\cf1\highlight2 
\par \cf0\highlight0   jq libjq1 libonig4\cf1\highlight2 
\par \cf0\highlight0 0 upgraded, 3 newly installed, 0 to remove and 159 not upgraded.\cf1\highlight2 
\par \cf0\highlight0 Need to get 276 kB of archives.\cf1\highlight2 
\par \cf0\highlight0 After this operation, 930 kB of additional disk space will be used.\cf1\highlight2 
\par \cf0\highlight0 Get:1 http://in.archive.ubuntu.com/ubuntu bionic/universe amd64 libonig4 amd64 6.7.0-1 [119 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:2 http://in.archive.ubuntu.com/ubuntu bionic/universe amd64 libjq1 amd64 1.5+dfsg-2 [111 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:3 http://in.archive.ubuntu.com/ubuntu bionic/universe amd64 jq amd64 1.5+dfsg-2 [45.6 kB]\cf1\highlight2 
\par \cf0\highlight0 Fetched 276 kB in 1s (480 kB/s)\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package libonig4:amd64.\cf1\highlight2 
\par \cf0\highlight0 (Reading database ... 71335 files and directories currently installed.)\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../libonig4_6.7.0-1_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking libonig4:amd64 (6.7.0-1) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package libjq1:amd64.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../libjq1_1.5+dfsg-2_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking libjq1:amd64 (1.5+dfsg-2) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package jq.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../jq_1.5+dfsg-2_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking jq (1.5+dfsg-2) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up libonig4:amd64 (6.7.0-1) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up libjq1:amd64 (1.5+dfsg-2) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up jq (1.5+dfsg-2) ...\cf1\highlight2 
\par \cf0\highlight0 Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\cf1\highlight2 
\par \cf0\highlight0 Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# sudo modprobe br_netfilter\cf1\highlight2 
\par \cf0\highlight0 cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf\cf1\highlight2 
\par \cf0\highlight0 net.bridge.bridge-nf-call-ip6tables = 1\cf1\highlight2 
\par \cf0\highlight0 net.bridge.bridge-nf-call-iptables = 1\cf1\highlight2 
\par \cf0\highlight0 EOF\cf1\highlight2 
\par \cf0\highlight0 sudo sysctl --systemroot@node-d:~# lsmod | grep br_netfilter\cf1\highlight2 
\par \cf9\highlight0 br_netfilter\cf0            24576  0\cf1\highlight2 
\par \cf0\highlight0 bridge                155648  1 \cf9 br_netfilter\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~#\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf\cf1\highlight2 
\par \cf0\highlight0 > br_netfilter\cf1\highlight2 
\par \cf0\highlight0 > EOF\cf1\highlight2 
\par \cf0\highlight0 br_netfilter\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~#\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf\cf1\highlight2 
\par \cf0\highlight0 > net.bridge.bridge-nf-call-ip6tables = 1\cf1\highlight2 
\par \cf0\highlight0 > net.bridge.bridge-nf-call-iptables = 1\cf1\highlight2 
\par \cf0\highlight0 > EOF\cf1\highlight2 
\par \cf0\highlight0 net.bridge.bridge-nf-call-ip6tables = 1\cf1\highlight2 
\par \cf0\highlight0 net.bridge.bridge-nf-call-iptables = 1\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# sudo sysctl --system\cf1\highlight2 
\par \cf0\highlight0 * Applying /etc/sysctl.d/10-console-messages.conf ...\cf1\highlight2 
\par \cf0\highlight0 kernel.printk = 4 4 1 7\cf1\highlight2 
\par \cf0\highlight0 * Applying /etc/sysctl.d/10-ipv6-privacy.conf ...\cf1\highlight2 
\par \cf0\highlight0 net.ipv6.conf.all.use_tempaddr = 2\cf1\highlight2 
\par \cf0\highlight0 net.ipv6.conf.default.use_tempaddr = 2\cf1\highlight2 
\par \cf0\highlight0 * Applying /etc/sysctl.d/10-kernel-hardening.conf ...\cf1\highlight2 
\par \cf0\highlight0 kernel.kptr_restrict = 1\cf1\highlight2 
\par \cf0\highlight0 * Applying /etc/sysctl.d/10-link-restrictions.conf ...\cf1\highlight2 
\par \cf0\highlight0 fs.protected_hardlinks = 1\cf1\highlight2 
\par \cf0\highlight0 fs.protected_symlinks = 1\cf1\highlight2 
\par \cf0\highlight0 * Applying /etc/sysctl.d/10-lxd-inotify.conf ...\cf1\highlight2 
\par \cf0\highlight0 fs.inotify.max_user_instances = 1024\cf1\highlight2 
\par \cf0\highlight0 * Applying /etc/sysctl.d/10-magic-sysrq.conf ...\cf1\highlight2 
\par \cf0\highlight0 kernel.sysrq = 176\cf1\highlight2 
\par \cf0\highlight0 * Applying /etc/sysctl.d/10-network-security.conf ...\cf1\highlight2 
\par \cf0\highlight0 net.ipv4.conf.default.rp_filter = 1\cf1\highlight2 
\par \cf0\highlight0 net.ipv4.conf.all.rp_filter = 1\cf1\highlight2 
\par \cf0\highlight0 net.ipv4.tcp_syncookies = 1\cf1\highlight2 
\par \cf0\highlight0 * Applying /etc/sysctl.d/10-ptrace.conf ...\cf1\highlight2 
\par \cf0\highlight0 kernel.yama.ptrace_scope = 1\cf1\highlight2 
\par \cf0\highlight0 * Applying /etc/sysctl.d/10-zeropage.conf ...\cf1\highlight2 
\par \cf0\highlight0 vm.mmap_min_addr = 65536\cf1\highlight2 
\par \cf0\highlight0 * Applying /usr/lib/sysctl.d/50-default.conf ...\cf1\highlight2 
\par \cf0\highlight0 net.ipv4.conf.all.promote_secondaries = 1\cf1\highlight2 
\par \cf0\highlight0 net.core.default_qdisc = fq_codel\cf1\highlight2 
\par \cf0\highlight0 * Applying /etc/sysctl.d/99-sysctl.conf ...\cf1\highlight2 
\par \cf0\highlight0 * Applying /etc/sysctl.d/k8s.conf ...\cf1\highlight2 
\par \cf0\highlight0 net.bridge.bridge-nf-call-ip6tables = 1\cf1\highlight2 
\par \cf0\highlight0 net.bridge.bridge-nf-call-iptables = 1\cf1\highlight2 
\par \cf0\highlight0 * Applying /etc/sysctl.conf ...\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# lsmod | grep br_netfilter\cf1\highlight2 
\par \cf9\highlight0 br_netfilter\cf0            24576  0\cf1\highlight2 
\par \cf0\highlight0 bridge                155648  1 \cf9 br_netfilter\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# sudo apt-get update\cf1\highlight2 
\par \cf0\highlight0 Hit:1 http://in.archive.ubuntu.com/ubuntu bionic InRelease\cf1\highlight2 
\par \cf0\highlight0 Hit:2 http://in.archive.ubuntu.com/ubuntu bionic-updates InRelease\cf1\highlight2 
\par \cf0\highlight0 Hit:3 http://in.archive.ubuntu.com/ubuntu bionic-backports InRelease\cf1\highlight2 
\par \cf0\highlight0 Hit:4 http://in.archive.ubuntu.com/ubuntu bionic-security InRelease\cf1\highlight2 
\par \cf0\highlight0 Reading package lists... Done\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# sudo apt-get install docker.io\cf1\highlight2 
\par \cf0\highlight0 Reading package lists... Done\cf1\highlight2 
\par \cf0\highlight0 Building dependency tree\cf1\highlight2 
\par \cf0\highlight0 Reading state information... Done\cf1\highlight2 
\par \cf0\highlight0 The following additional packages will be installed:\cf1\highlight2 
\par \cf0\highlight0   bridge-utils containerd pigz runc ubuntu-fan\cf1\highlight2 
\par \cf0\highlight0 Suggested packages:\cf1\highlight2 
\par \cf0\highlight0   ifupdown aufs-tools cgroupfs-mount | cgroup-lite debootstrap docker-doc rinse zfs-fuse | zfsutils\cf1\highlight2 
\par \cf0\highlight0 The following NEW packages will be installed:\cf1\highlight2 
\par \cf0\highlight0   bridge-utils containerd docker.io pigz runc ubuntu-fan\cf1\highlight2 
\par \cf0\highlight0 0 upgraded, 6 newly installed, 0 to remove and 159 not upgraded.\cf1\highlight2 
\par \cf0\highlight0 Need to get 74.2 MB of archives.\cf1\highlight2 
\par \cf0\highlight0 After this operation, 360 MB of additional disk space will be used.\cf1\highlight2 
\par \cf0\highlight0 Do you want to continue? [Y/n] y\cf1\highlight2 
\par \cf0\highlight0 Get:1 http://in.archive.ubuntu.com/ubuntu bionic/universe amd64 pigz amd64 2.4-1 [57.4 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:2 http://in.archive.ubuntu.com/ubuntu bionic/main amd64 bridge-utils amd64 1.5-15ubuntu1 [30.1 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:3 http://in.archive.ubuntu.com/ubuntu bionic-updates/universe amd64 runc amd64 1.0.1-0ubuntu2~18.04.1 [4,155 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:4 http://in.archive.ubuntu.com/ubuntu bionic-updates/universe amd64 containerd amd64 1.5.5-0ubuntu3~18.04.1 [33.0 MB]\cf1\highlight2 
\par \cf0\highlight0 Get:5 http://in.archive.ubuntu.com/ubuntu bionic-updates/universe amd64 docker.io amd64 20.10.7-0ubuntu5~18.04.3 [36.9 MB]\cf1\highlight2 
\par \cf0\highlight0 Get:6 http://in.archive.ubuntu.com/ubuntu bionic/main amd64 ubuntu-fan all 0.12.10 [34.7 kB]\cf1\highlight2 
\par \cf0\highlight0 Fetched 74.2 MB in 9s (7,894 kB/s)\cf1\highlight2 
\par \cf0\highlight0 Preconfiguring packages ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package pigz.\cf1\highlight2 
\par \cf0\highlight0 (Reading database ... 71352 files and directories currently installed.)\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../0-pigz_2.4-1_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking pigz (2.4-1) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package bridge-utils.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../1-bridge-utils_1.5-15ubuntu1_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking bridge-utils (1.5-15ubuntu1) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package runc.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../2-runc_1.0.1-0ubuntu2~18.04.1_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking runc (1.0.1-0ubuntu2~18.04.1) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package containerd.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../3-containerd_1.5.5-0ubuntu3~18.04.1_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking containerd (1.5.5-0ubuntu3~18.04.1) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package docker.io.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../4-docker.io_20.10.7-0ubuntu5~18.04.3_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking docker.io (20.10.7-0ubuntu5~18.04.3) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package ubuntu-fan.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../5-ubuntu-fan_0.12.10_all.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking ubuntu-fan (0.12.10) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up runc (1.0.1-0ubuntu2~18.04.1) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up containerd (1.5.5-0ubuntu3~18.04.1) ...\cf1\highlight2 
\par \cf0\highlight0 Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service \u8594? /lib/systemd/system/containerd.service.\cf1\highlight2 
\par \cf0\highlight0 /usr/sbin/policy-rc.d returned 101, not running 'start containerd.service'\cf1\highlight2 
\par \cf0\highlight0 Setting up bridge-utils (1.5-15ubuntu1) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up ubuntu-fan (0.12.10) ...\cf1\highlight2 
\par \cf0\highlight0 Created symlink /etc/systemd/system/multi-user.target.wants/ubuntu-fan.service \u8594? /lib/systemd/system/ubuntu-fan.service.\cf1\highlight2 
\par \cf0\highlight0 invoke-rc.d: policy-rc.d denied execution of start.\cf1\highlight2 
\par \cf0\highlight0 Setting up pigz (2.4-1) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up docker.io (20.10.7-0ubuntu5~18.04.3) ...\cf1\highlight2 
\par \cf0\highlight0 Adding group `docker' (GID 113) ...\cf1\highlight2 
\par \cf0\highlight0 Done.\cf1\highlight2 
\par \cf0\highlight0 Created symlink /etc/systemd/system/multi-user.target.wants/docker.service \u8594? /lib/systemd/system/docker.service.\cf1\highlight2 
\par \cf0\highlight0 Created symlink /etc/systemd/system/sockets.target.wants/docker.socket \u8594? /lib/systemd/system/docker.socket.\cf1\highlight2 
\par \cf0\highlight0 invoke-rc.d: policy-rc.d denied execution of start.\cf1\highlight2 
\par \cf0\highlight0 Processing triggers for systemd (237-3ubuntu10.42) ...\cf1\highlight2 
\par \cf0\highlight0 Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\cf1\highlight2 
\par \cf0\highlight0 Processing triggers for ureadahead (0.100.0-21) ...\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# sudo apt-get update\cf1\highlight2 
\par \cf0\highlight0 Hit:1 http://in.archive.ubuntu.com/ubuntu bionic InRelease\cf1\highlight2 
\par \cf0\highlight0 Hit:2 http://in.archive.ubuntu.com/ubuntu bionic-updates InRelease\cf1\highlight2 
\par \cf0\highlight0 Hit:3 http://in.archive.ubuntu.com/ubuntu bionic-backports InRelease\cf1\highlight2 
\par \cf0\highlight0 Hit:4 http://in.archive.ubuntu.com/ubuntu bionic-security InRelease\cf1\highlight2 
\par \cf0\highlight0 Reading package lists... Done\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# sudo apt-get install -y apt-transport-https ca-certificates curl\cf1\highlight2 
\par \cf0\highlight0 Reading package lists... Done\cf1\highlight2 
\par \cf0\highlight0 Building dependency tree\cf1\highlight2 
\par \cf0\highlight0 Reading state information... Done\cf1\highlight2 
\par \cf0\highlight0 The following additional packages will be installed:\cf1\highlight2 
\par \cf0\highlight0   libcurl4\cf1\highlight2 
\par \cf0\highlight0 The following NEW packages will be installed:\cf1\highlight2 
\par \cf0\highlight0   apt-transport-https\cf1\highlight2 
\par \cf0\highlight0 The following packages will be upgraded:\cf1\highlight2 
\par \cf0\highlight0   ca-certificates curl libcurl4\cf1\highlight2 
\par \cf0\highlight0 3 upgraded, 1 newly installed, 0 to remove and 156 not upgraded.\cf1\highlight2 
\par \cf0\highlight0 Need to get 4,348 B/528 kB of archives.\cf1\highlight2 
\par \cf0\highlight0 After this operation, 165 kB of additional disk space will be used.\cf1\highlight2 
\par \cf0\highlight0 Get:1 http://in.archive.ubuntu.com/ubuntu bionic-updates/universe amd64 apt-transport-https all 1.6.14 [4,348 B]\cf1\highlight2 
\par \cf0\highlight0 Fetched 4,348 B in 0s (46.7 kB/s)\cf1\highlight2 
\par \cf0\highlight0 Preconfiguring packages ...\cf1\highlight2 
\par \cf0\highlight0 (Reading database ... 71675 files and directories currently installed.)\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../ca-certificates_20210119~18.04.2_all.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking ca-certificates (20210119~18.04.2) over (20190110~18.04.1) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package apt-transport-https.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../apt-transport-https_1.6.14_all.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking apt-transport-https (1.6.14) ...\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../curl_7.58.0-2ubuntu3.16_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking curl (7.58.0-2ubuntu3.16) over (7.58.0-2ubuntu3.9) ...\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../libcurl4_7.58.0-2ubuntu3.16_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking libcurl4:amd64 (7.58.0-2ubuntu3.16) over (7.58.0-2ubuntu3.9) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up apt-transport-https (1.6.14) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up libcurl4:amd64 (7.58.0-2ubuntu3.16) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up ca-certificates (20210119~18.04.2) ...\cf1\highlight2 
\par \cf0\highlight0 Updating certificates in /etc/ssl/certs...\cf1\highlight2 
\par \cf0\highlight0 21 added, 20 removed; done.\cf1\highlight2 
\par \cf0\highlight0 Setting up curl (7.58.0-2ubuntu3.16) ...\cf1\highlight2 
\par \cf0\highlight0 Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\cf1\highlight2 
\par \cf0\highlight0 Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\cf1\highlight2 
\par \cf0\highlight0 Processing triggers for ca-certificates (20210119~18.04.2) ...\cf1\highlight2 
\par \cf0\highlight0 Updating certificates in /etc/ssl/certs...\cf1\highlight2 
\par \cf0\highlight0 0 added, 0 removed; done.\cf1\highlight2 
\par \cf0\highlight0 Running hooks in /etc/ca-certificates/update.d...\cf1\highlight2 
\par \cf0\highlight0 done.\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources      .list.d/kubernetes.list\cf1\highlight2 
\par \cf0\highlight0 deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# sudo apt-get update\cf1\highlight2 
\par \cf0\highlight0 Hit:1 http://in.archive.ubuntu.com/ubuntu bionic InRelease\cf1\highlight2 
\par \cf0\highlight0 Hit:2 http://in.archive.ubuntu.com/ubuntu bionic-updates InRelease\cf1\highlight2 
\par \cf0\highlight0 Hit:3 http://in.archive.ubuntu.com/ubuntu bionic-backports InRelease\cf1\highlight2 
\par \cf0\highlight0 Hit:4 http://in.archive.ubuntu.com/ubuntu bionic-security InRelease\cf1\highlight2 
\par \cf0\highlight0 Get:5 https://packages.cloud.google.com/apt kubernetes-xenial InRelease [9,383 B]\cf1\highlight2 
\par \cf0\highlight0 Get:6 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 Packages [52.6 kB]\cf1\highlight2 
\par \cf0\highlight0 Fetched 62.0 kB in 2s (32.6 kB/s)\cf1\highlight2 
\par \cf0\highlight0 Reading package lists... Done\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# apt-get install -qy kubelet=1.21.1-00 kubectl=1.21.1-00 kubeadm=1.21.1-00\cf1\highlight2 
\par \cf0\highlight0 Reading package lists...\cf1\highlight2 
\par \cf0\highlight0 Building dependency tree...\cf1\highlight2 
\par \cf0\highlight0 Reading state information...\cf1\highlight2 
\par \cf0\highlight0 The following additional packages will be installed:\cf1\highlight2 
\par \cf0\highlight0   conntrack cri-tools kubernetes-cni socat\cf1\highlight2 
\par \cf0\highlight0 The following NEW packages will be installed:\cf1\highlight2 
\par \cf0\highlight0   conntrack cri-tools kubeadm kubectl kubelet kubernetes-cni socat\cf1\highlight2 
\par \cf0\highlight0 0 upgraded, 7 newly installed, 0 to remove and 156 not upgraded.\cf1\highlight2 
\par \cf0\highlight0 Need to get 73.5 MB of archives.\cf1\highlight2 
\par \cf0\highlight0 After this operation, 316 MB of additional disk space will be used.\cf1\highlight2 
\par \cf0\highlight0 Get:1 http://in.archive.ubuntu.com/ubuntu bionic/main amd64 conntrack amd64 1:1.4.4+snapshot20161117-6ubuntu2 [30.6 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:2 http://in.archive.ubuntu.com/ubuntu bionic/main amd64 socat amd64 1.7.3.2-2ubuntu2 [342 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:3 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 cri-tools amd64 1.19.0-00 [11.2 MB]\cf1\highlight2 
\par \cf0\highlight0 Get:4 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubernetes-cni amd64 0.8.7-00 [25.0 MB]\cf1\highlight2 
\par \cf0\highlight0 Get:5 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubelet amd64 1.21.1-00 [18.8 MB]\cf1\highlight2 
\par \cf0\highlight0 Get:6 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubectl amd64 1.21.1-00 [9,225 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:7 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubeadm amd64 1.21.1-00 [8,985 kB]\cf1\highlight2 
\par \cf0\highlight0 Fetched 73.5 MB in 20s (3,628 kB/s)\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package conntrack.\cf1\highlight2 
\par \cf0\highlight0 (Reading database ... 71680 files and directories currently installed.)\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../0-conntrack_1%3a1.4.4+snapshot20161117-6ubuntu2_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking conntrack (1:1.4.4+snapshot20161117-6ubuntu2) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package cri-tools.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../1-cri-tools_1.19.0-00_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking cri-tools (1.19.0-00) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package kubernetes-cni.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../2-kubernetes-cni_0.8.7-00_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking kubernetes-cni (0.8.7-00) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package socat.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../3-socat_1.7.3.2-2ubuntu2_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking socat (1.7.3.2-2ubuntu2) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package kubelet.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../4-kubelet_1.21.1-00_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking kubelet (1.21.1-00) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package kubectl.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../5-kubectl_1.21.1-00_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking kubectl (1.21.1-00) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package kubeadm.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../6-kubeadm_1.21.1-00_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking kubeadm (1.21.1-00) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up conntrack (1:1.4.4+snapshot20161117-6ubuntu2) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up kubernetes-cni (0.8.7-00) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up cri-tools (1.19.0-00) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up socat (1.7.3.2-2ubuntu2) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up kubelet (1.21.1-00) ...\cf1\highlight2 
\par \cf0\highlight0 Created symlink /etc/systemd/system/multi-user.target.wants/kubelet.service \u8594? /lib/systemd/system/kubelet.service.\cf1\highlight2 
\par \cf0\highlight0 /usr/sbin/policy-rc.d returned 101, not running 'start kubelet.service'\cf1\highlight2 
\par \cf0\highlight0 Setting up kubectl (1.21.1-00) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up kubeadm (1.21.1-00) ...\cf1\highlight2 
\par \cf0\highlight0 Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# kubeadm init\cf1\highlight2 
\par \cf0\highlight0 W1221 10:21:34.740524   14827 kubelet.go:210] cannot automatically set CgroupDriver when starting the Kubelet: cannot execute 'docker info -f \{\{.CgroupDriver\}\}': exit       status 2\cf1\highlight2 
\par \cf0\highlight0 I1221 10:21:35.956718   14827 version.go:254] remote version is much newer: v1.23.1; falling back to: stable-1.21\cf1\highlight2 
\par \cf0\highlight0 [init] Using Kubernetes version: v1.21.8\cf1\highlight2 
\par \cf0\highlight0 [preflight] Running pre-flight checks\cf1\highlight2 
\par \cf0\highlight0 [preflight] The system verification failed. Printing the output from the verification:\cf1\highlight2 
\par \cf10\highlight0 KERNEL_VERSION\cf0 : \cf11 4.15.0-159-generic\cf1\highlight2 
\par \cf10\highlight0 CONFIG_NAMESPACES\cf0 : \cf11 enabled\cf1\highlight2 
\par \cf10\highlight0 CONFIG_NET_NS\cf0 : \cf11 enabled\cf1\highlight2 
\par \cf10\highlight0 CONFIG_PID_NS\cf0 : \cf11 enabled\cf1\highlight2 
\par \cf10\highlight0 CONFIG_IPC_NS\cf0 : \cf11 enabled\cf1\highlight2 
\par \cf10\highlight0 CONFIG_UTS_NS\cf0 : \cf11 enabled\cf1\highlight2 
\par \cf10\highlight0 CONFIG_CGROUPS\cf0 : \cf11 enabled\cf1\highlight2 
\par \cf10\highlight0 CONFIG_CGROUP_CPUACCT\cf0 : \cf11 enabled\cf1\highlight2 
\par \cf10\highlight0 CONFIG_CGROUP_DEVICE\cf0 : \cf11 enabled\cf1\highlight2 
\par \cf10\highlight0 CONFIG_CGROUP_FREEZER\cf0 : \cf11 enabled\cf1\highlight2 
\par \cf10\highlight0 CONFIG_CGROUP_PIDS\cf0 : \cf11 enabled\cf1\highlight2 
\par \cf10\highlight0 CONFIG_CGROUP_SCHED\cf0 : \cf11 enabled\cf1\highlight2 
\par \cf10\highlight0 CONFIG_CPUSETS\cf0 : \cf11 enabled\cf1\highlight2 
\par \cf10\highlight0 CONFIG_MEMCG\cf0 : \cf11 enabled\cf1\highlight2 
\par \cf10\highlight0 CONFIG_INET\cf0 : \cf11 enabled\cf1\highlight2 
\par \cf10\highlight0 CONFIG_EXT4_FS\cf0 : \cf11 enabled\cf1\highlight2 
\par \cf10\highlight0 CONFIG_PROC_FS\cf0 : \cf11 enabled\cf1\highlight2 
\par \cf10\highlight0 CONFIG_NETFILTER_XT_TARGET_REDIRECT\cf0 : \cf11 enabled (as module)\cf1\highlight2 
\par \cf10\highlight0 CONFIG_NETFILTER_XT_MATCH_COMMENT\cf0 : \cf11 enabled (as module)\cf1\highlight2 
\par \cf10\highlight0 CONFIG_FAIR_GROUP_SCHED\cf0 : \cf11 enabled\cf1\highlight2 
\par \cf10\highlight0 CONFIG_OVERLAY_FS\cf0 : \cf11 enabled (as module)\cf1\highlight2 
\par \cf10\highlight0 CONFIG_AUFS_FS\cf0 : \cf11 enabled (as module)\cf1\highlight2 
\par \cf10\highlight0 CONFIG_BLK_DEV_DM\cf0 : \cf11 enabled\cf1\highlight2 
\par \cf10\highlight0 CONFIG_CFS_BANDWIDTH\cf0 : \cf11 enabled\cf1\highlight2 
\par \cf10\highlight0 CONFIG_CGROUP_HUGETLB\cf0 : \cf11 enabled\cf1\highlight2 
\par \cf10\highlight0 OS\cf0 : \cf11 Linux\cf1\highlight2 
\par \cf10\highlight0 CGROUPS_CPU\cf0 : \cf11 enabled\cf1\highlight2 
\par \cf10\highlight0 CGROUPS_CPUACCT\cf0 : \cf11 enabled\cf1\highlight2 
\par \cf10\highlight0 CGROUPS_CPUSET\cf0 : \cf11 enabled\cf1\highlight2 
\par \cf10\highlight0 CGROUPS_DEVICES\cf0 : \cf11 enabled\cf1\highlight2 
\par \cf10\highlight0 CGROUPS_FREEZER\cf0 : \cf11 enabled\cf1\highlight2 
\par \cf10\highlight0 CGROUPS_MEMORY\cf0 : \cf11 enabled\cf1\highlight2 
\par \cf10\highlight0 CGROUPS_PIDS\cf0 : \cf11 enabled\cf1\highlight2 
\par \cf10\highlight0 CGROUPS_HUGETLB\cf0 : \cf11 enabled\cf1\highlight2 
\par \cf0\highlight0 error execution phase preflight: [preflight] Some fatal errors occurred:\cf1\highlight2 
\par \cf0\highlight0         [ERROR CRI]: container runtime is not running: output: Client:\cf1\highlight2 
\par \cf0\highlight0  Context:    default\cf1\highlight2 
\par \cf0\highlight0  Debug Mode: false\cf1\highlight2 
\par 
\par \cf0\highlight0 Server:\cf1\highlight2 
\par \cf0\highlight0 ERROR: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\cf1\highlight2 
\par \cf0\highlight0 errors pretty printing info\cf1\highlight2 
\par \cf0\highlight0 , error: exit status 1\cf1\highlight2 
\par \cf0\highlight0         [ERROR Service-Docker]: docker service is not active, please run 'systemctl start docker.service'\cf1\highlight2 
\par \cf0\highlight0         [ERROR IsDockerSystemdCheck]: cannot execute 'docker info -f \{\{.CgroupDriver\}\}': exit status 2\cf1\highlight2 
\par \cf0\highlight0         [ERROR FileContent--proc-sys-net-ipv4-ip_forward]: /proc/sys/net/ipv4/ip_forward contents are not set to 1\cf1\highlight2 
\par \cf0\highlight0         [ERROR Swap]: running with swap on is not supported. Please disable swap\cf1\highlight2 
\par \cf0\highlight0         [ERROR SystemVerification]: error verifying Docker info: "Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?"\cf1\highlight2 
\par \cf0\highlight0 [preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`\cf1\highlight2 
\par \cf0\highlight0 To see the stack trace of this error execute with --v=5 or higher\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# systemctl status docker\cf1\highlight2 
\par \cf0\highlight0\u9679? docker.service - Docker Application Container Engine\cf1\highlight2 
\par \cf0\highlight0    Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled)\cf1\highlight2 
\par \cf0\highlight0    Active: inactive (dead)\cf1\highlight2 
\par \cf0\highlight0      Docs: https://docs.docker.com\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# systemctl start docker\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# systemctl enable docker\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# systemctl status containerd\cf1\highlight2 
\par \cf3\highlight0\u9679?\cf0  containerd.service - containerd container runtime\cf1\highlight2 
\par \cf0\highlight0    Loaded: loaded (/lib/systemd/system/containerd.service; enabled; vendor preset: enabled)\cf1\highlight2 
\par \cf0\highlight0    Active: \cf3 active (running)\cf0  since Tue 2021-12-21 10:21:56 UTC; 16s ago\cf1\highlight2 
\par \cf0\highlight0      Docs: https://containerd.io\cf1\highlight2 
\par \cf0\highlight0  Main PID: 15004 (containerd)\cf1\highlight2 
\par \cf0\highlight0     Tasks: 16\cf1\highlight2 
\par \cf0\highlight0    CGroup: /system.slice/containerd.service\cf1\highlight2 
\par \cf0\highlight0            \u9492?\u9472?15004 /usr/bin/containerd\cf1\highlight2 
\par 
\par \cf0\highlight0 Dec 21 10:21:56 node-d containerd[15004]: time="2021-12-21T10:21:56.441900487Z" level=info msg="Start subscribing containerd event"\cf1\highlight2 
\par \cf0\highlight0 Dec 21 10:21:56 node-d containerd[15004]: time="2021-12-21T10:21:56.442072034Z" level=info msg="Start recovering state"\cf1\highlight2 
\par \cf0\highlight0 Dec 21 10:21:56 node-d containerd[15004]: time="2021-12-21T10:21:56.442465389Z" level=info msg="Start event monitor"\cf1\highlight2 
\par \cf0\highlight0 Dec 21 10:21:56 node-d containerd[15004]: time="2021-12-21T10:21:56.442492956Z" level=info msg=serving... address=/run/containerd/containerd.sock.ttrpc\cf1\highlight2 
\par \cf0\highlight0 Dec 21 10:21:56 node-d containerd[15004]: time="2021-12-21T10:21:56.442580778Z" level=info msg="Start snapshots syncer"\cf1\highlight2 
\par \cf0\highlight0 Dec 21 10:21:56 node-d containerd[15004]: time="2021-12-21T10:21:56.442692709Z" level=info msg="Start cni network conf syncer"\cf1\highlight2 
\par \cf0\highlight0 Dec 21 10:21:56 node-d containerd[15004]: time="2021-12-21T10:21:56.442732638Z" level=info msg="Start streaming server"\cf1\highlight2 
\par \cf0\highlight0 Dec 21 10:21:56 node-d containerd[15004]: time="2021-12-21T10:21:56.442753672Z" level=info msg=serving... address=/run/containerd/containerd.sock\cf1\highlight2 
\par \cf0\highlight0 Dec 21 10:21:56 node-d containerd[15004]: time="2021-12-21T10:21:56.443061698Z" level=info msg="containerd successfully booted in 0.193866s"\cf1\highlight2 
\par \cf0\highlight0 Dec 21 10:21:56 node-d systemd[1]: Started containerd container runtime.\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# systemctl enable containerd\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# kubeadm init\cf1\highlight2 
\par \cf0\highlight0 I1221 10:22:30.811672   15369 version.go:254] remote version is much newer: v1.23.1; falling back to: stable-1.21\cf1\highlight2 
\par \cf0\highlight0 [init] Using Kubernetes version: v1.21.8\cf1\highlight2 
\par \cf0\highlight0 [preflight] Running pre-flight checks\cf1\highlight2 
\par \cf0\highlight0         [WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kuber      netes.io/docs/setup/cri/\cf1\highlight2 
\par \cf0\highlight0 error execution phase preflight: [preflight] Some fatal errors occurred:\cf1\highlight2 
\par \cf0\highlight0         [ERROR Swap]: running with swap on is not supported. Please disable swap\cf1\highlight2 
\par \cf0\highlight0 [preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`\cf1\highlight2 
\par \cf0\highlight0 To see the stack trace of this error execute with --v=5 or higher\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# swapoff -a\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# kubeadm init\cf1\highlight2 
\par \cf0\highlight0 I1221 10:23:21.537688   15658 version.go:254] remote version is much newer: v1.23.1; falling back to: stable-1.21\cf1\highlight2 
\par \cf0\highlight0 [init] Using Kubernetes version: v1.21.8\cf1\highlight2 
\par \cf0\highlight0 [preflight] Running pre-flight checks\cf1\highlight2 
\par \cf0\highlight0         [WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kuber      netes.io/docs/setup/cri/\cf1\highlight2 
\par \cf0\highlight0 [preflight] Pulling images required for setting up a Kubernetes cluster\cf1\highlight2 
\par \cf0\highlight0 [preflight] This might take a minute or two, depending on the speed of your internet connection\cf1\highlight2 
\par \cf0\highlight0 [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'\cf1\highlight2 
\par \cf0\highlight0 [certs] Using certificateDir folder "/etc/kubernetes/pki"\cf1\highlight2 
\par \cf0\highlight0 [certs] Generating "ca" certificate and key\cf1\highlight2 
\par \cf0\highlight0 [certs] Generating "apiserver" certificate and key\cf1\highlight2 
\par \cf0\highlight0 [certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local node-d] and IPs [10.      96.0.1 192.168.1.210]\cf1\highlight2 
\par \cf0\highlight0 [certs] Generating "apiserver-kubelet-client" certificate and key\cf1\highlight2 
\par \cf0\highlight0 [certs] Generating "front-proxy-ca" certificate and key\cf1\highlight2 
\par \cf0\highlight0 [certs] Generating "front-proxy-client" certificate and key\cf1\highlight2 
\par \cf0\highlight0 [certs] Generating "etcd/ca" certificate and key\cf1\highlight2 
\par \cf0\highlight0 [certs] Generating "etcd/server" certificate and key\cf1\highlight2 
\par \cf0\highlight0 [certs] etcd/server serving cert is signed for DNS names [localhost node-d] and IPs [192.168.1.210 127.0.0.1 ::1]\cf1\highlight2 
\par \cf0\highlight0 [certs] Generating "etcd/peer" certificate and key\cf1\highlight2 
\par \cf0\highlight0 [certs] etcd/peer serving cert is signed for DNS names [localhost node-d] and IPs [192.168.1.210 127.0.0.1 ::1]\cf1\highlight2 
\par \cf0\highlight0 [certs] Generating "etcd/healthcheck-client" certificate and key\cf1\highlight2 
\par \cf0\highlight0 [certs] Generating "apiserver-etcd-client" certificate and key\cf1\highlight2 
\par \cf0\highlight0 [certs] Generating "sa" key and public key\cf1\highlight2 
\par \cf0\highlight0 [kubeconfig] Using kubeconfig folder "/etc/kubernetes"\cf1\highlight2 
\par \cf0\highlight0 [kubeconfig] Writing "admin.conf" kubeconfig file\cf1\highlight2 
\par \cf0\highlight0 [kubeconfig] Writing "kubelet.conf" kubeconfig file\cf1\highlight2 
\par \cf0\highlight0 [kubeconfig] Writing "controller-manager.conf" kubeconfig file\cf1\highlight2 
\par \cf0\highlight0 [kubeconfig] Writing "scheduler.conf" kubeconfig file\cf1\highlight2 
\par \cf0\highlight0 [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"\cf1\highlight2 
\par \cf0\highlight0 [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"\cf1\highlight2 
\par \cf0\highlight0 [kubelet-start] Starting the kubelet\cf1\highlight2 
\par \cf0\highlight0 [control-plane] Using manifest folder "/etc/kubernetes/manifests"\cf1\highlight2 
\par \cf0\highlight0 [control-plane] Creating static Pod manifest for "kube-apiserver"\cf1\highlight2 
\par \cf0\highlight0 [control-plane] Creating static Pod manifest for "kube-controller-manager"\cf1\highlight2 
\par \cf0\highlight0 [control-plane] Creating static Pod manifest for "kube-scheduler"\cf1\highlight2 
\par \cf0\highlight0 [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"\cf1\highlight2 
\par \cf0\highlight0 [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s\cf1\highlight2 
\par \cf0\highlight0 [apiclient] All control plane components are healthy after 23.007941 seconds\cf1\highlight2 
\par \cf0\highlight0 [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace\cf1\highlight2 
\par \cf0\highlight0 [kubelet] Creating a ConfigMap "kubelet-config-1.21" in namespace kube-system with the configuration for the kubelets in the cluster\cf1\highlight2 
\par \cf0\highlight0 [upload-certs] Skipping phase. Please see --upload-certs\cf1\highlight2 
\par \cf0\highlight0 [mark-control-plane] Marking the node node-d as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane       node.kubernetes.io/exclude-from-external-load-balancers]\cf1\highlight2 
\par \cf0\highlight0 [mark-control-plane] Marking the node node-d as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]\cf1\highlight2 
\par \cf0\highlight0 [bootstrap-token] Using token: e8buo7.oajndabhug4dlc17\cf1\highlight2 
\par \cf0\highlight0 [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles\cf1\highlight2 
\par \cf0\highlight0 [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes\cf1\highlight2 
\par \cf0\highlight0 [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\cf1\highlight2 
\par \cf0\highlight0 [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\cf1\highlight2 
\par \cf0\highlight0 [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\cf1\highlight2 
\par \cf0\highlight0 [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace\cf1\highlight2 
\par \cf0\highlight0 [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key\cf1\highlight2 
\par \cf0\highlight0 [addons] Applied essential addon: CoreDNS\cf1\highlight2 
\par \cf0\highlight0 [addons] Applied essential addon: kube-proxy\cf1\highlight2 
\par 
\par \cf0\highlight0 Your Kubernetes control-plane has initialized successfully!\cf1\highlight2 
\par 
\par \cf0\highlight0 To start using your cluster, you need to run the following as a regular user:\cf1\highlight2 
\par 
\par \cf0\highlight0   mkdir -p $HOME/.kube\cf1\highlight2 
\par \cf0\highlight0   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\cf1\highlight2 
\par \cf0\highlight0   sudo chown $(id -u):$(id -g) $HOME/.kube/config\cf1\highlight2 
\par 
\par \cf0\highlight0 Alternatively, if you are the root user, you can run:\cf1\highlight2 
\par 
\par \cf0\highlight0   export KUBECONFIG=/etc/kubernetes/admin.conf\cf1\highlight2 
\par 
\par \cf0\highlight0 You should now deploy a pod network to the cluster.\cf1\highlight2 
\par \cf0\highlight0 Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:\cf1\highlight2 
\par \cf0\highlight0   https://kubernetes.io/docs/concepts/cluster-administration/addons/\cf1\highlight2 
\par 
\par \cf0\highlight0 Then you can join any number of worker nodes by running the following on each as root:\cf1\highlight2 
\par 
\par \cf0\highlight0 kubeadm join 192.168.1.210:6443 --token e8buo7.oajndabhug4dlc17 \\\cf1\highlight2 
\par \cf0\highlight0         --discovery-token-ca-cert-hash sha256:b8676dd1a9a6afa4b18af99cd19cfbe0aa9882ff98d546edb4dfcd4665efb737\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# export KUBECONFIG=/etc/kubernetes/admin.conf\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# kubectl get nodes\cf1\highlight2 
\par \cf0\highlight0 NAME     STATUS     ROLES                  AGE   VERSION\cf1\highlight2 
\par \cf0\highlight0 node-d   NotReady   control-plane,master   29s   v1.21.1\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# export kubever=$(kubectl version | base64 | tr -d '\\n')\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$kubever"\cf1\highlight2 
\par \cf0\highlight0 serviceaccount/weave-net created\cf1\highlight2 
\par \cf0\highlight0 clusterrole.rbac.authorization.k8s.io/weave-net created\cf1\highlight2 
\par \cf0\highlight0 clusterrolebinding.rbac.authorization.k8s.io/weave-net created\cf1\highlight2 
\par \cf0\highlight0 role.rbac.authorization.k8s.io/weave-net created\cf1\highlight2 
\par \cf0\highlight0 rolebinding.rbac.authorization.k8s.io/weave-net created\cf1\highlight2 
\par \cf0\highlight0 daemonset.apps/weave-net created\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# kubectl get nodes\cf1\highlight2 
\par \cf0\highlight0 NAME     STATUS     ROLES                  AGE   VERSION\cf1\highlight2 
\par \cf0\highlight0 node-d   NotReady   control-plane,master   65s   v1.21.1\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# kubectl get nodes\cf1\highlight2 
\par \cf0\highlight0 NAME     STATUS     ROLES                  AGE   VERSION\cf1\highlight2 
\par \cf0\highlight0 node-d   NotReady   control-plane,master   67s   v1.21.1\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# kubectl get nodes\cf1\highlight2 
\par \cf0\highlight0 NAME     STATUS     ROLES                  AGE   VERSION\cf1\highlight2 
\par \cf0\highlight0 node-d   NotReady   control-plane,master   68s   v1.21.1\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# kubectl get nodes\cf1\highlight2 
\par \cf0\highlight0 NAME     STATUS     ROLES                  AGE   VERSION\cf1\highlight2 
\par \cf0\highlight0 node-d   NotReady   control-plane,master   69s   v1.21.1\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# kubectl get nodes\cf1\highlight2 
\par \cf0\highlight0 NAME     STATUS     ROLES                  AGE   VERSION\cf1\highlight2 
\par \cf0\highlight0 node-d   NotReady   control-plane,master   70s   v1.21.1\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# kubectl get nodes\cf1\highlight2 
\par \cf0\highlight0 NAME     STATUS     ROLES                  AGE   VERSION\cf1\highlight2 
\par \cf0\highlight0 node-d   NotReady   control-plane,master   70s   v1.21.1\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# kubectl get nodes\cf1\highlight2 
\par \cf0\highlight0 NAME     STATUS     ROLES                  AGE   VERSION\cf1\highlight2 
\par \cf0\highlight0 node-d   NotReady   control-plane,master   71s   v1.21.1\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# kubectl get nodes\cf1\highlight2 
\par \cf0\highlight0 NAME     STATUS   ROLES                  AGE   VERSION\cf1\highlight2 
\par \cf0\highlight0 node-d   Ready    control-plane,master   72s   v1.21.1\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# kubectl get nodes\cf1\highlight2 
\par \cf0\highlight0 NAME     STATUS   ROLES                  AGE   VERSION\cf1\highlight2 
\par \cf0\highlight0 node-d   Ready    control-plane,master   73s   v1.21.1\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# GOLANG_VERSION=$\{GOLANG_VERSION:-"1.14.15"\}\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# sudo apt -y update\cf1\highlight2 
\par \cf0\highlight0 Hit:1 http://in.archive.ubuntu.com/ubuntu bionic InRelease\cf1\highlight2 
\par \cf0\highlight0 Hit:2 http://in.archive.ubuntu.com/ubuntu bionic-updates InRelease\cf1\highlight2 
\par \cf0\highlight0 Hit:3 http://in.archive.ubuntu.com/ubuntu bionic-backports InRelease\cf1\highlight2 
\par \cf0\highlight0 Hit:4 http://in.archive.ubuntu.com/ubuntu bionic-security InRelease\cf1\highlight2 
\par \cf0\highlight0 Hit:5 https://packages.cloud.google.com/apt kubernetes-xenial InRelease\cf1\highlight2 
\par \cf0\highlight0 Reading package lists... Done\cf1\highlight2 
\par \cf0\highlight0 Building dependency tree\cf1\highlight2 
\par \cf0\highlight0 Reading state information... Done\cf1\highlight2 
\par \cf0\highlight0 159 packages can be upgraded. Run 'apt list --upgradable' to see them.\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~#\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~#  sudo apt -y install make\cf1\highlight2 
\par \cf0\highlight0 Reading package lists... Done\cf1\highlight2 
\par \cf0\highlight0 Building dependency tree\cf1\highlight2 
\par \cf0\highlight0 Reading state information... Done\cf1\highlight2 
\par \cf0\highlight0 make is already the newest version (4.1-9.1ubuntu1).\cf1\highlight2 
\par \cf0\highlight0 0 upgraded, 0 newly installed, 0 to remove and 159 not upgraded.\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~#\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~#  sudo apt -y install gcc\cf1\highlight2 
\par \cf0\highlight0 Reading package lists... Done\cf1\highlight2 
\par \cf0\highlight0 Building dependency tree\cf1\highlight2 
\par \cf0\highlight0 Reading state information... Done\cf1\highlight2 
\par \cf0\highlight0 gcc is already the newest version (4:7.4.0-1ubuntu2.3).\cf1\highlight2 
\par \cf0\highlight0 0 upgraded, 0 newly installed, 0 to remove and 159 not upgraded.\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~#\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~#  sudo apt -y install jq\cf1\highlight2 
\par \cf0\highlight0 Reading package lists... Done\cf1\highlight2 
\par \cf0\highlight0 Building dependency tree\cf1\highlight2 
\par \cf0\highlight0 Reading state information... Done\cf1\highlight2 
\par \cf0\highlight0 jq is already the newest version (1.5+dfsg-2).\cf1\highlight2 
\par \cf0\highlight0 0 upgraded, 0 newly installed, 0 to remove and 159 not upgraded.\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~#  wget https://dl.google.com/go/go$\{GOLANG_VERSION\}.linux-amd64.tar.gz -P /tmp\cf1\highlight2 
\par \cf0\highlight0 --2021-12-21 10:27:16--  https://dl.google.com/go/go1.14.15.linux-amd64.tar.gz\cf1\highlight2 
\par \cf0\highlight0 Resolving dl.google.com (dl.google.com)... 142.250.183.142, 2404:6800:4009:824::200e\cf1\highlight2 
\par \cf0\highlight0 Connecting to dl.google.com (dl.google.com)|142.250.183.142|:443... connected.\cf1\highlight2 
\par \cf0\highlight0 HTTP request sent, awaiting response... 200 OK\cf1\highlight2 
\par \cf0\highlight0 Length: 124135233 (118M) [application/octet-stream]\cf1\highlight2 
\par \cf0\highlight0\f1 Saving to: \lquote /tmp/go1.14.15.linux-amd64.tar.gz\rquote\cf1\highlight2\f0 
\par 
\par \cf0\highlight0 go1.14.15.linux-amd64.tar.gz                7%[=====>                                                                              ]   9.38M  2.16MB/s    eta 67s    s      go1.14.15.linux-amd64.tar.gz              100%[===================================================================================>] 118.38M  2.85MB/s    in 55s\cf1\highlight2 
\par 
\par \cf0\highlight0\f1 2021-12-21 10:28:14 (2.14 MB/s) - \lquote /tmp/go1.14.15.linux-amd64.tar.gz\rquote  saved [124135233/124135233]\cf1\highlight2\f0 
\par 
\par \cf0\highlight0 root@node-d:~# sudo tar -C /usr/local -xzf /tmp/go$\{GOLANG_VERSION\}.linux-amd64.tar.gz\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# sudo apt-get install vim\cf1\highlight2 
\par \cf0\highlight0 Reading package lists... Done\cf1\highlight2 
\par \cf0\highlight0 Building dependency tree\cf1\highlight2 
\par \cf0\highlight0 Reading state information... Done\cf1\highlight2 
\par \cf0\highlight0 The following additional packages will be installed:\cf1\highlight2 
\par \cf0\highlight0   vim-common vim-runtime vim-tiny\cf1\highlight2 
\par \cf0\highlight0 Suggested packages:\cf1\highlight2 
\par \cf0\highlight0   ctags vim-doc vim-scripts indent\cf1\highlight2 
\par \cf0\highlight0 The following packages will be upgraded:\cf1\highlight2 
\par \cf0\highlight0   vim vim-common vim-runtime vim-tiny\cf1\highlight2 
\par \cf0\highlight0 4 upgraded, 0 newly installed, 0 to remove and 155 not upgraded.\cf1\highlight2 
\par \cf0\highlight0 Need to get 7,135 kB of archives.\cf1\highlight2 
\par \cf0\highlight0 After this operation, 4,096 B of additional disk space will be used.\cf1\highlight2 
\par \cf0\highlight0 Do you want to continue? [Y/n] y\cf1\highlight2 
\par \cf0\highlight0 Get:1 http://in.archive.ubuntu.com/ubuntu bionic-updates/main amd64 vim amd64 2:8.0.1453-1ubuntu1.7 [1,153 kB]\cf1\highlight2 
\par \cf0\highlight0 0% [1 vim 12.8 kB/1,153 kB 1%]\cf1\highlight2 
\par \cf0\highlight0 Get:2 http://in.archive.ubuntu.com/ubuntu bionic-updates/main amd64 vim-tiny amd64 2:8.0.1453-1ubuntu1.7 [476 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:3 http://in.archive.ubuntu.com/ubuntu bionic-updates/main amd64 vim-runtime all 2:8.0.1453-1ubuntu1.7 [5,435 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:4 http://in.archive.ubuntu.com/ubuntu bionic-updates/main amd64 vim-common all 2:8.0.1453-1ubuntu1.7 [70.8 kB]\cf1\highlight2 
\par \cf0\highlight0 Fetched 7,135 kB in 1s (6,457 kB/s)\cf1\highlight2 
\par \cf0\highlight0 (Reading database ... 71754 files and directories currently installed.)\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../vim_2%3a8.0.1453-1ubuntu1.7_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking vim (2:8.0.1453-1ubuntu1.7) over (2:8.0.1453-1ubuntu1.3) ...\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../vim-tiny_2%3a8.0.1453-1ubuntu1.7_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking vim-tiny (2:8.0.1453-1ubuntu1.7) over (2:8.0.1453-1ubuntu1.3) ...\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../vim-runtime_2%3a8.0.1453-1ubuntu1.7_all.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking vim-runtime (2:8.0.1453-1ubuntu1.7) over (2:8.0.1453-1ubuntu1.3) ...\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../vim-common_2%3a8.0.1453-1ubuntu1.7_all.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking vim-common (2:8.0.1453-1ubuntu1.7) over (2:8.0.1453-1ubuntu1.3) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up vim-common (2:8.0.1453-1ubuntu1.7) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up vim-runtime (2:8.0.1453-1ubuntu1.7) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up vim-tiny (2:8.0.1453-1ubuntu1.7) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up vim (2:8.0.1453-1ubuntu1.7) ...\cf1\highlight2 
\par \cf0\highlight0 Processing triggers for mime-support (3.60ubuntu1) ...\cf1\highlight2 
\par \cf0\highlight0 Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# go version\cf1\highlight2 
\par 
\par \cf0\highlight0 Command 'go' not found, but can be installed with:\cf1\highlight2 
\par 
\par \cf0\highlight0 apt install golang-go\cf1\highlight2 
\par \cf0\highlight0 apt install gccgo-go\cf1\highlight2 
\par 
\par \cf0\highlight0 root@node-d:~# vi ~/.bashrc\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# source ~/.bashrc\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# go version\cf1\highlight2 
\par \cf0\highlight0 go version go1.14.15 linux/amd64\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# go env\cf1\highlight2 
\par \cf0\highlight0 GO111MODULE=""\cf1\highlight2 
\par \cf0\highlight0 GOARCH="amd64"\cf1\highlight2 
\par \cf0\highlight0 GOBIN=""\cf1\highlight2 
\par \cf0\highlight0 GOCACHE="/root/.cache/go-build"\cf1\highlight2 
\par \cf0\highlight0 GOENV="/root/.config/go/env"\cf1\highlight2 
\par \cf0\highlight0 GOEXE=""\cf1\highlight2 
\par \cf0\highlight0 GOFLAGS=""\cf1\highlight2 
\par \cf0\highlight0 GOHOSTARCH="amd64"\cf1\highlight2 
\par \cf0\highlight0 GOHOSTOS="linux"\cf1\highlight2 
\par \cf0\highlight0 GOINSECURE=""\cf1\highlight2 
\par \cf0\highlight0 GONOPROXY=""\cf1\highlight2 
\par \cf0\highlight0 GONOSUMDB=""\cf1\highlight2 
\par \cf0\highlight0 GOOS="linux"\cf1\highlight2 
\par \cf0\highlight0 GOPATH="/usr/local/go/bin"\cf1\highlight2 
\par \cf0\highlight0 GOPRIVATE=""\cf1\highlight2 
\par \cf0\highlight0 GOPROXY="https://proxy.golang.org,direct"\cf1\highlight2 
\par \cf0\highlight0 GOROOT="/usr/local/go"\cf1\highlight2 
\par \cf0\highlight0 GOSUMDB="sum.golang.org"\cf1\highlight2 
\par \cf0\highlight0 GOTMPDIR=""\cf1\highlight2 
\par \cf0\highlight0 GOTOOLDIR="/usr/local/go/pkg/tool/linux_amd64"\cf1\highlight2 
\par \cf0\highlight0 GCCGO="gccgo"\cf1\highlight2 
\par \cf0\highlight0 AR="ar"\cf1\highlight2 
\par \cf0\highlight0 CC="gcc"\cf1\highlight2 
\par \cf0\highlight0 CXX="g++"\cf1\highlight2 
\par \cf0\highlight0 CGO_ENABLED="1"\cf1\highlight2 
\par \cf0\highlight0 GOMOD=""\cf1\highlight2 
\par \cf0\highlight0 CGO_CFLAGS="-g -O2"\cf1\highlight2 
\par \cf0\highlight0 CGO_CPPFLAGS=""\cf1\highlight2 
\par \cf0\highlight0 CGO_CXXFLAGS="-g -O2"\cf1\highlight2 
\par \cf0\highlight0 CGO_FFLAGS="-g -O2"\cf1\highlight2 
\par \cf0\highlight0 CGO_LDFLAGS="-g -O2"\cf1\highlight2 
\par \cf0\highlight0 PKG_CONFIG="pkg-config"\cf1\highlight2 
\par \cf0\highlight0 GOGCCFLAGS="-fPIC -m64 -pthread -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build479461053=/tmp/go-build -gno-record-gcc-switches"\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# mkdir -p go/src/github.com\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~# cd go/src/github.com\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com# git clone https://github.com/CentaurusInfra/fornax.git\cf1\highlight2 
\par \cf0\highlight0 Cloning into 'fornax'...\cf1\highlight2 
\par \cf0\highlight0 remote: Enumerating objects: 50860, done.\cf1\highlight2 
\par \cf0\highlight0 remote: Counting objects: 100% (788/788), done.\cf1\highlight2 
\par \cf0\highlight0 remote: Compressing objects: 100% (452/452), done.\cf1\highlight2 
\par \cf0\highlight0 remote: Total 50860 (delta 371), reused 638 (delta 294), pack-reused 50072\cf1\highlight2 
\par \cf0\highlight0 Receiving objects: 100% (50860/50860), 125.52 MiB | 7.09 MiB/s, done.\cf1\highlight2 
\par \cf0\highlight0 Resolving deltas: 100% (27949/27949), done.\cf1\highlight2 
\par \cf0\highlight0 Checking out files: 100% (8900/8900), done.\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com# mv fornax kubeedge\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com# cd kubeedge\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# make all\cf1\highlight2 
\par \cf0\highlight0 hack/verify-golang.sh\cf1\highlight2 
\par \cf0\highlight0 go detail version: go version go1.14.15 linux/amd64\cf1\highlight2 
\par \cf0\highlight0 go version: 1.14.15\cf1\highlight2 
\par \cf0\highlight0 KUBEEDGE_OUTPUT_SUBPATH=_output/local hack/make-rules/build.sh\cf1\highlight2 
\par \cf0\highlight0 building github.com/kubeedge/kubeedge/cloud/cmd/cloudcore\cf1\highlight2 
\par \cf0\highlight0 + go build -o /root/go/src/github.com/kubeedge/_output/local/bin/cloudcore -gcflags= -ldflags '-s -w -buildid= -X github.com/kubeedge/kubeedge/pkg/version.buildDate=2      021-12-21T10:31:12Z -X github.com/kubeedge/kubeedge/pkg/version.gitCommit=aa964bae0c857b761262e2c44e68d6eb0ff6202d -X github.com/kubeedge/kubeedge/pkg/version.gitTree      State=clean -X github.com/kubeedge/kubeedge/pkg/version.gitVersion=v0.2-16+aa964bae0c857b -X github.com/kubeedge/kubeedge/pkg/version.gitMajor=0 -X github.com/kubeedg      e/kubeedge/pkg/version.gitMinor=2+' github.com/kubeedge/kubeedge/cloud/cmd/cloudcore\cf1\highlight2 
\par \cf0\highlight0 + set +x\cf1\highlight2 
\par \cf0\highlight0 building github.com/kubeedge/kubeedge/cloud/cmd/admission\cf1\highlight2 
\par \cf0\highlight0 + go build -o /root/go/src/github.com/kubeedge/_output/local/bin/admission -gcflags= -ldflags '-s -w -buildid= -X github.com/kubeedge/kubeedge/pkg/version.buildDate=2      021-12-21T10:31:12Z -X github.com/kubeedge/kubeedge/pkg/version.gitCommit=aa964bae0c857b761262e2c44e68d6eb0ff6202d -X github.com/kubeedge/kubeedge/pkg/version.gitTree      State=clean -X github.com/kubeedge/kubeedge/pkg/version.gitVersion=v0.2-16+aa964bae0c857b -X github.com/kubeedge/kubeedge/pkg/version.gitMajor=0 -X github.com/kubeedg      e/kubeedge/pkg/version.gitMinor=2+' github.com/kubeedge/kubeedge/cloud/cmd/admission\cf1\highlight2 
\par \cf0\highlight0 + set +x\cf1\highlight2 
\par \cf0\highlight0 building github.com/kubeedge/kubeedge/keadm/cmd/keadm\cf1\highlight2 
\par \cf0\highlight0 + go build -o /root/go/src/github.com/kubeedge/_output/local/bin/keadm -gcflags= -ldflags '-s -w -buildid= -X github.com/kubeedge/kubeedge/pkg/version.buildDate=2021-      12-21T10:31:12Z -X github.com/kubeedge/kubeedge/pkg/version.gitCommit=aa964bae0c857b761262e2c44e68d6eb0ff6202d -X github.com/kubeedge/kubeedge/pkg/version.gitTreeStat      e=clean -X github.com/kubeedge/kubeedge/pkg/version.gitVersion=v0.2-16+aa964bae0c857b -X github.com/kubeedge/kubeedge/pkg/version.gitMajor=0 -X github.com/kubeedge/ku      beedge/pkg/version.gitMinor=2+' github.com/kubeedge/kubeedge/keadm/cmd/keadm\cf1\highlight2 
\par \cf0\highlight0 + set +x\cf1\highlight2 
\par \cf0\highlight0 building github.com/kubeedge/kubeedge/edge/cmd/edgecore\cf1\highlight2 
\par \cf0\highlight0 + go build -o /root/go/src/github.com/kubeedge/_output/local/bin/edgecore -gcflags= -ldflags '-s -w -buildid= -X github.com/kubeedge/kubeedge/pkg/version.buildDate=20      21-12-21T10:31:12Z -X github.com/kubeedge/kubeedge/pkg/version.gitCommit=aa964bae0c857b761262e2c44e68d6eb0ff6202d -X github.com/kubeedge/kubeedge/pkg/version.gitTreeS      tate=clean -X github.com/kubeedge/kubeedge/pkg/version.gitVersion=v0.2-16+aa964bae0c857b -X github.com/kubeedge/kubeedge/pkg/version.gitMajor=0 -X github.com/kubeedge      /kubeedge/pkg/version.gitMinor=2+' github.com/kubeedge/kubeedge/edge/cmd/edgecore\cf1\highlight2 
\par \cf0\highlight0 + set +x\cf1\highlight2 
\par \cf0\highlight0 building github.com/kubeedge/kubeedge/edgesite/cmd/edgesite-agent\cf1\highlight2 
\par \cf0\highlight0 + go build -o /root/go/src/github.com/kubeedge/_output/local/bin/edgesite-agent -gcflags= -ldflags '-s -w -buildid= -X github.com/kubeedge/kubeedge/pkg/version.buildD      ate=2021-12-21T10:31:12Z -X github.com/kubeedge/kubeedge/pkg/version.gitCommit=aa964bae0c857b761262e2c44e68d6eb0ff6202d -X github.com/kubeedge/kubeedge/pkg/version.gi      tTreeState=clean -X github.com/kubeedge/kubeedge/pkg/version.gitVersion=v0.2-16+aa964bae0c857b -X github.com/kubeedge/kubeedge/pkg/version.gitMajor=0 -X github.com/ku      beedge/kubeedge/pkg/version.gitMinor=2+' github.com/kubeedge/kubeedge/edgesite/cmd/edgesite-agent\cf1\highlight2 
\par \cf0\highlight0 + set +x\cf1\highlight2 
\par \cf0\highlight0 building github.com/kubeedge/kubeedge/edgesite/cmd/edgesite-server\cf1\highlight2 
\par \cf0\highlight0 + go build -o /root/go/src/github.com/kubeedge/_output/local/bin/edgesite-server -gcflags= -ldflags '-s -w -buildid= -X github.com/kubeedge/kubeedge/pkg/version.build      Date=2021-12-21T10:31:12Z -X github.com/kubeedge/kubeedge/pkg/version.gitCommit=aa964bae0c857b761262e2c44e68d6eb0ff6202d -X github.com/kubeedge/kubeedge/pkg/version.g      itTreeState=clean -X github.com/kubeedge/kubeedge/pkg/version.gitVersion=v0.2-16+aa964bae0c857b -X github.com/kubeedge/kubeedge/pkg/version.gitMajor=0 -X github.com/k      ubeedge/kubeedge/pkg/version.gitMinor=2+' github.com/kubeedge/kubeedge/edgesite/cmd/edgesite-server\cf1\highlight2 
\par \cf0\highlight0 + set +x\cf1\highlight2 
\par \cf0\highlight0 yes | cp -r binaries/kubectl _output/local/bin/\cf1\highlight2 
\par \cf0\highlight0 mkdir -p _output/local/bin/crds\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/devices/devices_v1alpha2_device.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/devices/devices_v1alpha2_devicemodel.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/reliablesyncs/cluster_objectsync_v1alpha1.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/reliablesyncs/objectsync_v1alpha1.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/router/router_v1_rule.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/router/router_v1_ruleEndpoint.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/edgecluster/mission_v1.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/edgecluster/edgecluster_v1.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# scp 192.168.2.52:/etc/kubernetes/admin.conf  .\cf1\highlight2 
\par \cf0\highlight0 The authenticity of host '192.168.2.52 (192.168.2.52)' can't be established.\cf1\highlight2 
\par \cf0\highlight0 ECDSA key fingerprint is SHA256:CB1dE2wmZ3LcNKU6RDD0D8xosH3X8k+M4YQMZtC34RI.\cf1\highlight2 
\par \cf0\highlight0 Are you sure you want to continue connecting (yes/no)? yes\cf1\highlight2 
\par \cf0\highlight0 Warning: Permanently added '192.168.2.52' (ECDSA) to the list of known hosts.\cf1\highlight2 
\par \cf0\highlight0 root@192.168.2.52's password:\cf1\highlight2 
\par \cf0\highlight0 Permission denied, please try again.\cf1\highlight2 
\par \cf0\highlight0 root@192.168.2.52's password:\cf1\highlight2 
\par \cf0\highlight0 admin.conf                                                                                                                          100% 5596     3.6MB/s   00:00\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ls\cf1\highlight2 
\par \cf0\highlight0 admin.conf   \cf12 build\cf0          \cf12 cloud\cf0                CONTRIBUTING.md  \cf12 edgemesh\cf0                 go.mod  \cf12 keadm\cf0      MAINTAINERS     \cf12 mappers\cf0   \cf12 pkg\cf0            \cf12 staging\cf1\highlight2 
\par \cf0\highlight0 ADOPTERS.md  \cf12 CHANGELOG\cf0      CODE_OF_CONDUCT.md  \cf12 docs\cf0              \cf12 edgesite\cf0                 go.sum  LICENSE   MAINTAINERS.md  \cf12 _output\cf0   README.md     \cf12 tests\cf1\highlight2 
\par \cf12\highlight0 binaries\cf0      CHANGELOG.md  \cf12 common\cf0               \cf12 edge\cf0              external-dependency.md  \cf12 hack\cf0     \cf12 LICENSES\cf0   Makefile        OWNERS   README_zh.md  \cf12 vendor\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# vi admin.conf\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# make WHAT=cloudcore\cf1\highlight2 
\par \cf0\highlight0 hack/verify-golang.sh\cf1\highlight2 
\par \cf0\highlight0 make WHAT=cloudcore\cf1\highlight2 
\par \cf0\highlight0 make WHAT=edgecorego detail version: go version go1.14.15 linux/amd64\cf1\highlight2 
\par \cf0\highlight0 go version: 1.14.15\cf1\highlight2 
\par \cf0\highlight0 KUBEEDGE_OUTPUT_SUBPATH=_output/local hack/make-rules/build.sh cloudcore\cf1\highlight2 
\par \cf0\highlight0 building github.com/kubeedge/kubeedge/cloud/cmd/cloudcore\cf1\highlight2 
\par \cf0\highlight0 + go build -o /root/go/src/github.com/kubeedge/_output/local/bin/cloudcore -gcflags= -ldflags '-s -w -buildid= -X github.com/kubeedge/kubeedge/pkg/version.buildDate=2      021-12-21T10:36:33Z -X github.com/kubeedge/kubeedge/pkg/version.gitCommit=aa964bae0c857b761262e2c44e68d6eb0ff6202d -X github.com/kubeedge/kubeedge/pkg/version.gitTree      State=dirty -X github.com/kubeedge/kubeedge/pkg/version.gitVersion=v0.2-16+aa964bae0c857b-dirty -X github.com/kubeedge/kubeedge/pkg/version.gitMajor=0 -X github.com/k      ubeedge/kubeedge/pkg/version.gitMinor=2+' github.com/kubeedge/kubeedge/cloud/cmd/cloudcore\cf1\highlight2 
\par \cf0\highlight0 + set +x\cf1\highlight2 
\par \cf0\highlight0 yes | cp -r binaries/kubectl _output/local/bin/\cf1\highlight2 
\par \cf0\highlight0 mkdir -p _output/local/bin/crds\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/devices/devices_v1alpha2_device.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/devices/devices_v1alpha2_devicemodel.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/reliablesyncs/cluster_objectsync_v1alpha1.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/reliablesyncs/objectsync_v1alpha1.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/router/router_v1_rule.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/router/router_v1_ruleEndpoint.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/edgecluster/mission_v1.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/edgecluster/edgecluster_v1.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# make WHAT=edgecoremake WHAT=cloudcore\cf1\highlight2 
\par \cf0\highlight0 hack/verify-golang.sh\cf1\highlight2 
\par \cf0\highlight0 go detail version: go version go1.14.15 linux/amd64\cf1\highlight2 
\par \cf0\highlight0 go version: 1.14.15\cf1\highlight2 
\par \cf0\highlight0 KUBEEDGE_OUTPUT_SUBPATH=_output/local hack/make-rules/build.sh cloudcore\cf1\highlight2 
\par \cf0\highlight0 building github.com/kubeedge/kubeedge/cloud/cmd/cloudcore\cf1\highlight2 
\par \cf0\highlight0 + go build -o /root/go/src/github.com/kubeedge/_output/local/bin/cloudcore -gcflags= -ldflags '-s -w -buildid= -X github.com/kubeedge/kubeedge/pkg/version.buildDate=2      021-12-21T10:36:43Z -X github.com/kubeedge/kubeedge/pkg/version.gitCommit=aa964bae0c857b761262e2c44e68d6eb0ff6202d -X github.com/kubeedge/kubeedge/pkg/version.gitTree      State=dirty -X github.com/kubeedge/kubeedge/pkg/version.gitVersion=v0.2-16+aa964bae0c857b-dirty -X github.com/kubeedge/kubeedge/pkg/version.gitMajor=0 -X github.com/k      ubeedge/kubeedge/pkg/version.gitMinor=2+' github.com/kubeedge/kubeedge/cloud/cmd/cloudcore\cf1\highlight2 
\par \cf0\highlight0 + set +x\cf1\highlight2 
\par \cf0\highlight0 yes | cp -r binaries/kubectl _output/local/bin/\cf1\highlight2 
\par \cf0\highlight0 mkdir -p _output/local/bin/crds\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/devices/devices_v1alpha2_device.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/devices/devices_v1alpha2_devicemodel.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/reliablesyncs/cluster_objectsync_v1alpha1.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/reliablesyncs/objectsync_v1alpha1.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/router/router_v1_rule.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/router/router_v1_ruleEndpoint.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/edgecluster/mission_v1.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/edgecluster/edgecluster_v1.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# make WHAT=edgecore\cf1\highlight2 
\par \cf0\highlight0 hack/verify-golang.sh\cf1\highlight2 
\par \cf0\highlight0 go detail version: go version go1.14.15 linux/amd64\cf1\highlight2 
\par \cf0\highlight0 go version: 1.14.15\cf1\highlight2 
\par \cf0\highlight0 KUBEEDGE_OUTPUT_SUBPATH=_output/local hack/make-rules/build.sh edgecore\cf1\highlight2 
\par \cf0\highlight0 building github.com/kubeedge/kubeedge/edge/cmd/edgecore\cf1\highlight2 
\par \cf0\highlight0 + go build -o /root/go/src/github.com/kubeedge/_output/local/bin/edgecore -gcflags= -ldflags '-s -w -buildid= -X github.com/kubeedge/kubeedge/pkg/version.buildDate=20      21-12-21T10:36:55Z -X github.com/kubeedge/kubeedge/pkg/version.gitCommit=aa964bae0c857b761262e2c44e68d6eb0ff6202d -X github.com/kubeedge/kubeedge/pkg/version.gitTreeS      tate=dirty -X github.com/kubeedge/kubeedge/pkg/version.gitVersion=v0.2-16+aa964bae0c857b-dirty -X github.com/kubeedge/kubeedge/pkg/version.gitMajor=0 -X github.com/ku      beedge/kubeedge/pkg/version.gitMinor=2+' github.com/kubeedge/kubeedge/edge/cmd/edgecore\cf1\highlight2 
\par \cf0\highlight0 + set +x\cf1\highlight2 
\par \cf0\highlight0 yes | cp -r binaries/kubectl _output/local/bin/\cf1\highlight2 
\par \cf0\highlight0 mkdir -p _output/local/bin/crds\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/devices/devices_v1alpha2_device.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/devices/devices_v1alpha2_devicemodel.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/reliablesyncs/cluster_objectsync_v1alpha1.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/reliablesyncs/objectsync_v1alpha1.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/router/router_v1_rule.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/router/router_v1_ruleEndpoint.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/edgecluster/mission_v1.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/edgecluster/edgecluster_v1.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# mkdir /etc/kubeedge/config -p\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# cp /etc/kubernetes/admin.conf /root/.kube/config\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# _output/local/bin/cloudcore --minconfig > /etc/kubeedge/config/cloudcore.yaml\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# mkdir /etc/kubeedge/config -p\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# mkdir /root/.kube/config -p\cf1\highlight2 
\par \cf0\highlight0\f1 mkdir: cannot create directory \lquote /root/.kube/config\rquote : File exists\cf1\highlight2\f0 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# cd /etc/kubeedge/\cf1\highlight2 
\par \cf0\highlight0 root@node-d:/etc/kubeedge# ls\cf1\highlight2 
\par \cf12\highlight0 config\cf1\highlight2 
\par \cf0\highlight0 root@node-d:/etc/kubeedge# scp -r 192.168.2.50:/etc/kubeedge/certs .\cf1\highlight2 
\par \cf0\highlight0 The authenticity of host '192.168.2.50 (192.168.2.50)' can't be established.\cf1\highlight2 
\par \cf0\highlight0 ECDSA key fingerprint is SHA256:CB1dE2wmZ3LcNKU6RDD0D8xosH3X8k+M4YQMZtC34RI.\cf1\highlight2 
\par \cf0\highlight0 Are you sure you want to continue connecting (yes/no)? yes\cf1\highlight2 
\par \cf0\highlight0 Warning: Permanently added '192.168.2.50' (ECDSA) to the list of known hosts.\cf1\highlight2 
\par \cf0\highlight0 root@192.168.2.50's password:\cf1\highlight2 
\par \cf0\highlight0 server.crt                                                                                                                          100% 1619   794.6KB/s   00:00\cf1\highlight2 
\par \cf0\highlight0 server.csr                                                                                                                          100%  989   442.6KB/s   00:00\cf1\highlight2 
\par \cf0\highlight0 server.key                                                                                                                          100% 1679   883.1KB/s   00:00\cf1\highlight2 
\par \cf0\highlight0 root@node-d:/etc/kubeedge# scp -r 192.168.2.50:/etc/kubeedge/ca .\cf1\highlight2 
\par \cf0\highlight0 root@192.168.2.50's password:\cf1\highlight2 
\par \cf0\highlight0 rootCA.crt                                                                                                                          100% 2078     1.1MB/s   00:00\cf1\highlight2 
\par \cf0\highlight0 rootCA.key                                                                                                                          100% 3311     2.3MB/s   00:00\cf1\highlight2 
\par \cf0\highlight0 rootCA.srl                                                                                                                          100%   41    26.7KB/s   00:00\cf1\highlight2 
\par \cf0\highlight0 root@node-d:/etc/kubeedge# ls\cf1\highlight2 
\par \cf12\highlight0 ca\cf0   \cf12 certs\cf0   \cf12 config\cf1\highlight2 
\par \cf0\highlight0 root@node-d:/etc/kubeedge# cd ~/go/src/github.com/kubeedge\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ls\cf1\highlight2 
\par \cf0\highlight0 admin.conf   \cf12 build\cf0          \cf12 cloud\cf0                CONTRIBUTING.md  \cf12 edgemesh\cf0                 go.mod  \cf12 keadm\cf0      MAINTAINERS     \cf12 mappers\cf0   \cf12 pkg\cf0            \cf12 staging\cf1\highlight2 
\par \cf0\highlight0 ADOPTERS.md  \cf12 CHANGELOG\cf0      CODE_OF_CONDUCT.md  \cf12 docs\cf0              \cf12 edgesite\cf0                 go.sum  LICENSE   MAINTAINERS.md  \cf12 _output\cf0   README.md     \cf12 tests\cf1\highlight2 
\par \cf12\highlight0 binaries\cf0      CHANGELOG.md  \cf12 common\cf0               \cf12 edge\cf0              external-dependency.md  \cf12 hack\cf0     \cf12 LICENSES\cf0   Makefile        OWNERS   README_zh.md  \cf12 vendor\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# export KUBECONFIG=/etc/kubernetes/admin.conf\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge#  cp /etc/kubernetes/admin.conf  /root/edgecluster.kubeconfig\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# _output/local/bin/edgecore --edgeclusterconfig > /etc/kubeedge/config/edgecore.yaml\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# tests/edgecluster/hack/update_edgecore_config.sh admin.conf\cf1\highlight2 
\par \cf0\highlight0 Error from server (NotFound): namespaces "kubeedge" not found\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl apply -f build/crds/devices/devices_v1alpha2_device.yaml\cf1\highlight2 
\par \cf0\highlight0 kubectl apply -f build/crds/edgecluster/edgecluster_v1.yaml\cf1\highlight2 
\par \cf4\highlight0 Warning:\cf0  apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition\cf1\highlight2 
\par \cf0\highlight0 customresourcedefinition.apiextensions.k8s.io/devices.devices.kubeedge.io created\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl apply -f build/crds/devices/devices_v1alpha2_devicemodel.yaml\cf1\highlight2 
\par \cf4\highlight0 Warning:\cf0  apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition\cf1\highlight2 
\par \cf0\highlight0 customresourcedefinition.apiextensions.k8s.io/devicemodels.devices.kubeedge.io created\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge#\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl apply -f build/crds/reliablesyncs/cluster_objectsync_v1alpha1.yaml\cf1\highlight2 
\par \cf4\highlight0 Warning:\cf0  apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition\cf1\highlight2 
\par \cf0\highlight0 customresourcedefinition.apiextensions.k8s.io/clusterobjectsyncs.reliablesyncs.kubeedge.io created\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl apply -f build/crds/reliablesyncs/objectsync_v1alpha1.yaml\cf1\highlight2 
\par \cf4\highlight0 Warning:\cf0  apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition\cf1\highlight2 
\par \cf0\highlight0 customresourcedefinition.apiextensions.k8s.io/objectsyncs.reliablesyncs.kubeedge.io created\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge#\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl apply -f  build/crds/router/router_v1_rule.yaml\cf1\highlight2 
\par \cf0\highlight0 customresourcedefinition.apiextensions.k8s.io/rules.rules.kubeedge.io created\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl apply -f  build/crds/router/router_v1_ruleEndpoint.yaml\cf1\highlight2 
\par \cf0\highlight0 customresourcedefinition.apiextensions.k8s.io/ruleendpoints.rules.kubeedge.io created\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge#\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl apply -f build/crds/edgecluster/mission_v1.yaml\cf1\highlight2 
\par \cf4\highlight0 Warning:\cf0  apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition\cf1\highlight2 
\par \cf0\highlight0 customresourcedefinition.apiextensions.k8s.io/missions.edgeclusters.kubeedge.io created\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl apply -f build/crds/edgecluster/edgecluster_v1.yaml\cf1\highlight2 
\par \cf4\highlight0 Warning:\cf0  apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition\cf1\highlight2 
\par \cf0\highlight0 customresourcedefinition.apiextensions.k8s.io/edgeclusters.edgeclusters.kubeedge.io created\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# cp /etc/kubernetes/admin.conf /root/.kube/config\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# _output/local/bin/cloudcore --minconfig > /etc/kubeedge/config/cloudcore.yaml\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge#  cp /etc/kubernetes/admin.conf /root/edgecluster.kubeconfig\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge#  _output/local/bin/edgecore --edgeclusterconfig > /etc/kubeedge/config/edgecore.yaml\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# nohup  _output/local/bin/edgecore --edgecluster > edgecore.logs 2>&1 &\cf1\highlight2 
\par \cf0\highlight0 [1] 26367\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# tail -f edgecore.logs\cf1\highlight2 
\par 
\par \cf0\highlight0 I1221 11:08:55.833563   26367 core.go:24] Starting module websocket\cf1\highlight2 
\par \cf0\highlight0 I1221 11:08:55.833678   26367 core.go:24] Starting module metaManager\cf1\highlight2 
\par \cf0\highlight0 I1221 11:08:55.833855   26367 core.go:24] Starting module clusterd\cf1\highlight2 
\par \cf0\highlight0 I1221 11:08:55.834538   26367 clusterd.go:85] Starting clusterd...\cf1\highlight2 
\par \cf0\highlight0 I1221 11:08:55.834577   26367 clusterd.go:87] starting sync with cloud\cf1\highlight2 
\par \cf0\highlight0 I1221 11:08:55.835348   26367 certmanager.go:159] Certificate rotation is enabled.\cf1\highlight2 
\par \cf0\highlight0 I1221 11:08:55.835431   26367 websocket.go:51] Websocket start to connect Access\cf1\highlight2 
\par \cf0\highlight0 E1221 11:08:55.836853   26367 ws.go:78] dial websocket error(dial tcp 192.168.1.210:10000: connect: connection refused), response message:\cf1\highlight2 
\par \cf0\highlight0 E1221 11:08:55.836963   26367 websocket.go:90] Init websocket connection failed dial tcp 192.168.1.210:10000: connect: connection refused\cf1\highlight2 
\par \cf0\highlight0 E1221 11:09:00.837765   26367 ws.go:78] dial websocket error(dial tcp 192.168.1.210:10000: connect: connection refused), response message:\cf1\highlight2 
\par \cf0\highlight0 E1221 11:09:00.837915   26367 websocket.go:90] Init websocket connection failed dial tcp 192.168.1.210:10000: connect: connection refused\cf1\highlight2 
\par \cf0\highlight0 I1221 11:09:01.382726   26367 edgecluster_state_reporter.go:113] Attempting to register edgeCluster (node-d), default/edgeclusterstate/node-d\cf1\highlight2 
\par \cf0\highlight0 E1221 11:09:05.839591   26367 ws.go:78] dial websocket error(dial tcp 192.168.1.210:10000: connect: connection refused), response message:\cf1\highlight2 
\par \cf0\highlight0 E1221 11:09:05.839766   26367 websocket.go:90] Init websocket connection failed dial tcp 192.168.1.210:10000: connect: connection refused\cf1\highlight2 
\par \cf0\highlight0 E1221 11:09:10.840530   26367 ws.go:78] dial websocket error(dial tcp 192.168.1.210:10000: connect: connection refused), response message:\cf1\highlight2 
\par \cf0\highlight0 E1221 11:09:10.840680   26367 websocket.go:90] Init websocket connection failed dial tcp 192.168.1.210:10000: connect: connection refused\cf1\highlight2 
\par \cf0\highlight0 ^C\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# history\cf1\highlight2 
\par \cf0\highlight0     1  vi /etc/ssh/sshd_config\cf1\highlight2 
\par \cf0\highlight0     2  init 6\cf1\highlight2 
\par \cf0\highlight0     3  init 0\cf1\highlight2 
\par \cf0\highlight0     4  ip a\cf1\highlight2 
\par \cf0\highlight0     5  init 0\cf1\highlight2 
\par \cf0\highlight0     6  ip a\cf1\highlight2 
\par \cf0\highlight0     7  vim /etc/netplan/00-installer-config.yaml\cf1\highlight2 
\par \cf0\highlight0     8  netplan apply\cf1\highlight2 
\par \cf0\highlight0     9  init 0\cf1\highlight2 
\par \cf0\highlight0    10  vi /etc/hostname\cf1\highlight2 
\par \cf0\highlight0    11  vi /etc/hosts\cf1\highlight2 
\par \cf0\highlight0    12  init 6\cf1\highlight2 
\par \cf0\highlight0    13  sudo apt -y update && sudo apt -y install make && sudo apt -y install gcc && sudo apt -y install jq\cf1\highlight2 
\par \cf0\highlight0    14  sudo modprobe br_netfilter\cf1\highlight2 
\par \cf0\highlight0    15  lsmod | grep br_netfilter\cf1\highlight2 
\par \cf0\highlight0    16  cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf\cf1\highlight2 
\par \cf0\highlight0 br_netfilter\cf1\highlight2 
\par \cf0\highlight0 EOF\cf1\highlight2 
\par 
\par \cf0\highlight0    17  cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf\cf1\highlight2 
\par \cf0\highlight0 net.bridge.bridge-nf-call-ip6tables = 1\cf1\highlight2 
\par \cf0\highlight0 net.bridge.bridge-nf-call-iptables = 1\cf1\highlight2 
\par \cf0\highlight0 EOF\cf1\highlight2 
\par 
\par \cf0\highlight0    18  sudo sysctl --system\cf1\highlight2 
\par \cf0\highlight0    19  lsmod | grep br_netfilter\cf1\highlight2 
\par \cf0\highlight0    20  sudo apt-get update\cf1\highlight2 
\par \cf0\highlight0    21  sudo apt-get install docker.io\cf1\highlight2 
\par \cf0\highlight0    22  sudo apt-get update\cf1\highlight2 
\par \cf0\highlight0    23  sudo apt-get install -y apt-transport-https ca-certificates curl\cf1\highlight2 
\par \cf0\highlight0    24  sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg\cf1\highlight2 
\par \cf0\highlight0    25  echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/      kubernetes.list\cf1\highlight2 
\par \cf0\highlight0    26  sudo apt-get update\cf1\highlight2 
\par \cf0\highlight0    27  apt-get install -qy kubelet=1.21.1-00 kubectl=1.21.1-00 kubeadm=1.21.1-00\cf1\highlight2 
\par \cf0\highlight0    28  kubeadm init\cf1\highlight2 
\par \cf0\highlight0    29  systemctl status docker\cf1\highlight2 
\par \cf0\highlight0    30  systemctl start docker\cf1\highlight2 
\par \cf0\highlight0    31  systemctl enable docker\cf1\highlight2 
\par \cf0\highlight0    32  systemctl status containerd\cf1\highlight2 
\par \cf0\highlight0    33  systemctl enable containerd\cf1\highlight2 
\par \cf0\highlight0    34  kubeadm init\cf1\highlight2 
\par \cf0\highlight0    35  swapoff -a\cf1\highlight2 
\par \cf0\highlight0    36  kubeadm init\cf1\highlight2 
\par \cf0\highlight0    37  export KUBECONFIG=/etc/kubernetes/admin.conf\cf1\highlight2 
\par \cf0\highlight0    38  kubectl get nodes\cf1\highlight2 
\par \cf0\highlight0    39  export kubever=$(kubectl version | base64 | tr -d '\\n')\cf1\highlight2 
\par \cf0\highlight0    40  kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$kubever"\cf1\highlight2 
\par \cf0\highlight0    41  kubectl get nodes\cf1\highlight2 
\par \cf0\highlight0    42  GOLANG_VERSION=$\{GOLANG_VERSION:-"1.14.15"\}\cf1\highlight2 
\par \cf0\highlight0    43  sudo apt -y update\cf1\highlight2 
\par \cf0\highlight0    44  sudo tar -C /usr/local -xzf /tmp/go$\{GOLANG_VERSION\}.linux-amd64.tar.gz\cf1\highlight2 
\par \cf0\highlight0    45  sudo apt-get install vim\cf1\highlight2 
\par \cf0\highlight0    46  go version\cf1\highlight2 
\par \cf0\highlight0    47  vi ~/.bashrc\cf1\highlight2 
\par \cf0\highlight0    48  source ~/.bashrc\cf1\highlight2 
\par \cf0\highlight0    49  go version\cf1\highlight2 
\par \cf0\highlight0    50  go env\cf1\highlight2 
\par \cf0\highlight0    51  mkdir -p go/src/github.com\cf1\highlight2 
\par \cf0\highlight0    52  cd go/src/github.com\cf1\highlight2 
\par \cf0\highlight0    53  git clone https://github.com/CentaurusInfra/fornax.git\cf1\highlight2 
\par \cf0\highlight0    54  mv fornax kubeedge\cf1\highlight2 
\par \cf0\highlight0    55  cd kubeedge\cf1\highlight2 
\par \cf0\highlight0    56  make all\cf1\highlight2 
\par \cf0\highlight0    57  scp 192.168.2.52:/etc/kubernetes/admin.conf  .\cf1\highlight2 
\par \cf0\highlight0    58  ls\cf1\highlight2 
\par \cf0\highlight0    59  vi admin.conf\cf1\highlight2 
\par \cf0\highlight0    60  make WHAT=cloudcore\cf1\highlight2 
\par \cf0\highlight0    61  make WHAT=edgecoremake WHAT=cloudcore\cf1\highlight2 
\par \cf0\highlight0    62  make WHAT=edgecore\cf1\highlight2 
\par \cf0\highlight0    63  mkdir /etc/kubeedge/config -p\cf1\highlight2 
\par \cf0\highlight0    64  cp /etc/kubernetes/admin.conf /root/.kube/config\cf1\highlight2 
\par \cf0\highlight0    65  _output/local/bin/cloudcore --minconfig > /etc/kubeedge/config/cloudcore.yaml\cf1\highlight2 
\par \cf0\highlight0    66  mkdir /etc/kubeedge/config -p\cf1\highlight2 
\par \cf0\highlight0    67  mkdir /root/.kube/config -p\cf1\highlight2 
\par \cf0\highlight0    68  cd /etc/kubeedge/\cf1\highlight2 
\par \cf0\highlight0    69  ls\cf1\highlight2 
\par \cf0\highlight0    70  scp -r 192.168.2.50:/etc/kubeedge/certs .\cf1\highlight2 
\par \cf0\highlight0    71  scp -r 192.168.2.50:/etc/kubeedge/ca .\cf1\highlight2 
\par \cf0\highlight0    72  ls\cf1\highlight2 
\par \cf0\highlight0    73  cd ~/go/src/github.com/kubeedge\cf1\highlight2 
\par \cf0\highlight0    74  ls\cf1\highlight2 
\par \cf0\highlight0    75  export KUBECONFIG=/etc/kubernetes/admin.conf\cf1\highlight2 
\par \cf0\highlight0    76  _output/local/bin/edgecore --edgeclusterconfig > /etc/kubeedge/config/edgecore.yaml\cf1\highlight2 
\par \cf0\highlight0    77  tests/edgecluster/hack/update_edgecore_config.sh admin.conf\cf1\highlight2 
\par \cf0\highlight0    78  kubectl apply -f build/crds/devices/devices_v1alpha2_device.yaml\cf1\highlight2 
\par \cf0\highlight0    79  kubectl apply -f build/crds/devices/devices_v1alpha2_devicemodel.yaml\cf1\highlight2 
\par \cf0\highlight0    80  kubectl apply -f build/crds/reliablesyncs/cluster_objectsync_v1alpha1.yaml\cf1\highlight2 
\par \cf0\highlight0    81  kubectl apply -f build/crds/reliablesyncs/objectsync_v1alpha1.yaml\cf1\highlight2 
\par \cf0\highlight0    82  kubectl apply -f  build/crds/router/router_v1_rule.yaml\cf1\highlight2 
\par \cf0\highlight0    83  kubectl apply -f  build/crds/router/router_v1_ruleEndpoint.yaml\cf1\highlight2 
\par \cf0\highlight0    84  kubectl apply -f build/crds/edgecluster/mission_v1.yaml\cf1\highlight2 
\par \cf0\highlight0    85  kubectl apply -f build/crds/edgecluster/edgecluster_v1.yaml\cf1\highlight2 
\par \cf0\highlight0    86  cp /etc/kubernetes/admin.conf /root/.kube/config\cf1\highlight2 
\par \cf0\highlight0    87  _output/local/bin/cloudcore --minconfig > /etc/kubeedge/config/cloudcore.yaml\cf1\highlight2 
\par \cf0\highlight0    88  nohup  _output/local/bin/edgecore --edgecluster > edgecore.logs 2>&1 &\cf1\highlight2 
\par \cf0\highlight0    89  tail -f edgecore.logs\cf1\highlight2 
\par \cf0\highlight0    90  history\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge#  export KUBECONFIG=/etc/kubernetes/admin.conf\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# cd /etc/kubeedge/\cf1\highlight2 
\par \cf0\highlight0 root@node-d:/etc/kubeedge# ls\cf1\highlight2 
\par \cf12\highlight0 ca\cf0   \cf12 certs\cf0   \cf12 config\cf1\highlight2 
\par \cf0\highlight0 root@node-d:/etc/kubeedge# cd ca\cf1\highlight2 
\par \cf0\highlight0 root@node-d:/etc/kubeedge/ca# ls\cf1\highlight2 
\par \cf0\highlight0 rootCA.crt  rootCA.key  rootCA.srl\cf1\highlight2 
\par \cf0\highlight0 root@node-d:/etc/kubeedge/ca# cd ~/go/src/github.com/kubeedge\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ls\cf1\highlight2 
\par \cf0\highlight0 admin.conf   \cf12 CHANGELOG\cf0            \cf12 common\cf0            edgecore.logs           go.mod  LICENSE         Makefile  \cf12 pkg\cf0            \cf12 tests\cf1\highlight2 
\par \cf0\highlight0 ADOPTERS.md  CHANGELOG.md        CONTRIBUTING.md  \cf12 edgemesh\cf0                 go.sum  \cf12 LICENSES\cf0         \cf12 mappers\cf0    README.md     \cf12 vendor\cf1\highlight2 
\par \cf12\highlight0 binaries\cf0      \cf12 cloud\cf0                \cf12 docs\cf0              \cf12 edgesite\cf0                 \cf12 hack\cf0     MAINTAINERS     \cf12 _output\cf0    README_zh.md\cf1\highlight2 
\par \cf12\highlight0 build\cf0         CODE_OF_CONDUCT.md  \cf12 edge\cf0              external-dependency.md  \cf12 keadm\cf0    MAINTAINERS.md  OWNERS    \cf12 staging\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# vi admin.conf\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# rm -rf admin.conf\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# scp -r 192.168.2.50:/etc/kubernetes/admin.conf  .\cf1\highlight2 
\par \cf0\highlight0 root@192.168.2.50's password:\cf1\highlight2 
\par \cf0\highlight0 admin.conf                                                                                                                          100% 5596     3.2MB/s   00:00\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# rm -rf edgecore.logs\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge#\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# nohup  _output/local/bin/edgecore --edgecluster > edgecore.logs 2>&1 &\cf1\highlight2 
\par \cf0\highlight0 [2] 4657\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# tail -f edgecore.logs\cf1\highlight2 
\par \cf0\highlight0 I1221 11:14:41.163006    4657 core.go:24] Starting module websocket\cf1\highlight2 
\par \cf0\highlight0 I1221 11:14:41.163110    4657 core.go:24] Starting module metaManager\cf1\highlight2 
\par \cf0\highlight0 I1221 11:14:41.163206    4657 core.go:24] Starting module clusterd\cf1\highlight2 
\par \cf0\highlight0 I1221 11:14:41.163347    4657 clusterd.go:85] Starting clusterd...\cf1\highlight2 
\par \cf0\highlight0 I1221 11:14:41.163447    4657 clusterd.go:87] starting sync with cloud\cf1\highlight2 
\par \cf0\highlight0 I1221 11:14:41.164267    4657 certmanager.go:159] Certificate rotation is enabled.\cf1\highlight2 
\par \cf0\highlight0 I1221 11:14:41.164314    4657 websocket.go:51] Websocket start to connect Access\cf1\highlight2 
\par \cf0\highlight0 E1221 11:14:41.171478    4657 ws.go:78] dial websocket error(dial tcp 192.168.1.210:10000: connect: connection refused), response message:\cf1\highlight2 
\par \cf0\highlight0 E1221 11:14:41.171723    4657 websocket.go:90] Init websocket connection failed dial tcp 192.168.1.210:10000: connect: connection refused\cf1\highlight2 
\par \cf0\highlight0 I1221 11:14:42.167546    4657 edgecluster_state_reporter.go:113] Attempting to register edgeCluster (node-d), default/edgeclusterstate/node-d\cf1\highlight2 
\par \cf0\highlight0 E1221 11:14:46.172350    4657 ws.go:78] dial websocket error(dial tcp 192.168.1.210:10000: connect: connection refused), response message:\cf1\highlight2 
\par \cf0\highlight0 E1221 11:14:46.172485    4657 websocket.go:90] Init websocket connection failed dial tcp 192.168.1.210:10000: connect: connection refused\cf1\highlight2 
\par \cf0\highlight0 ^C\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# cd /etc/kubeedge/\cf1\highlight2 
\par \cf0\highlight0 root@node-d:/etc/kubeedge# ls\cf1\highlight2 
\par \cf12\highlight0 ca\cf0   \cf12 certs\cf0   \cf12 config\cf1\highlight2 
\par \cf0\highlight0 root@node-d:/etc/kubeedge# cdcca\cf1\highlight2 
\par \cf0\highlight0 cdcca: command not found\cf1\highlight2 
\par \cf0\highlight0 root@node-d:/etc/kubeedge# cd ca\cf1\highlight2 
\par \cf0\highlight0 root@node-d:/etc/kubeedge/ca# ls\cf1\highlight2 
\par \cf0\highlight0 rootCA.crt  rootCA.key  rootCA.srl\cf1\highlight2 
\par \cf0\highlight0 root@node-d:/etc/kubeedge/ca# ll\cf1\highlight2 
\par \cf0\highlight0 total 20\cf1\highlight2 
\par \cf0\highlight0 drwxr-xr-x 2 root root 4096 Dec 21 10:44 \cf12 .\cf0 /\cf1\highlight2 
\par \cf0\highlight0 drwxr-xr-x 5 root root 4096 Dec 21 10:44 \cf12 ..\cf0 /\cf1\highlight2 
\par \cf0\highlight0 -rw-r--r-- 1 root root 2078 Dec 21 10:44 rootCA.crt\cf1\highlight2 
\par \cf0\highlight0 -rw------- 1 root root 3311 Dec 21 10:44 rootCA.key\cf1\highlight2 
\par \cf0\highlight0 -rw-r--r-- 1 root root   41 Dec 21 10:44 rootCA.srl\cf1\highlight2 
\par \cf0\highlight0 root@node-d:/etc/kubeedge/ca# cd ../certs/\cf1\highlight2 
\par \cf0\highlight0 root@node-d:/etc/kubeedge/certs# LS\cf1\highlight2 
\par 
\par \cf0\highlight0 Command 'LS' not found, but can be installed with:\cf1\highlight2 
\par 
\par \cf0\highlight0 apt install sl\cf1\highlight2 
\par 
\par \cf0\highlight0 root@node-d:/etc/kubeedge/certs# ls\cf1\highlight2 
\par \cf0\highlight0 server.crt  server.csr  server.key\cf1\highlight2 
\par \cf0\highlight0 root@node-d:/etc/kubeedge/certs#\cf1\highlight2 
\par \cf0\highlight0 root@node-d:/etc/kubeedge/certs# nohup  _output/local/bin/edgecore --edgecluster > edgecore.logs 2>&1 &\cf1\highlight2 
\par \cf0\highlight0 [3] 28374\cf1\highlight2 
\par \cf0\highlight0 root@node-d:/etc/kubeedge/certs# tail -f edgecore.logs\cf1\highlight2 
\par \cf0\highlight0 nohup: ignoring input\cf1\highlight2 
\par \cf0\highlight0 nohup: failed to run command '_output/local/bin/edgecore': No such file or directory\cf1\highlight2 
\par \cf0\highlight0 cd ^C[3]+  Exit 127                nohup _output/local/bin/edgecore --edgecluster > edgecore.logs 2>&1\cf1\highlight2 
\par 
\par \cf0\highlight0 root@node-d:/etc/kubeedge/certs# cd ~/go/src/github.com/kubeedge\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ps -ef | grep edgecore.logs\cf1\highlight2 
\par \cf0\highlight0 root     30306  1899  0 11:22 pts/0    00:00:00 grep --color=auto \cf9 edgecore.logs\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ls\cf1\highlight2 
\par \cf0\highlight0 admin.conf   \cf12 CHANGELOG\cf0            \cf12 common\cf0            edgecore.logs           go.mod  LICENSE         Makefile  \cf12 pkg\cf0            \cf12 tests\cf1\highlight2 
\par \cf0\highlight0 ADOPTERS.md  CHANGELOG.md        CONTRIBUTING.md  \cf12 edgemesh\cf0                 go.sum  \cf12 LICENSES\cf0         \cf12 mappers\cf0    README.md     \cf12 vendor\cf1\highlight2 
\par \cf12\highlight0 binaries\cf0      \cf12 cloud\cf0                \cf12 docs\cf0              \cf12 edgesite\cf0                 \cf12 hack\cf0     MAINTAINERS     \cf12 _output\cf0    README_zh.md\cf1\highlight2 
\par \cf12\highlight0 build\cf0         CODE_OF_CONDUCT.md  \cf12 edge\cf0              external-dependency.md  \cf12 keadm\cf0    MAINTAINERS.md  OWNERS    \cf12 staging\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# rm -rf edgecore.logs\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# nohup  _output/local/bin/edgecore --edgecluster > edgecore.logs 2>&1 &\cf1\highlight2 
\par \cf0\highlight0 [3] 31336\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# tail -f edgecore.logs\cf1\highlight2 
\par \cf0\highlight0 I1221 11:22:16.765501   31336 core.go:24] Starting module websocket\cf1\highlight2 
\par \cf0\highlight0 I1221 11:22:16.765689   31336 core.go:24] Starting module metaManager\cf1\highlight2 
\par \cf0\highlight0 I1221 11:22:16.765774   31336 core.go:24] Starting module clusterd\cf1\highlight2 
\par \cf0\highlight0 I1221 11:22:16.765832   31336 clusterd.go:85] Starting clusterd...\cf1\highlight2 
\par \cf0\highlight0 I1221 11:22:16.765854   31336 clusterd.go:87] starting sync with cloud\cf1\highlight2 
\par \cf0\highlight0 I1221 11:22:16.767095   31336 certmanager.go:159] Certificate rotation is enabled.\cf1\highlight2 
\par \cf0\highlight0 I1221 11:22:16.767170   31336 websocket.go:51] Websocket start to connect Access\cf1\highlight2 
\par \cf0\highlight0 E1221 11:22:16.768989   31336 ws.go:78] dial websocket error(dial tcp 192.168.1.210:10000: connect: connection refused), response message:\cf1\highlight2 
\par \cf0\highlight0 E1221 11:22:16.769104   31336 websocket.go:90] Init websocket connection failed dial tcp 192.168.1.210:10000: connect: connection refused\cf1\highlight2 
\par \cf0\highlight0 I1221 11:22:17.527125   31336 edgecluster_state_reporter.go:113] Attempting to register edgeCluster (node-d), default/edgeclusterstate/node-d\cf1\highlight2 
\par \cf0\highlight0 E1221 11:22:21.769680   31336 ws.go:78] dial websocket error(dial tcp 192.168.1.210:10000: connect: connection refused), response message:\cf1\highlight2 
\par \cf0\highlight0 E1221 11:22:21.769777   31336 websocket.go:90] Init websocket connection failed dial tcp 192.168.1.210:10000: connect: connection refused\cf1\highlight2 
\par \cf0\highlight0 E1221 11:22:26.770438   31336 ws.go:78] dial websocket error(dial tcp 192.168.1.210:10000: connect: connection refused), response message:\cf1\highlight2 
\par \cf0\highlight0 E1221 11:22:26.770567   31336 websocket.go:90] Init websocket connection failed dial tcp 192.168.1.210:10000: connect: connection refused\cf1\highlight2 
\par \cf0\highlight0 ^C\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ls\cf1\highlight2 
\par \cf0\highlight0 admin.conf   \cf12 CHANGELOG\cf0            \cf12 common\cf0            edgecore.logs           go.mod  LICENSE         Makefile  \cf12 pkg\cf0            \cf12 tests\cf1\highlight2 
\par \cf0\highlight0 ADOPTERS.md  CHANGELOG.md        CONTRIBUTING.md  \cf12 edgemesh\cf0                 go.sum  \cf12 LICENSES\cf0         \cf12 mappers\cf0    README.md     \cf12 vendor\cf1\highlight2 
\par \cf12\highlight0 binaries\cf0      \cf12 cloud\cf0                \cf12 docs\cf0              \cf12 edgesite\cf0                 \cf12 hack\cf0     MAINTAINERS     \cf12 _output\cf0    README_zh.md\cf1\highlight2 
\par \cf12\highlight0 build\cf0         CODE_OF_CONDUCT.md  \cf12 edge\cf0              external-dependency.md  \cf12 keadm\cf0    MAINTAINERS.md  OWNERS    \cf12 staging\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ufw status\cf1\highlight2 
\par \cf0\highlight0 Status: inactive\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# swapoff -a\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# tail -f edgecore.logs\cf1\highlight2 
\par \cf0\highlight0 E1221 11:22:36.772398   31336 websocket.go:90] Init websocket connection failed dial tcp 192.168.1.210:10000: connect: connection refused\cf1\highlight2 
\par \cf0\highlight0 E1221 11:22:41.772614   31336 edgehub.go:89] connection failed: max retry count reached when connecting to cloud, will reconnect after 30s\cf1\highlight2 
\par \cf0\highlight0 I1221 11:23:11.772878   31336 websocket.go:51] Websocket start to connect Access\cf1\highlight2 
\par \cf0\highlight0 E1221 11:23:11.774121   31336 ws.go:78] dial websocket error(dial tcp 192.168.1.210:10000: connect: connection refused), response message:\cf1\highlight2 
\par \cf0\highlight0 E1221 11:23:11.774185   31336 websocket.go:90] Init websocket connection failed dial tcp 192.168.1.210:10000: connect: connection refused\cf1\highlight2 
\par \cf0\highlight0 E1221 11:23:16.774729   31336 ws.go:78] dial websocket error(dial tcp 192.168.1.210:10000: connect: connection refused), response message:\cf1\highlight2 
\par \cf0\highlight0 E1221 11:23:16.774833   31336 websocket.go:90] Init websocket connection failed dial tcp 192.168.1.210:10000: connect: connection refused\cf1\highlight2 
\par \cf0\highlight0 E1221 11:23:17.527355   31336 edgecluster_state_reporter.go:124] register edgeCluster failed, error: timeout to get response\cf1\highlight2 
\par \cf0\highlight0 E1221 11:23:17.527435   31336 edgecluster_state_reporter.go:126] response from cloud core: <nil>\cf1\highlight2 
\par \cf0\highlight0 E1221 11:23:17.527455   31336 edgecluster_state_reporter.go:192] Register edgeCluster failed: timeout to get response\cf1\highlight2 
\par \cf0\highlight0 E1221 11:23:21.775472   31336 ws.go:78] dial websocket error(dial tcp 192.168.1.210:10000: connect: connection refused), response message:\cf1\highlight2 
\par \cf0\highlight0 E1221 11:23:21.775561   31336 websocket.go:90] Init websocket connection failed dial tcp 192.168.1.210:10000: connect: connection refused\cf1\highlight2 
\par \cf0\highlight0 E1221 11:23:26.776288   31336 ws.go:78] dial websocket error(dial tcp 192.168.1.210:10000: connect: connection refused), response message:\cf1\highlight2 
\par \cf0\highlight0 E1221 11:23:26.776409   31336 websocket.go:90] Init websocket connection failed dial tcp 192.168.1.210:10000: connect: connection refused\cf1\highlight2 
\par \cf0\highlight0 ^C\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# vi admin.conf\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# rm -ef edgecore.logs\cf1\highlight2 
\par \cf0\highlight0 rm: invalid option -- 'e'\cf1\highlight2 
\par \cf0\highlight0 Try 'rm --help' for more information.\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# rm -rf edgecore.logs\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ps -ef | grep edgecore\cf1\highlight2 
\par \cf0\highlight0 root      4657  1899  0 11:14 pts/0    00:00:02 _output/local/bin/\cf9 edgecore\cf0  --edgecluster\cf1\highlight2 
\par \cf0\highlight0 root      7457  1899  0 11:24 pts/0    00:00:00 grep --color=auto \cf9 edgecore\cf1\highlight2 
\par \cf0\highlight0 root     26367  1899  0 11:08 pts/0    00:00:05 _output/local/bin/\cf9 edgecore\cf0  --edgecluster\cf1\highlight2 
\par \cf0\highlight0 root     31336  1899  0 11:22 pts/0    00:00:00 _output/local/bin/\cf9 edgecore\cf0  --edgecluster\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kill -9 26367\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kill -9 31336\cf1\highlight2 
\par \cf0\highlight0 [1]   Killed                  nohup _output/local/bin/edgecore --edgecluster > edgecore.logs 2>&1\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kill -9 4657\cf1\highlight2 
\par \cf0\highlight0 [3]+  Killed                  nohup _output/local/bin/edgecore --edgecluster > edgecore.logs 2>&1\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ps -ef | grep edgecore\cf1\highlight2 
\par \cf0\highlight0 root      9160  1899  0 11:24 pts/0    00:00:00 grep --color=auto \cf9 edgecore\cf1\highlight2 
\par \cf0\highlight0 [2]+  Killed                  nohup _output/local/bin/edgecore --edgecluster > edgecore.logs 2>&1\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ls\cf1\highlight2 
\par \cf0\highlight0 admin.conf   \cf12 build\cf0          \cf12 cloud\cf0                CONTRIBUTING.md  \cf12 edgemesh\cf0                 go.mod  \cf12 keadm\cf0      MAINTAINERS     \cf12 mappers\cf0   \cf12 pkg\cf0            \cf12 staging\cf1\highlight2 
\par \cf0\highlight0 ADOPTERS.md  \cf12 CHANGELOG\cf0      CODE_OF_CONDUCT.md  \cf12 docs\cf0              \cf12 edgesite\cf0                 go.sum  LICENSE   MAINTAINERS.md  \cf12 _output\cf0   README.md     \cf12 tests\cf1\highlight2 
\par \cf12\highlight0 binaries\cf0      CHANGELOG.md  \cf12 common\cf0               \cf12 edge\cf0              external-dependency.md  \cf12 hack\cf0     \cf12 LICENSES\cf0   Makefile        OWNERS   README_zh.md  \cf12 vendor\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# nohup  _output/local/bin/edgecore --edgecluster > edgecore.logs 2>&1 &\cf1\highlight2 
\par \cf0\highlight0 [1] 9320\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# tail -f edgecore.logs\cf1\highlight2 
\par \cf0\highlight0 I1221 11:24:49.323220    9320 core.go:24] Starting module clusterd\cf1\highlight2 
\par \cf0\highlight0 I1221 11:24:49.323423    9320 clusterd.go:85] Starting clusterd...\cf1\highlight2 
\par \cf0\highlight0 I1221 11:24:49.323456    9320 clusterd.go:87] starting sync with cloud\cf1\highlight2 
\par \cf0\highlight0 I1221 11:24:49.323816    9320 certmanager.go:159] Certificate rotation is enabled.\cf1\highlight2 
\par \cf0\highlight0 I1221 11:24:49.323905    9320 websocket.go:51] Websocket start to connect Access\cf1\highlight2 
\par \cf0\highlight0 E1221 11:24:49.325291    9320 ws.go:78] dial websocket error(dial tcp 192.168.1.210:10000: connect: connection refused), response message:\cf1\highlight2 
\par \cf0\highlight0 E1221 11:24:49.326830    9320 websocket.go:90] Init websocket connection failed dial tcp 192.168.1.210:10000: connect: connection refused\cf1\highlight2 
\par \cf0\highlight0 I1221 11:24:50.150563    9320 edgecluster_state_reporter.go:113] Attempting to register edgeCluster (node-d), default/edgeclusterstate/node-d\cf1\highlight2 
\par \cf0\highlight0 E1221 11:24:54.327695    9320 ws.go:78] dial websocket error(dial tcp 192.168.1.210:10000: connect: connection refused), response message:\cf1\highlight2 
\par \cf0\highlight0 E1221 11:24:54.327795    9320 websocket.go:90] Init websocket connection failed dial tcp 192.168.1.210:10000: connect: connection refused\cf1\highlight2 
\par \cf0\highlight0 E1221 11:24:59.328381    9320 ws.go:78] dial websocket error(dial tcp 192.168.1.210:10000: connect: connection refused), response message:\cf1\highlight2 
\par \cf0\highlight0 E1221 11:24:59.328484    9320 websocket.go:90] Init websocket connection failed dial tcp 192.168.1.210:10000: connect: connection refused\cf1\highlight2 
\par \cf0\highlight0 ^C\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ps -ef | grep edgecore\cf1\highlight2 
\par \cf0\highlight0 root      9320  1899  2 11:24 pts/0    00:00:00 _output/local/bin/\cf9 edgecore\cf0  --edgecluster\cf1\highlight2 
\par \cf0\highlight0 root      9902  1899  0 11:25 pts/0    00:00:00 grep --color=auto \cf9 edgecore\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kill -9 9320\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# rm -rf edgecore.logs\cf1\highlight2 
\par \cf0\highlight0 [1]+  Killed                  nohup _output/local/bin/edgecore --edgecluster > edgecore.logs 2>&1\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# mkdir /etc/kubeedge/config -p\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# cp /etc/kubernetes/admin.conf /root/.kube/config\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# _output/local/bin/cloudcore --minconfig > /etc/kubeedge/config/cloudcore.yaml\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ls\cf1\highlight2 
\par \cf0\highlight0 admin.conf   \cf12 build\cf0          \cf12 cloud\cf0                CONTRIBUTING.md  \cf12 edgemesh\cf0                 go.mod  \cf12 keadm\cf0      MAINTAINERS     \cf12 mappers\cf0   \cf12 pkg\cf0            \cf12 staging\cf1\highlight2 
\par \cf0\highlight0 ADOPTERS.md  \cf12 CHANGELOG\cf0      CODE_OF_CONDUCT.md  \cf12 docs\cf0              \cf12 edgesite\cf0                 go.sum  LICENSE   MAINTAINERS.md  \cf12 _output\cf0   README.md     \cf12 tests\cf1\highlight2 
\par \cf12\highlight0 binaries\cf0      CHANGELOG.md  \cf12 common\cf0               \cf12 edge\cf0              external-dependency.md  \cf12 hack\cf0     \cf12 LICENSES\cf0   Makefile        OWNERS   README_zh.md  \cf12 vendor\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# vi adminc\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# vi admin.conf\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# make WHAT=cloudcore\cf1\highlight2 
\par \cf0\highlight0 hack/verify-golang.sh\cf1\highlight2 
\par \cf0\highlight0 go detail version: go version go1.14.15 linux/amd64\cf1\highlight2 
\par \cf0\highlight0 go version: 1.14.15\cf1\highlight2 
\par \cf0\highlight0 KUBEEDGE_OUTPUT_SUBPATH=_output/local hack/make-rules/build.sh cloudcore\cf1\highlight2 
\par \cf0\highlight0 building github.com/kubeedge/kubeedge/cloud/cmd/cloudcore\cf1\highlight2 
\par \cf0\highlight0 + go build -o /root/go/src/github.com/kubeedge/_output/local/bin/cloudcore -gcflags= -ldflags '-s -w -buildid= -X github.com/kubeedge/kubeedge/pkg/version.buildDate=2      021-12-21T11:59:25Z -X github.com/kubeedge/kubeedge/pkg/version.gitCommit=aa964bae0c857b761262e2c44e68d6eb0ff6202d -X github.com/kubeedge/kubeedge/pkg/version.gitTree      State=dirty -X github.com/kubeedge/kubeedge/pkg/version.gitVersion=v0.2-16+aa964bae0c857b-dirty -X github.com/kubeedge/kubeedge/pkg/version.gitMajor=0 -X github.com/k      ubeedge/kubeedge/pkg/version.gitMinor=2+' github.com/kubeedge/kubeedge/cloud/cmd/cloudcore\cf1\highlight2 
\par \cf0\highlight0 + set +x\cf1\highlight2 
\par \cf0\highlight0 yes | cp -r binaries/kubectl _output/local/bin/\cf1\highlight2 
\par \cf0\highlight0 mkdir -p _output/local/bin/crds\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/devices/devices_v1alpha2_device.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/devices/devices_v1alpha2_devicemodel.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/reliablesyncs/cluster_objectsync_v1alpha1.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/reliablesyncs/objectsync_v1alpha1.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/router/router_v1_rule.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/router/router_v1_ruleEndpoint.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/edgecluster/mission_v1.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/edgecluster/edgecluster_v1.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# make WHAT=edgecore\cf1\highlight2 
\par \cf0\highlight0 hack/verify-golang.sh\cf1\highlight2 
\par \cf0\highlight0 go detail version: go version go1.14.15 linux/amd64\cf1\highlight2 
\par \cf0\highlight0 go version: 1.14.15\cf1\highlight2 
\par \cf0\highlight0 KUBEEDGE_OUTPUT_SUBPATH=_output/local hack/make-rules/build.sh edgecore\cf1\highlight2 
\par \cf0\highlight0 building github.com/kubeedge/kubeedge/edge/cmd/edgecore\cf1\highlight2 
\par \cf0\highlight0 + go build -o /root/go/src/github.com/kubeedge/_output/local/bin/edgecore -gcflags= -ldflags '-s -w -buildid= -X github.com/kubeedge/kubeedge/pkg/version.buildDate=20      21-12-21T11:59:44Z -X github.com/kubeedge/kubeedge/pkg/version.gitCommit=aa964bae0c857b761262e2c44e68d6eb0ff6202d -X github.com/kubeedge/kubeedge/pkg/version.gitTreeS      tate=dirty -X github.com/kubeedge/kubeedge/pkg/version.gitVersion=v0.2-16+aa964bae0c857b-dirty -X github.com/kubeedge/kubeedge/pkg/version.gitMajor=0 -X github.com/ku      beedge/kubeedge/pkg/version.gitMinor=2+' github.com/kubeedge/kubeedge/edge/cmd/edgecore\cf1\highlight2 
\par \cf0\highlight0 + set +x\cf1\highlight2 
\par \cf0\highlight0 yes | cp -r binaries/kubectl _output/local/bin/\cf1\highlight2 
\par \cf0\highlight0 mkdir -p _output/local/bin/crds\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/devices/devices_v1alpha2_device.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/devices/devices_v1alpha2_devicemodel.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/reliablesyncs/cluster_objectsync_v1alpha1.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/reliablesyncs/objectsync_v1alpha1.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/router/router_v1_rule.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/router/router_v1_ruleEndpoint.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/edgecluster/mission_v1.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/edgecluster/edgecluster_v1.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# mkdir -p /etc/kubeedge/ca\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge#  mkdir -p /etc/kubeedge/certs\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# cd /etc/kubee\cf1\highlight2 
\par \cf0\highlight0 -bash: cd: /etc/kubee: No such file or directory\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# cd /etc/kubeedge/\cf1\highlight2 
\par \cf0\highlight0 root@node-d:/etc/kubeedge# ls\cf1\highlight2 
\par \cf12\highlight0 ca\cf0   \cf12 certs\cf0   \cf12 config\cf1\highlight2 
\par \cf0\highlight0 root@node-d:/etc/kubeedge# vi config\cf1\highlight2 
\par \cf0\highlight0 root@node-d:/etc/kubeedge# rm -rf ca certs\cf1\highlight2 
\par \cf0\highlight0 root@node-d:/etc/kubeedge# scp -r 192.168.2.50:/etc/kubeedge/ca /etc/kubeedge\cf1\highlight2 
\par \cf0\highlight0 root@192.168.2.50's password:\cf1\highlight2 
\par \cf0\highlight0 rootCA.crt                                                                                                                          100% 2078     1.9MB/s   00:00\cf1\highlight2 
\par \cf0\highlight0 rootCA.key                                                                                                                          100% 3311     2.9MB/s   00:00\cf1\highlight2 
\par \cf0\highlight0 rootCA.srl                                                                                                                          100%   41    46.2KB/s   00:00\cf1\highlight2 
\par \cf0\highlight0 root@node-d:/etc/kubeedge# scp -r 192.168.2.50:/etc/kubeedge/certs /etc/kubeedge\cf1\highlight2 
\par \cf0\highlight0 root@192.168.2.50's password:\cf1\highlight2 
\par \cf0\highlight0 server.crt                                                                                                                          100% 1619     1.2MB/s   00:00\cf1\highlight2 
\par \cf0\highlight0 server.csr                                                                                                                          100%  989   984.3KB/s   00:00\cf1\highlight2 
\par \cf0\highlight0 server.key                                                                                                                          100% 1679     1.3MB/s   00:00\cf1\highlight2 
\par \cf0\highlight0 root@node-d:/etc/kubeedge# mkdir /etc/kubeedge/config -p\cf1\highlight2 
\par \cf0\highlight0 root@node-d:/etc/kubeedge# rm -rf config/\cf1\highlight2 
\par \cf0\highlight0 root@node-d:/etc/kubeedge# ls\cf1\highlight2 
\par \cf12\highlight0 ca\cf0   \cf12 certs\cf1\highlight2 
\par \cf0\highlight0 root@node-d:/etc/kubeedge# cd ~/go/src/github.com/kubeedge\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# cp /etc/kubernetes/admin.conf /root/edgecluster.kubeconfig\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# _output/local/bin/edgecore --edgeclusterconfig > /etc/kubeedge/config/edgecore.yaml\cf1\highlight2 
\par \cf0\highlight0 -bash: /etc/kubeedge/config/edgecore.yaml: No such file or directory\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# mkdir /etc/kubeedge/config -p\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# cp /etc/kubernetes/admin.conf /root/edgecluster.kubeconfig\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# _output/local/bin/edgecore --edgeclusterconfig > /etc/kubeedge/config/edgecore.yaml\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# tests/edgecluster/hack/update_edgecore_config.sh admin.conf\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# chmod 777 /root/go/src/github.com/kubeedge/_output/local/bin/kubectl/vanilla/kubectl\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# mkdir /root/.kube/config -p\cf1\highlight2 
\par \cf0\highlight0\f1 mkdir: cannot create directory \lquote /root/.kube/config\rquote : File exists\cf1\highlight2\f0 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# cp /etc/kubernetes/admin.conf /root/.kube/config\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# _output/local/bin/cloudcore --minconfig > /etc/kubeedge/config/cloudcore.yaml\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ls\cf1\highlight2 
\par \cf0\highlight0 admin.conf   \cf12 build\cf0          \cf12 cloud\cf0                CONTRIBUTING.md  \cf12 edgemesh\cf0                 go.mod  \cf12 keadm\cf0      MAINTAINERS     \cf12 mappers\cf0   \cf12 pkg\cf0            \cf12 staging\cf1\highlight2 
\par \cf0\highlight0 ADOPTERS.md  \cf12 CHANGELOG\cf0      CODE_OF_CONDUCT.md  \cf12 docs\cf0              \cf12 edgesite\cf0                 go.sum  LICENSE   MAINTAINERS.md  \cf12 _output\cf0   README.md     \cf12 tests\cf1\highlight2 
\par \cf12\highlight0 binaries\cf0      CHANGELOG.md  \cf12 common\cf0               \cf12 edge\cf0              external-dependency.md  \cf12 hack\cf0     \cf12 LICENSES\cf0   Makefile        OWNERS   README_zh.md  \cf12 vendor\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# export KUBECONFIG=/etc/kubernetes/admin.conf\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ls\cf1\highlight2 
\par \cf0\highlight0 admin.conf   \cf12 build\cf0          \cf12 cloud\cf0                CONTRIBUTING.md  \cf12 edgemesh\cf0                 go.mod  \cf12 keadm\cf0      MAINTAINERS     \cf12 mappers\cf0   \cf12 pkg\cf0            \cf12 staging\cf1\highlight2 
\par \cf0\highlight0 ADOPTERS.md  \cf12 CHANGELOG\cf0      CODE_OF_CONDUCT.md  \cf12 docs\cf0              \cf12 edgesite\cf0                 go.sum  LICENSE   MAINTAINERS.md  \cf12 _output\cf0   README.md     \cf12 tests\cf1\highlight2 
\par \cf12\highlight0 binaries\cf0      CHANGELOG.md  \cf12 common\cf0               \cf12 edge\cf0              external-dependency.md  \cf12 hack\cf0     \cf12 LICENSES\cf0   Makefile        OWNERS   README_zh.md  \cf12 vendor\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# nohup  _output/local/bin/cloudcore > cloudcore.logs 2>&1 &\cf1\highlight2 
\par \cf0\highlight0 [1] 31277\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# tail -f cloudcore.logs\cf1\highlight2 
\par \cf0\highlight0 I1221 12:06:28.919358   31277 core.go:24] Starting module edgecontroller\cf1\highlight2 
\par \cf0\highlight0 I1221 12:06:28.919445   31277 core.go:24] Starting module devicecontroller\cf1\highlight2 
\par \cf0\highlight0 I1221 12:06:28.919960   31277 upstream.go:123] start upstream controller\cf1\highlight2 
\par \cf0\highlight0 I1221 12:06:28.920268   31277 downstream.go:446] start downstream controller\cf1\highlight2 
\par \cf0\highlight0 I1221 12:06:28.922872   31277 downstream.go:873] Start downstream devicecontroller\cf1\highlight2 
\par \cf0\highlight0 I1221 12:06:29.047787   31277 signcerts.go:100] Succeed to creating token\cf1\highlight2 
\par \cf0\highlight0 I1221 12:06:29.047916   31277 server.go:44] start unix domain socket server\cf1\highlight2 
\par \cf0\highlight0 I1221 12:06:29.048327   31277 uds.go:71] listening on: //var/lib/kubeedge/kubeedge.sock\cf1\highlight2 
\par \cf0\highlight0 I1221 12:06:29.049440   31277 server.go:64] Starting cloudhub websocket server\cf1\highlight2 
\par \cf0\highlight0 I1221 12:06:30.924252   31277 upstream.go:63] Start upstream devicecontroller\cf1\highlight2 
\par \cf0\highlight0 ^C\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge#\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# nohup  _output/local/bin/edgecore --edgecluster > edgecore.logs 2>&1 &\cf1\highlight2 
\par \cf0\highlight0 [2] 31502\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# tail -f edgecore.logs\cf1\highlight2 
\par \cf0\highlight0 I1221 12:06:54.203879   31502 ws.go:46] dial wss://192.168.2.50:10000/e632aba927ea4ac2b575ec1603d56f10/node-d/events successfully\cf1\highlight2 
\par \cf0\highlight0 I1221 12:06:54.204314   31502 websocket.go:93] Websocket connect to cloud access successful\cf1\highlight2 
\par \cf0\highlight0 W1221 12:06:54.204392   31502 context_channel.go:335] Failed to get type channel, type:twin\cf1\highlight2 
\par \cf0\highlight0 W1221 12:06:54.204426   31502 context_channel.go:184] Get bad module type:twin when sendToGroup message, do nothing\cf1\highlight2 
\par \cf0\highlight0 W1221 12:06:54.204483   31502 context_channel.go:335] Failed to get type channel, type:bus\cf1\highlight2 
\par \cf0\highlight0 W1221 12:06:54.204527   31502 context_channel.go:184] Get bad module type:bus when sendToGroup message, do nothing\cf1\highlight2 
\par \cf0\highlight0 I1221 12:06:54.208336   31502 process.go:411] node connection event occur: cloud_connected\cf1\highlight2 
\par \cf0\highlight0 I1221 12:06:54.210143   31502 process.go:411] node connection event occur: cloud_connected\cf1\highlight2 
\par \cf0\highlight0 I1221 12:06:56.475676   31502 edgecluster_state_reporter.go:113] Attempting to register edgeCluster (node-d), default/edgeclusterstate/node-d\cf1\highlight2 
\par \cf0\highlight0 I1221 12:06:56.494213   31502 edgecluster_state_reporter.go:131] Successfully registered edgeCluster node-d\cf1\highlight2 
\par \cf0\highlight0 ^C\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ^C\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ^C\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ls\cf1\highlight2 
\par \cf0\highlight0 admin.conf   \cf12 CHANGELOG\cf0        CODE_OF_CONDUCT.md  \cf12 edge\cf0            external-dependency.md  \cf12 keadm\cf0         MAINTAINERS.md  OWNERS        \cf12 staging\cf1\highlight2 
\par \cf0\highlight0 ADOPTERS.md  CHANGELOG.md    \cf12 common\cf0               edgecore.logs  go.mod                  LICENSE      Makefile        \cf12 pkg\cf0            \cf12 tests\cf1\highlight2 
\par \cf12\highlight0 binaries\cf0      \cf12 cloud\cf0            CONTRIBUTING.md     \cf12 edgemesh\cf0        go.sum                  \cf12 LICENSES\cf0      \cf12 mappers\cf0          README.md     \cf12 vendor\cf1\highlight2 
\par \cf12\highlight0 build\cf0         cloudcore.logs  \cf12 docs\cf0                 \cf12 edgesite\cf0        \cf12 hack\cf0                     MAINTAINERS  \cf12 _output\cf0          README_zh.md\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# vi admin.conf\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# tail -f cloudcore.logs\cf1\highlight2 
\par \cf0\highlight0 I1221 12:06:28.919358   31277 core.go:24] Starting module edgecontroller\cf1\highlight2 
\par \cf0\highlight0 I1221 12:06:28.919445   31277 core.go:24] Starting module devicecontroller\cf1\highlight2 
\par \cf0\highlight0 I1221 12:06:28.919960   31277 upstream.go:123] start upstream controller\cf1\highlight2 
\par \cf0\highlight0 I1221 12:06:28.920268   31277 downstream.go:446] start downstream controller\cf1\highlight2 
\par \cf0\highlight0 I1221 12:06:28.922872   31277 downstream.go:873] Start downstream devicecontroller\cf1\highlight2 
\par \cf0\highlight0 I1221 12:06:29.047787   31277 signcerts.go:100] Succeed to creating token\cf1\highlight2 
\par \cf0\highlight0 I1221 12:06:29.047916   31277 server.go:44] start unix domain socket server\cf1\highlight2 
\par \cf0\highlight0 I1221 12:06:29.048327   31277 uds.go:71] listening on: //var/lib/kubeedge/kubeedge.sock\cf1\highlight2 
\par \cf0\highlight0 I1221 12:06:29.049440   31277 server.go:64] Starting cloudhub websocket server\cf1\highlight2 
\par \cf0\highlight0 I1221 12:06:30.924252   31277 upstream.go:63] Start upstream devicecontroller\cf1\highlight2 
\par \cf0\highlight0 ^C\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# rm -rf cloudcore.logs edgecore.logs\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ls\cf1\highlight2 
\par \cf0\highlight0 admin.conf   \cf12 build\cf0          \cf12 cloud\cf0                CONTRIBUTING.md  \cf12 edgemesh\cf0                 go.mod  \cf12 keadm\cf0      MAINTAINERS     \cf12 mappers\cf0   \cf12 pkg\cf0            \cf12 staging\cf1\highlight2 
\par \cf0\highlight0 ADOPTERS.md  \cf12 CHANGELOG\cf0      CODE_OF_CONDUCT.md  \cf12 docs\cf0              \cf12 edgesite\cf0                 go.sum  LICENSE   MAINTAINERS.md  \cf12 _output\cf0   README.md     \cf12 tests\cf1\highlight2 
\par \cf12\highlight0 binaries\cf0      CHANGELOG.md  \cf12 common\cf0               \cf12 edge\cf0              external-dependency.md  \cf12 hack\cf0     \cf12 LICENSES\cf0   Makefile        OWNERS   README_zh.md  \cf12 vendor\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ps -ef cloudcore\cf1\highlight2 
\par \cf0\highlight0 error: unknown user-defined format specifier "udcore"\cf1\highlight2 
\par 
\par \cf0\highlight0 Usage:\cf1\highlight2 
\par \cf0\highlight0  ps [options]\cf1\highlight2 
\par 
\par \cf0\highlight0  Try 'ps --help <simple|list|output|threads|misc|all>'\cf1\highlight2 
\par \cf0\highlight0   or 'ps --help <s|l|o|t|m|a>'\cf1\highlight2 
\par \cf0\highlight0  for additional help text.\cf1\highlight2 
\par 
\par \cf0\highlight0 For more details see ps(1).\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ps -ef | grepcloudcore\cf1\highlight2 
\par \cf0\highlight0 grepcloudcore: command not found\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ps -ef | grep cloudcore\cf1\highlight2 
\par \cf0\highlight0 root     24003  1899  0 12:17 pts/0    00:00:00 grep --color=auto \cf9 cloudcore\cf1\highlight2 
\par \cf0\highlight0 root     31277  1899  0 12:06 pts/0    00:00:00 _output/local/bin/\cf9 cloudcore\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kill -9 331277\cf1\highlight2 
\par \cf0\highlight0 -bash: kill: (331277) - No such process\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kill -9 31277\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# pas -ef | grep edgecore\cf1\highlight2 
\par 
\par \cf0\highlight0 Command 'pas' not found, but there are 20 similar ones.\cf1\highlight2 
\par 
\par \cf0\highlight0 [1]-  Killed                  nohup _output/local/bin/cloudcore > cloudcore.logs 2>&1\cf1\highlight2 
\par \cf0\highlight0 [1]-  Killed                  nohup _output/local/bin/cloudcore > cloudcore.logs 2>&1\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# pas -ef | grep edgecore\cf1\highlight2 
\par 
\par \cf0\highlight0 Command 'pas' not found, but there are 20 similar ones.\cf1\highlight2 
\par 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ps -ef | grep edgecore\cf1\highlight2 
\par \cf0\highlight0 root     25687  1899  0 12:17 pts/0    00:00:00 grep --color=auto \cf9 edgecore\cf1\highlight2 
\par \cf0\highlight0 root     31502  1899  0 12:06 pts/0    00:00:05 _output/local/bin/\cf9 edgecore\cf0  --edgecluster\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kill -9 31502\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# rm -rf admin.conf\cf1\highlight2 
\par \cf0\highlight0 [2]+  Killed                  nohup _output/local/bin/edgecore --edgecluster > edgecore.logs 2>&1\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ls\cf1\highlight2 
\par \cf0\highlight0 ADOPTERS.md  \cf12 CHANGELOG\cf0      CODE_OF_CONDUCT.md  \cf12 docs\cf0       \cf12 edgesite\cf0                 go.sum  LICENSE      MAINTAINERS.md  \cf12 _output\cf0   README.md     \cf12 tests\cf1\highlight2 
\par \cf12\highlight0 binaries\cf0      CHANGELOG.md  \cf12 common\cf0               \cf12 edge\cf0       external-dependency.md  \cf12 hack\cf0     \cf12 LICENSES\cf0      Makefile        OWNERS   README_zh.md  \cf12 vendor\cf1\highlight2 
\par \cf12\highlight0 build\cf0         \cf12 cloud\cf0          CONTRIBUTING.md     \cf12 edgemesh\cf0   go.mod                  \cf12 keadm\cf0    MAINTAINERS  \cf12 mappers\cf0          \cf12 pkg\cf0       \cf12 staging\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# scp 192.168.2.52:/etc/kubernetes/admin.conf .\cf1\highlight2 
\par \cf0\highlight0 root@192.168.2.52's password:\cf1\highlight2 
\par \cf0\highlight0 admin.conf                                                                                                                          100% 5596     3.5MB/s   00:00\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# cp /etc/kubernetes/admin.conf /root/edgecluster.kubeconfig\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge#  _output/local/bin/edgecore --edgeclusterconfig > /etc/kubeedge/config/edgecore.yaml\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge#\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge#    48  mv fornax kubeedge\cf1\highlight2 
\par \cf0\highlight0 48: command not found\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# tests/edgecluster/hack/update_edgecore_config.sh admin.conf\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# chmod 777 /root/go/src/github.com/kubeedge/_output/local/bin/kubectl/vanilla/kubectl\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# export KUBECONFIG=/etc/kubernetes/admin.conf\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# cp /etc/kubernetes/admin.conf /root/.kube/config\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# _output/local/bin/cloudcore --minconfig > /etc/kubeedge/config/cloudcore.yaml\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# nohup  _output/local/bin/cloudcore > cloudcore.logs 2>&1 &\cf1\highlight2 
\par \cf0\highlight0 [1] 27689\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# tail -f cloudcore.logs\cf1\highlight2 
\par \cf0\highlight0 I1221 12:21:36.952217   27689 core.go:24] Starting module synccontroller\cf1\highlight2 
\par \cf0\highlight0 I1221 12:21:36.952324   27689 core.go:24] Starting module missionstatepruner\cf1\highlight2 
\par \cf0\highlight0 I1221 12:21:36.952334   27689 downstream.go:873] Start downstream devicecontroller\cf1\highlight2 
\par \cf0\highlight0 I1221 12:21:36.952367   27689 downstream.go:446] start downstream controller\cf1\highlight2 
\par \cf0\highlight0 I1221 12:21:36.952430   27689 core.go:24] Starting module cloudhub\cf1\highlight2 
\par \cf0\highlight0 I1221 12:21:37.110344   27689 signcerts.go:100] Succeed to creating token\cf1\highlight2 
\par \cf0\highlight0 I1221 12:21:37.110452   27689 server.go:44] start unix domain socket server\cf1\highlight2 
\par \cf0\highlight0 I1221 12:21:37.110903   27689 uds.go:71] listening on: //var/lib/kubeedge/kubeedge.sock\cf1\highlight2 
\par \cf0\highlight0 I1221 12:21:37.111400   27689 server.go:64] Starting cloudhub websocket server\cf1\highlight2 
\par \cf0\highlight0 I1221 12:21:38.952740   27689 upstream.go:63] Start upstream devicecontroller\cf1\highlight2 
\par \cf0\highlight0 ^C\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge#\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# nohup  _output/local/bin/edgecore --edgecluster > edgecore.logs 2>&1 &\cf1\highlight2 
\par \cf0\highlight0 [2] 27861\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# tail -f edgecore.logs\cf1\highlight2 
\par \cf0\highlight0 I1221 12:21:54.938459   27861 ws.go:46] dial wss://192.168.2.52:10000/e632aba927ea4ac2b575ec1603d56f10/node-d/events successfully\cf1\highlight2 
\par \cf0\highlight0 I1221 12:21:54.938712   27861 websocket.go:93] Websocket connect to cloud access successful\cf1\highlight2 
\par \cf0\highlight0 W1221 12:21:54.938781   27861 context_channel.go:335] Failed to get type channel, type:twin\cf1\highlight2 
\par \cf0\highlight0 W1221 12:21:54.938813   27861 context_channel.go:184] Get bad module type:twin when sendToGroup message, do nothing\cf1\highlight2 
\par \cf0\highlight0 W1221 12:21:54.938866   27861 context_channel.go:335] Failed to get type channel, type:bus\cf1\highlight2 
\par \cf0\highlight0 W1221 12:21:54.938887   27861 context_channel.go:184] Get bad module type:bus when sendToGroup message, do nothing\cf1\highlight2 
\par \cf0\highlight0 I1221 12:21:54.939057   27861 process.go:411] node connection event occur: cloud_connected\cf1\highlight2 
\par \cf0\highlight0 I1221 12:21:54.939379   27861 process.go:411] node connection event occur: cloud_connected\cf1\highlight2 
\par \cf0\highlight0 I1221 12:21:55.966801   27861 edgecluster_state_reporter.go:113] Attempting to register edgeCluster (node-d), default/edgeclusterstate/node-d\cf1\highlight2 
\par \cf0\highlight0 I1221 12:21:55.985823   27861 edgecluster_state_reporter.go:131] Successfully registered edgeCluster node-d\cf1\highlight2 
\par \cf0\highlight0 ^C\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ls\cf1\highlight2 
\par \cf0\highlight0 admin.conf   \cf12 build\cf0          \cf12 cloud\cf0                \cf12 common\cf0            \cf12 edge\cf0            \cf12 edgesite\cf0                 go.sum  LICENSE      MAINTAINERS.md  \cf12 _output\cf0   README.md     \cf12 tests\cf1\highlight2 
\par \cf0\highlight0 ADOPTERS.md  \cf12 CHANGELOG\cf0      cloudcore.logs      CONTRIBUTING.md  edgecore.logs  external-dependency.md  \cf12 hack\cf0     \cf12 LICENSES\cf0      Makefile        OWNERS   README_zh.md  \cf12 vendor\cf1\highlight2 
\par \cf12\highlight0 binaries\cf0      CHANGELOG.md  CODE_OF_CONDUCT.md  \cf12 docs\cf0              \cf12 edgemesh\cf0        go.mod                  \cf12 keadm\cf0    MAINTAINERS  \cf12 mappers\cf0          \cf12 pkg\cf0       \cf12 staging\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# vi admin.conf\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# history\cf1\highlight2 
\par \cf0\highlight0     1  vi /etc/ssh/sshd_config\cf1\highlight2 
\par \cf0\highlight0     2  init 6\cf1\highlight2 
\par \cf0\highlight0     3  init 0\cf1\highlight2 
\par \cf0\highlight0     4  ip a\cf1\highlight2 
\par \cf0\highlight0     5  init 0\cf1\highlight2 
\par \cf0\highlight0     6  ip a\cf1\highlight2 
\par \cf0\highlight0     7  vim /etc/netplan/00-installer-config.yaml\cf1\highlight2 
\par \cf0\highlight0     8  netplan apply\cf1\highlight2 
\par \cf0\highlight0     9  init 0\cf1\highlight2 
\par \cf0\highlight0    10  vi /etc/hostname\cf1\highlight2 
\par \cf0\highlight0    11  vi /etc/hosts\cf1\highlight2 
\par \cf0\highlight0    12  init 6\cf1\highlight2 
\par \cf0\highlight0    13  sudo apt -y update && sudo apt -y install make && sudo apt -y install gcc && sudo apt -y install jq\cf1\highlight2 
\par \cf0\highlight0    14  sudo modprobe br_netfilter\cf1\highlight2 
\par \cf0\highlight0    15  lsmod | grep br_netfilter\cf1\highlight2 
\par \cf0\highlight0    16  cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf\cf1\highlight2 
\par \cf0\highlight0 br_netfilter\cf1\highlight2 
\par \cf0\highlight0 EOF\cf1\highlight2 
\par 
\par \cf0\highlight0    17  cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf\cf1\highlight2 
\par \cf0\highlight0 net.bridge.bridge-nf-call-ip6tables = 1\cf1\highlight2 
\par \cf0\highlight0 net.bridge.bridge-nf-call-iptables = 1\cf1\highlight2 
\par \cf0\highlight0 EOF\cf1\highlight2 
\par 
\par \cf0\highlight0    18  sudo sysctl --system\cf1\highlight2 
\par \cf0\highlight0    19  lsmod | grep br_netfilter\cf1\highlight2 
\par \cf0\highlight0    20  sudo apt-get update\cf1\highlight2 
\par \cf0\highlight0    21  sudo apt-get install docker.io\cf1\highlight2 
\par \cf0\highlight0    22  sudo apt-get update\cf1\highlight2 
\par \cf0\highlight0    23  sudo apt-get install -y apt-transport-https ca-certificates curl\cf1\highlight2 
\par \cf0\highlight0    24  sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg\cf1\highlight2 
\par \cf0\highlight0    25  echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubern\cf1\highlight2 
\par \cf0\highlight0    26  sudo apt-get update\cf1\highlight2 
\par \cf0\highlight0    27  apt-get install -qy kubelet=1.21.1-00 kubectl=1.21.1-00 kubeadm=1.21.1-00\cf1\highlight2 
\par \cf0\highlight0    28  kubeadm init\cf1\highlight2 
\par \cf0\highlight0    29  systemctl status docker\cf1\highlight2 
\par \cf0\highlight0    30  systemctl start docker\cf1\highlight2 
\par \cf0\highlight0    31  systemctl enable docker\cf1\highlight2 
\par \cf0\highlight0    32  systemctl status containerd\cf1\highlight2 
\par \cf0\highlight0    33  systemctl enable containerd\cf1\highlight2 
\par \cf0\highlight0    34  kubeadm init\cf1\highlight2 
\par \cf0\highlight0    35  swapoff -a\cf1\highlight2 
\par \cf0\highlight0    36  kubeadm init\cf1\highlight2 
\par \cf0\highlight0    37  export KUBECONFIG=/etc/kubernetes/admin.conf\cf1\highlight2 
\par \cf0\highlight0    38  kubectl get nodes\cf1\highlight2 
\par \cf0\highlight0    39  export kubever=$(kubectl version | base64 | tr -d '\\n')\cf1\highlight2 
\par \cf0\highlight0    40  kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$kubever"\cf1\highlight2 
\par \cf0\highlight0    41  kubectl get nodes\cf1\highlight2 
\par \cf0\highlight0    42  GOLANG_VERSION=$\{GOLANG_VERSION:-"1.14.15"\}\cf1\highlight2 
\par \cf0\highlight0    43  sudo apt -y update\cf1\highlight2 
\par \cf0\highlight0    44  sudo tar -C /usr/local -xzf /tmp/go$\{GOLANG_VERSION\}.linux-amd64.tar.gz\cf1\highlight2 
\par \cf0\highlight0    45  sudo apt-get install vim\cf1\highlight2 
\par \cf0\highlight0    46  go version\cf1\highlight2 
\par \cf0\highlight0    47  vi ~/.bashrc\cf1\highlight2 
\par \cf0\highlight0    48  source ~/.bashrc\cf1\highlight2 
\par \cf0\highlight0    49  go version\cf1\highlight2 
\par \cf0\highlight0    50  go env\cf1\highlight2 
\par \cf0\highlight0    51  mkdir -p go/src/github.com\cf1\highlight2 
\par \cf0\highlight0    52  cd go/src/github.com\cf1\highlight2 
\par \cf0\highlight0    53  git clone https://github.com/CentaurusInfra/fornax.git\cf1\highlight2 
\par \cf0\highlight0    54  mv fornax kubeedge\cf1\highlight2 
\par \cf0\highlight0    55  cd kubeedge\cf1\highlight2 
\par \cf0\highlight0    56  make all\cf1\highlight2 
\par \cf0\highlight0    57  scp 192.168.2.52:/etc/kubernetes/admin.conf  .\cf1\highlight2 
\par \cf0\highlight0    58  ls\cf1\highlight2 
\par \cf0\highlight0    59  vi admin.conf\cf1\highlight2 
\par \cf0\highlight0    60  make WHAT=cloudcore\cf1\highlight2 
\par \cf0\highlight0    61  make WHAT=edgecoremake WHAT=cloudcore\cf1\highlight2 
\par \cf0\highlight0    62  make WHAT=edgecore\cf1\highlight2 
\par \cf0\highlight0    63  mkdir /etc/kubeedge/config -p\cf1\highlight2 
\par \cf0\highlight0    64  cp /etc/kubernetes/admin.conf /root/.kube/config\cf1\highlight2 
\par \cf0\highlight0    65  _output/local/bin/cloudcore --minconfig > /etc/kubeedge/config/cloudcore.yaml\cf1\highlight2 
\par \cf0\highlight0    66  mkdir /etc/kubeedge/config -p\cf1\highlight2 
\par \cf0\highlight0    67  mkdir /root/.kube/config -p\cf1\highlight2 
\par \cf0\highlight0    68  cd /etc/kubeedge/\cf1\highlight2 
\par \cf0\highlight0    69  ls\cf1\highlight2 
\par \cf0\highlight0    70  scp -r 192.168.2.50:/etc/kubeedge/certs .\cf1\highlight2 
\par \cf0\highlight0    71  scp -r 192.168.2.50:/etc/kubeedge/ca .\cf1\highlight2 
\par \cf0\highlight0    72  ls\cf1\highlight2 
\par \cf0\highlight0    73  cd ~/go/src/github.com/kubeedge\cf1\highlight2 
\par \cf0\highlight0    74  ls\cf1\highlight2 
\par \cf0\highlight0    75  export KUBECONFIG=/etc/kubernetes/admin.conf\cf1\highlight2 
\par \cf0\highlight0    76  _output/local/bin/edgecore --edgeclusterconfig > /etc/kubeedge/config/edgecore.yaml\cf1\highlight2 
\par \cf0\highlight0    77  tests/edgecluster/hack/update_edgecore_config.sh admin.conf\cf1\highlight2 
\par \cf0\highlight0    78  kubectl apply -f build/crds/devices/devices_v1alpha2_device.yaml\cf1\highlight2 
\par \cf0\highlight0    79  kubectl apply -f build/crds/devices/devices_v1alpha2_devicemodel.yaml\cf1\highlight2 
\par \cf0\highlight0    80  kubectl apply -f build/crds/reliablesyncs/cluster_objectsync_v1alpha1.yaml\cf1\highlight2 
\par \cf0\highlight0    81  kubectl apply -f build/crds/reliablesyncs/objectsync_v1alpha1.yaml\cf1\highlight2 
\par \cf0\highlight0    82  kubectl apply -f  build/crds/router/router_v1_rule.yaml\cf1\highlight2 
\par \cf0\highlight0    83  kubectl apply -f  build/crds/router/router_v1_ruleEndpoint.yaml\cf1\highlight2 
\par \cf0\highlight0    84  kubectl apply -f build/crds/edgecluster/mission_v1.yaml\cf1\highlight2 
\par \cf0\highlight0    85  kubectl apply -f build/crds/edgecluster/edgecluster_v1.yaml\cf1\highlight2 
\par \cf0\highlight0    86  cp /etc/kubernetes/admin.conf /root/.kube/config\cf1\highlight2 
\par \cf0\highlight0    87  _output/local/bin/cloudcore --minconfig > /etc/kubeedge/config/cloudcore.yaml\cf1\highlight2 
\par \cf0\highlight0    88  nohup  _output/local/bin/edgecore --edgecluster > edgecore.logs 2>&1 &\cf1\highlight2 
\par \cf0\highlight0    89  tail -f edgecore.logs\cf1\highlight2 
\par \cf0\highlight0    90  history\cf1\highlight2 
\par \cf0\highlight0    91  cd /etc/kubeedge/\cf1\highlight2 
\par \cf0\highlight0    92  ls\cf1\highlight2 
\par \cf0\highlight0    93  cd ca\cf1\highlight2 
\par \cf0\highlight0    94  ls\cf1\highlight2 
\par \cf0\highlight0    95  cd ~/go/src/github.com/kubeedge\cf1\highlight2 
\par \cf0\highlight0    96  ls\cf1\highlight2 
\par \cf0\highlight0    97  vi admin.conf\cf1\highlight2 
\par \cf0\highlight0    98  rm -rf admin.conf\cf1\highlight2 
\par \cf0\highlight0    99  scp -r 192.168.2.50:/etc/kubernetes/admin.conf  .\cf1\highlight2 
\par \cf0\highlight0   100  rm -rf edgecore.logs\cf1\highlight2 
\par \cf0\highlight0   101  nohup  _output/local/bin/edgecore --edgecluster > edgecore.logs 2>&1 &\cf1\highlight2 
\par \cf0\highlight0   102  tail -f edgecore.logs\cf1\highlight2 
\par \cf0\highlight0   103  cd /etc/kubeedge/\cf1\highlight2 
\par \cf0\highlight0   104  ls\cf1\highlight2 
\par \cf0\highlight0   105  cdcca\cf1\highlight2 
\par \cf0\highlight0   106  cd ca\cf1\highlight2 
\par \cf0\highlight0   107  ls\cf1\highlight2 
\par \cf0\highlight0   108  ll\cf1\highlight2 
\par \cf0\highlight0   109  cd ../certs/\cf1\highlight2 
\par \cf0\highlight0   110  LS\cf1\highlight2 
\par \cf0\highlight0   111  ls\cf1\highlight2 
\par \cf0\highlight0   112  nohup  _output/local/bin/edgecore --edgecluster > edgecore.logs 2>&1 &\cf1\highlight2 
\par \cf0\highlight0   113  tail -f edgecore.logs\cf1\highlight2 
\par \cf0\highlight0   114  cd ~/go/src/github.com/kubeedge\cf1\highlight2 
\par \cf0\highlight0   115  ps -ef | grep edgecore.logs\cf1\highlight2 
\par \cf0\highlight0   116  ls\cf1\highlight2 
\par \cf0\highlight0   117  rm -rf edgecore.logs\cf1\highlight2 
\par \cf0\highlight0   118  nohup  _output/local/bin/edgecore --edgecluster > edgecore.logs 2>&1 &\cf1\highlight2 
\par \cf0\highlight0   119  tail -f edgecore.logs\cf1\highlight2 
\par \cf0\highlight0   120  ls\cf1\highlight2 
\par \cf0\highlight0   121  ufw status\cf1\highlight2 
\par \cf0\highlight0   122  swapoff -a\cf1\highlight2 
\par \cf0\highlight0   123  tail -f edgecore.logs\cf1\highlight2 
\par \cf0\highlight0   124  vi admin.conf\cf1\highlight2 
\par \cf0\highlight0   125  rm -ef edgecore.logs\cf1\highlight2 
\par \cf0\highlight0   126  rm -rf edgecore.logs\cf1\highlight2 
\par \cf0\highlight0   127  ps -ef | grep edgecore\cf1\highlight2 
\par \cf0\highlight0   128  kill -9 26367\cf1\highlight2 
\par \cf0\highlight0   129  kill -9 31336\cf1\highlight2 
\par \cf0\highlight0   130  kill -9 4657\cf1\highlight2 
\par \cf0\highlight0   131  ps -ef | grep edgecore\cf1\highlight2 
\par \cf0\highlight0   132  ls\cf1\highlight2 
\par \cf0\highlight0   133  nohup  _output/local/bin/edgecore --edgecluster > edgecore.logs 2>&1 &\cf1\highlight2 
\par \cf0\highlight0   134  tail -f edgecore.logs\cf1\highlight2 
\par \cf0\highlight0   135  ps -ef | grep edgecore\cf1\highlight2 
\par \cf0\highlight0   136  kill -9 9320\cf1\highlight2 
\par \cf0\highlight0   137  rm -rf edgecore.logs\cf1\highlight2 
\par \cf0\highlight0   138  mkdir /etc/kubeedge/config -p\cf1\highlight2 
\par \cf0\highlight0   139  cp /etc/kubernetes/admin.conf /root/.kube/config\cf1\highlight2 
\par \cf0\highlight0   140  _output/local/bin/cloudcore --minconfig > /etc/kubeedge/config/cloudcore.yaml\cf1\highlight2 
\par \cf0\highlight0   141  ls\cf1\highlight2 
\par \cf0\highlight0   142  vi adminc\cf1\highlight2 
\par \cf0\highlight0   143  vi admin.conf\cf1\highlight2 
\par \cf0\highlight0   144  make WHAT=cloudcore\cf1\highlight2 
\par \cf0\highlight0   145  make WHAT=edgecore\cf1\highlight2 
\par \cf0\highlight0   146  mkdir -p /etc/kubeedge/ca\cf1\highlight2 
\par \cf0\highlight0   147  cd /etc/kubee\cf1\highlight2 
\par \cf0\highlight0   148  cd /etc/kubeedge/\cf1\highlight2 
\par \cf0\highlight0   149  ls\cf1\highlight2 
\par \cf0\highlight0   150  vi config\cf1\highlight2 
\par \cf0\highlight0   151  rm -rf ca certs\cf1\highlight2 
\par \cf0\highlight0   152  scp -r 192.168.2.50:/etc/kubeedge/ca /etc/kubeedge\cf1\highlight2 
\par \cf0\highlight0   153  scp -r 192.168.2.50:/etc/kubeedge/certs /etc/kubeedge\cf1\highlight2 
\par \cf0\highlight0   154  mkdir /etc/kubeedge/config -p\cf1\highlight2 
\par \cf0\highlight0   155  rm -rf config/\cf1\highlight2 
\par \cf0\highlight0   156  ls\cf1\highlight2 
\par \cf0\highlight0   157  cd ~/go/src/github.com/kubeedge\cf1\highlight2 
\par \cf0\highlight0   158  cp /etc/kubernetes/admin.conf /root/edgecluster.kubeconfig\cf1\highlight2 
\par \cf0\highlight0   159  _output/local/bin/edgecore --edgeclusterconfig > /etc/kubeedge/config/edgecore.yaml\cf1\highlight2 
\par \cf0\highlight0   160  mkdir /etc/kubeedge/config -p\cf1\highlight2 
\par \cf0\highlight0   161  cp /etc/kubernetes/admin.conf /root/edgecluster.kubeconfig\cf1\highlight2 
\par \cf0\highlight0   162  _output/local/bin/edgecore --edgeclusterconfig > /etc/kubeedge/config/edgecore.yaml\cf1\highlight2 
\par \cf0\highlight0   163  tests/edgecluster/hack/update_edgecore_config.sh admin.conf\cf1\highlight2 
\par \cf0\highlight0   164  chmod 777 /root/go/src/github.com/kubeedge/_output/local/bin/kubectl/vanilla/kubectl\cf1\highlight2 
\par \cf0\highlight0   165  mkdir /root/.kube/config -p\cf1\highlight2 
\par \cf0\highlight0   166  cp /etc/kubernetes/admin.conf /root/.kube/config\cf1\highlight2 
\par \cf0\highlight0   167  _output/local/bin/cloudcore --minconfig > /etc/kubeedge/config/cloudcore.yaml\cf1\highlight2 
\par \cf0\highlight0   168  ls\cf1\highlight2 
\par \cf0\highlight0   169  export KUBECONFIG=/etc/kubernetes/admin.conf\cf1\highlight2 
\par \cf0\highlight0   170  ls\cf1\highlight2 
\par \cf0\highlight0   171  nohup  _output/local/bin/cloudcore > cloudcore.logs 2>&1 &\cf1\highlight2 
\par \cf0\highlight0   172  tail -f cloudcore.logs\cf1\highlight2 
\par \cf0\highlight0   173  nohup  _output/local/bin/edgecore --edgecluster > edgecore.logs 2>&1 &\cf1\highlight2 
\par \cf0\highlight0   174  tail -f edgecore.logs\cf1\highlight2 
\par \cf0\highlight0   175  ls\cf1\highlight2 
\par \cf0\highlight0   176  vi admin.conf\cf1\highlight2 
\par \cf0\highlight0   177  tail -f cloudcore.logs\cf1\highlight2 
\par \cf0\highlight0   178  rm -rf cloudcore.logs edgecore.logs\cf1\highlight2 
\par \cf0\highlight0   179  ls\cf1\highlight2 
\par \cf0\highlight0   180  ps -ef cloudcore\cf1\highlight2 
\par \cf0\highlight0   181  ps -ef | grepcloudcore\cf1\highlight2 
\par \cf0\highlight0   182  ps -ef | grep cloudcore\cf1\highlight2 
\par \cf0\highlight0   183  kill -9 331277\cf1\highlight2 
\par \cf0\highlight0   184  kill -9 31277\cf1\highlight2 
\par \cf0\highlight0   185  pas -ef | grep edgecore\cf1\highlight2 
\par \cf0\highlight0   186  ps -ef | grep edgecore\cf1\highlight2 
\par \cf0\highlight0   187  kill -9 31502\cf1\highlight2 
\par \cf0\highlight0   188  rm -rf admin.conf\cf1\highlight2 
\par \cf0\highlight0   189  ls\cf1\highlight2 
\par \cf0\highlight0   190  scp 192.168.2.52:/etc/kubernetes/admin.conf .\cf1\highlight2 
\par \cf0\highlight0   191  cp /etc/kubernetes/admin.conf /root/edgecluster.kubeconfig\cf1\highlight2 
\par \cf0\highlight0   192  tests/edgecluster/hack/update_edgecore_config.sh admin.conf\cf1\highlight2 
\par \cf0\highlight0   193  chmod 777 /root/go/src/github.com/kubeedge/_output/local/bin/kubectl/vanilla/kubectl\cf1\highlight2 
\par \cf0\highlight0   194  export KUBECONFIG=/etc/kubernetes/admin.conf\cf1\highlight2 
\par \cf0\highlight0   195  cp /etc/kubernetes/admin.conf /root/.kube/config\cf1\highlight2 
\par \cf0\highlight0   196  _output/local/bin/cloudcore --minconfig > /etc/kubeedge/config/cloudcore.yaml\cf1\highlight2 
\par \cf0\highlight0   197  nohup  _output/local/bin/cloudcore > cloudcore.logs 2>&1 &\cf1\highlight2 
\par \cf0\highlight0   198  tail -f cloudcore.logs\cf1\highlight2 
\par \cf0\highlight0   199  nohup  _output/local/bin/edgecore --edgecluster > edgecore.logs 2>&1 &\cf1\highlight2 
\par \cf0\highlight0   200  tail -f edgecore.logs\cf1\highlight2 
\par \cf0\highlight0   201  ls\cf1\highlight2 
\par \cf0\highlight0   202  vi admin.conf\cf1\highlight2 
\par \cf0\highlight0   203  history\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ps -ef | grep cloudcore\cf1\highlight2 
\par \cf0\highlight0 root     11996  1899  0 12:56 pts/0    00:00:00 grep --color=auto \cf9 cloudcore\cf1\highlight2 
\par \cf0\highlight0 root     27689  1899  0 12:21 pts/0    00:00:01 _output/local/bin/\cf9 cloudcore\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# chmod -R 777 /etc/kubeedge\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get edgecluster\cf1\highlight2 
\par \cf0\highlight0 NAME     LASTHEARBEAT   HEALTHSTATUS   SUBEDGECLUSTERS   RECEIVED_MISSIONS   MATCHED_MISSIONS\cf1\highlight2 
\par \cf0\highlight0 node-e   7s             healthy\cf1\highlight2 
\par \cf0\highlight0 node-f   5s             healthy\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get edgecluster\cf1\highlight2 
\par \cf0\highlight0 NAME     LASTHEARBEAT   HEALTHSTATUS   SUBEDGECLUSTERS   RECEIVED_MISSIONS   MATCHED_MISSIONS\cf1\highlight2 
\par \cf0\highlight0 node-e   6s             healthy\cf1\highlight2 
\par \cf0\highlight0 node-f   6s             healthy\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get edgecluster\cf1\highlight2 
\par \cf0\highlight0 NAME     LASTHEARBEAT   HEALTHSTATUS   SUBEDGECLUSTERS   RECEIVED_MISSIONS   MATCHED_MISSIONS\cf1\highlight2 
\par \cf0\highlight0 node-e   9s             healthy\cf1\highlight2 
\par \cf0\highlight0 node-f   10s            healthy\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get edgecluster\cf1\highlight2 
\par \cf0\highlight0 NAME     LASTHEARBEAT   HEALTHSTATUS   SUBEDGECLUSTERS   RECEIVED_MISSIONS   MATCHED_MISSIONS\cf1\highlight2 
\par \cf0\highlight0 node-e   8s             healthy\cf1\highlight2 
\par \cf0\highlight0 node-f   12s            healthy\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-qgfkq           0/1     Pending   0          4s\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          178m\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-qgfkq           0/1     Pending   0          7s\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          178m\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-qgfkq           0/1     Pending   0          8s\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          178m\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 ^[[ANAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-qgfkq           0/1     Pending   0          8s\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          178m\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-qgfkq           0/1     Pending   0          9s\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          179m\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          178m\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-qgfkq           0/1     Pending   0          33s\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          179m\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-qgfkq           0/1     Pending   0          36s\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          179m\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-qgfkq           0/1     Pending   0          37s\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          179m\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-qgfkq           0/1     Pending   0          56s\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-75286       0/1     Pending   0          5s\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          179m\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-qgfkq           0/1     Pending   0          76s\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-75286       0/1     Pending   0          25s\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          3h\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-qgfkq           0/1     Pending   0          79s\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-75286       0/1     Pending   0          28s\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          3h\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          3h\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-qgfkq           0/1     Pending   0          102s\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-75286       0/1     Pending   0          51s\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          3h1m\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          3h1m\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          3h1m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          3h1m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          3h1m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          3h1m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          3h1m\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          3h\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-qgfkq           0/1     Pending   0          103s\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-75286       0/1     Pending   0          52s\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          3h1m\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          3h1m\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          3h1m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          3h1m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          3h1m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          3h1m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          3h1m\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          3h\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-qgfkq           0/1     Pending   0          104s\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-75286       0/1     Pending   0          53s\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          3h1m\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          3h1m\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          3h1m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          3h1m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          3h1m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          3h1m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          3h1m\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          3h\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-qgfkq           0/1     Pending   0          3m57s\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-75286       0/1     Pending   0          3m6s\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          3h3m\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          3h3m\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          3h3m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          3h3m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          3h3m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          3h3m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          3h3m\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          3h2m\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE     IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-qgfkq           0/1     Pending   0          4m9s    <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-75286       0/1     Pending   0          3m18s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          3h3m    10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          3h3m    10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          3h3m    192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          3h3m    192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          3h3m    192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          3h3m    192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          3h3m    192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          3h2m    192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                                   READY   STATUS    RESTARTS   AGE     IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-qgfkq                 0/1     Pending   0          7m40s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-75286             0/1     Pending   0          6m49s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          receiver-deployment-74b5c7d449-m9mrn   0/1     Pending   0          7s      <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5               1/1     Running   0          3h7m    10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2               1/1     Running   0          3h7m    10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                            1/1     Running   0          3h7m    192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d                  1/1     Running   0          3h7m    192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d         1/1     Running   0          3h7m    192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                       1/1     Running   0          3h7m    192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d                  1/1     Running   0          3h7m    192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                        2/2     Running   1          3h6m    192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ps -ef | grep cloudcore\cf1\highlight2 
\par \cf0\highlight0 root     17001  1899  0 04:49 pts/0    00:00:00 grep --color=auto \cf9 cloudcore\cf1\highlight2 
\par \cf0\highlight0 root     27689  1899  0 Dec21 pts/0    00:04:55 _output/local/bin/\cf9 cloudcore\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kill -9 27689\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# rm -rf cloudcore.logs\cf1\highlight2 
\par \cf0\highlight0 [1]-  Killed                  nohup _output/local/bin/cloudcore > cloudcore.logs 2>&1\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ps -ef | grep edgecore\cf1\highlight2 
\par \cf0\highlight0 root     18236  1899  0 04:49 pts/0    00:00:00 grep --color=auto \cf9 edgecore\cf1\highlight2 
\par \cf0\highlight0 root     27861  1899  1 Dec21 pts/0    00:12:38 _output/local/bin/\cf9 edgecore\cf0  --edgecluster\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kill -9 27861\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# rm -rf edgecore.logs\cf1\highlight2 
\par \cf0\highlight0 [2]+  Killed                  nohup _output/local/bin/edgecore --edgecluster > edgecore.logs 2>&1\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ls\cf1\highlight2 
\par \cf0\highlight0 admin.conf   \cf12 build\cf0          \cf12 cloud\cf0                CONTRIBUTING.md  \cf12 edgemesh\cf0                 go.mod  \cf12 keadm\cf0      MAINTAINERS     \cf12 mappers\cf0   \cf12 pkg\cf0            \cf12 staging\cf1\highlight2 
\par \cf0\highlight0 ADOPTERS.md  \cf12 CHANGELOG\cf0      CODE_OF_CONDUCT.md  \cf12 docs\cf0              \cf12 edgesite\cf0                 go.sum  LICENSE   MAINTAINERS.md  \cf12 _output\cf0   README.md     \cf12 tests\cf1\highlight2 
\par \cf12\highlight0 binaries\cf0      CHANGELOG.md  \cf12 common\cf0               \cf12 edge\cf0              external-dependency.md  \cf12 hack\cf0     \cf12 LICENSES\cf0   Makefile        OWNERS   README_zh.md  \cf12 vendor\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# nohup  _output/local/bin/edgecore --edgecluster > edgecore.logs 2>&1 &\cf1\highlight2 
\par \cf0\highlight0 [1] 26253\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# tail -f edgecore.logs\cf1\highlight2 
\par \cf0\highlight0\}\cf1\highlight2 
\par \cf0\highlight0\{\cf1\highlight2 
\par \cf0\highlight0   "node-f": \{\cf1\highlight2 
\par \cf0\highlight0     "healthstatus": "disconnected",\cf1\highlight2 
\par \cf0\highlight0     "lastheartbeat": "2021-12-22T04:44:16Z"\cf1\highlight2 
\par \cf0\highlight0   \}\cf1\highlight2 
\par \cf0\highlight0\}\cf1\highlight2 
\par \cf0\highlight0 ), error: invalid character '\{' after top-level value\cf1\highlight2 
\par \cf0\highlight0 E1222 05:05:41.455383   26253 imitator.go:108] [imitator] delete error: <nil>\cf1\highlight2 
\par \cf0\highlight0 E1222 05:05:42.619981   26253 imitator.go:108] [imitator] delete error: <nil>\cf1\highlight2 
\par \cf0\highlight0 E1222 05:05:43.945001   26253 imitator.go:108] [imitator] delete error: <nil>\cf1\highlight2 
\par \cf0\highlight0 E1222 05:05:45.291234   26253 imitator.go:108] [imitator] delete error: <nil>\cf1\highlight2 
\par \cf0\highlight0 E1222 05:05:47.664423   26253 clusterd.go:172] handle mission failed: Failed to delete mission command-frontend-port-forward: Command (/root/go/src/github.com/kubeedge/_outtl/vanilla/kubectl delete mission command-frontend-port-forward --kubeconfig=/root/edgecluster.kubeconfig) failed: exitcode: 1, output (Error from server (NotFound): missioeedge.io "command-frontend-port-forward" not found\cf1\highlight2 
\par \cf0\highlight0 ), error: exit status 1\cf1\highlight2 
\par \cf0\highlight0 I1222 05:05:48.449887   26253 mission_deployer.go:125] Mission resource-secret is created\cf1\highlight2 
\par \cf0\highlight0 E1222 05:05:49.279876   26253 mission_deployer.go:60] Failed to deploy the resource of mission resource-secret: Command (printf 'apiVersion: v1\cf1\highlight2 
\par \cf0\highlight0 kind: Secret\cf1\highlight2 
\par \cf0\highlight0 metadata:\cf1\highlight2 
\par \cf0\highlight0   name: kube-face-secret\cf1\highlight2 
\par \cf0\highlight0   namespace: face\cf1\highlight2 
\par \cf0\highlight0 type: Opaque\cf1\highlight2 
\par \cf0\highlight0 data:\cf1\highlight2 
\par \cf0\highlight0   mysql_userpassword: cm9vdDpwYXNzd29yZDEyMw==\cf1\highlight2 
\par \cf0\highlight0   mysql_password: cGFzc3dvcmQxMjM=\cf1\highlight2 
\par \cf0\highlight0 ' | /root/go/src/github.com/kubeedge/_output/local/bin/kubectl/vanilla/kubectl apply --kubeconfig=/root/edgecluster.kubeconfig -f - ) failed: exitcode: 1, output (Error fro: error when creating "STDIN": namespaces "face" not found\cf1\highlight2 
\par \cf0\highlight0 ), error: exit status 1\cf1\highlight2 
\par \cf0\highlight0 E1222 05:05:50.220973   26253 mission_deployer.go:156] Failed to revert the content of mission resource-nsqd-svc: Command (printf 'apiVersion: v1\cf1\highlight2 
\par \cf0\highlight0 kind: Service\cf1\highlight2 
\par \cf0\highlight0 metadata:\cf1\highlight2 
\par \cf0\highlight0   name: nsqd\cf1\highlight2 
\par \cf0\highlight0   namespace: face\cf1\highlight2 
\par \cf0\highlight0 spec:\cf1\highlight2 
\par \cf0\highlight0   ports:\cf1\highlight2 
\par \cf0\highlight0   - name: main\cf1\highlight2 
\par \cf0\highlight0     protocol: TCP\cf1\highlight2 
\par \cf0\highlight0     port: 4150\cf1\highlight2 
\par \cf0\highlight0     targetPort: 4150\cf1\highlight2 
\par \cf0\highlight0   - name: secondary\cf1\highlight2 
\par \cf0\highlight0     protocol: TCP\cf1\highlight2 
\par \cf0\highlight0     port: 4151\cf1\highlight2 
\par \cf0\highlight0     targetPort: 4151\cf1\highlight2 
\par \cf0\highlight0   selector:\cf1\highlight2 
\par \cf0\highlight0     app: nsqd\cf1\highlight2 
\par \cf0\highlight0   clusterIP: None\cf1\highlight2 
\par \cf0\highlight0 ' | /root/go/src/github.com/kubeedge/_output/local/bin/kubectl/vanilla/kubectl delete --kubeconfig=/root/edgecluster.kubeconfig -f - ) failed: exitcode: 1, output (Error fr): error when deleting "STDIN": services "nsqd" not found\cf1\highlight2 
\par \cf0\highlight0 ), error: exit status 1\cf1\highlight2 
\par \cf0\highlight0 E1222 05:05:50.710473   26253 clusterd.go:172] handle mission failed: Failed to delete mission resource-nsqd-svc: Command (/root/go/src/github.com/kubeedge/_output/local/biubectl delete mission resource-nsqd-svc --kubeconfig=/root/edgecluster.kubeconfig) failed: exitcode: 1, output (Error from server (NotFound): missions.edgeclusters.kubeedgesvc" not found\cf1\highlight2 
\par \cf0\highlight0 ), error: exit status 1\cf1\highlight2 
\par \cf0\highlight0 I1222 05:05:51.671190   26253 mission_deployer.go:125] Mission resource-receiver-svc is created\cf1\highlight2 
\par \cf0\highlight0 E1222 05:05:52.386826   26253 edgecluster_state_reporter.go:226] Error in unmarshall edgecluster state json: (\{\cf1\highlight2 
\par \cf0\highlight0   "node-e": \{\cf1\highlight2 
\par \cf0\highlight0     "healthstatus": "disconnected",\cf1\highlight2 
\par \cf0\highlight0     "lastheartbeat": "2021-12-22T04:47:19Z"\cf1\highlight2 
\par \cf0\highlight0   \}\cf1\highlight2 
\par \cf0\highlight0\}\cf1\highlight2 
\par \cf0\highlight0\{\cf1\highlight2 
\par \cf0\highlight0   "node-f": \{\cf1\highlight2 
\par \cf0\highlight0     "healthstatus": "disconnected",\cf1\highlight2 
\par \cf0\highlight0     "lastheartbeat": "2021-12-22T04:44:16Z"\cf1\highlight2 
\par \cf0\highlight0   \}\cf1\highlight2 
\par \cf0\highlight0\}\cf1\highlight2 
\par \cf0\highlight0 ), error: invalid character '\{' after top-level value\cf1\highlight2 
\par \cf0\highlight0 E1222 05:05:52.600053   26253 mission_deployer.go:60] Failed to deploy the resource of mission resource-receiver-svc: Command (printf 'apiVersion: v1\cf1\highlight2 
\par \cf0\highlight0 kind: Service\cf1\highlight2 
\par \cf0\highlight0 metadata:\cf1\highlight2 
\par \cf0\highlight0   name: receiver-service\cf1\highlight2 
\par \cf0\highlight0   namespace: face\cf1\highlight2 
\par \cf0\highlight0 spec:\cf1\highlight2 
\par \cf0\highlight0   ports:\cf1\highlight2 
\par \cf0\highlight0   - protocol: TCP\cf1\highlight2 
\par \cf0\highlight0     port: 8000\cf1\highlight2 
\par \cf0\highlight0     targetPort: 8000\cf1\highlight2 
\par \cf0\highlight0   selector:\cf1\highlight2 
\par \cf0\highlight0     app: receiver\cf1\highlight2 
\par \cf0\highlight0   type: NodePort\cf1\highlight2 
\par \cf0\highlight0 ' | /root/go/src/github.com/kubeedge/_output/local/bin/kubectl/vanilla/kubectl apply --kubeconfig=/root/edgecluster.kubeconfig -f - ) failed: exitcode: 1, output (Error fro: error when creating "STDIN": namespaces "face" not found\cf1\highlight2 
\par \cf0\highlight0 ), error: exit status 1\cf1\highlight2 
\par \cf0\highlight0 E1222 05:05:53.573179   26253 mission_deployer.go:156] Failed to revert the content of mission resource-mysql-service: Command (printf 'apiVersion: v1\cf1\highlight2 
\par \cf0\highlight0 kind: Service\cf1\highlight2 
\par \cf0\highlight0 metadata:\cf1\highlight2 
\par \cf0\highlight0   name: mysql\cf1\highlight2 
\par \cf0\highlight0   namespace: face\cf1\highlight2 
\par \cf0\highlight0 spec:\cf1\highlight2 
\par \cf0\highlight0   ports:\cf1\highlight2 
\par \cf0\highlight0   - port: 3306\cf1\highlight2 
\par \cf0\highlight0   selector:\cf1\highlight2 
\par \cf0\highlight0     app: mysql\cf1\highlight2 
\par \cf0\highlight0   clusterIP: None\cf1\highlight2 
\par \cf0\highlight0 ' | /root/go/src/github.com/kubeedge/_output/local/bin/kubectl/vanilla/kubectl delete --kubeconfig=/root/edgecluster.kubeconfig -f - ) failed: exitcode: 1, output (Error fr): error when deleting "STDIN": services "mysql" not found\cf1\highlight2 
\par \cf0\highlight0 ), error: exit status 1\cf1\highlight2 
\par \cf0\highlight0 E1222 05:05:54.109489   26253 clusterd.go:172] handle mission failed: Failed to delete mission resource-mysql-service: Command (/root/go/src/github.com/kubeedge/_output/loclla/kubectl delete mission resource-mysql-service --kubeconfig=/root/edgecluster.kubeconfig) failed: exitcode: 1, output (Error from server (NotFound): missions.edgeclusterurce-mysql-service" not found\cf1\highlight2 
\par \cf0\highlight0 ), error: exit status 1\cf1\highlight2 
\par \cf0\highlight0 E1222 05:05:54.576672   26253 mission_deployer.go:156] Failed to revert the content of mission resource-frontend-svc: Command (printf 'apiVersion: v1\cf1\highlight2 
\par \cf0\highlight0 kind: Service\cf1\highlight2 
\par \cf0\highlight0 metadata:\cf1\highlight2 
\par \cf0\highlight0   name: frontend\cf1\highlight2 
\par \cf0\highlight0   namespace: face\cf1\highlight2 
\par \cf0\highlight0 spec:\cf1\highlight2 
\par \cf0\highlight0   ports:\cf1\highlight2 
\par \cf0\highlight0   - port: 8081\cf1\highlight2 
\par \cf0\highlight0   selector:\cf1\highlight2 
\par \cf0\highlight0     app: frontend\cf1\highlight2 
\par \cf0\highlight0   type: NodePort\cf1\highlight2 
\par \cf0\highlight0 ' | /root/go/src/github.com/kubeedge/_output/local/bin/kubectl/vanilla/kubectl delete --kubeconfig=/root/edgecluster.kubeconfig -f - ) failed: exitcode: 1, output (Error fr): error when deleting "STDIN": services "frontend" not found\cf1\highlight2 
\par \cf0\highlight0 ), error: exit status 1\cf1\highlight2 
\par \cf0\highlight0 E1222 05:05:55.015908   26253 clusterd.go:172] handle mission failed: Failed to delete mission resource-frontend-svc: Command (/root/go/src/github.com/kubeedge/_output/locala/kubectl delete mission resource-frontend-svc --kubeconfig=/root/edgecluster.kubeconfig) failed: exitcode: 1, output (Error from server (NotFound): missions.edgeclusters.ce-frontend-svc" not found\cf1\highlight2 
\par \cf0\highlight0 ), error: exit status 1\cf1\highlight2 
\par \cf0\highlight0 E1222 05:05:55.534048   26253 mission_deployer.go:156] Failed to revert the content of mission resource-frontend-deployment: Command (printf 'apiVersion: apps/v1\cf1\highlight2 
\par \cf0\highlight0 kind: Deployment\cf1\highlight2 
\par \cf0\highlight0 metadata:\cf1\highlight2 
\par \cf0\highlight0   name: frontend\cf1\highlight2 
\par \cf0\highlight0   namespace: face\cf1\highlight2 
\par \cf0\highlight0 spec:\cf1\highlight2 
\par \cf0\highlight0   selector:\cf1\highlight2 
\par \cf0\highlight0     matchLabels:\cf1\highlight2 
\par \cf0\highlight0       app: frontend\cf1\highlight2 
\par \cf0\highlight0   replicas: 1\cf1\highlight2 
\par \cf0\highlight0   template:\cf1\highlight2 
\par \cf0\highlight0     metadata:\cf1\highlight2 
\par \cf0\highlight0       labels:\cf1\highlight2 
\par \cf0\highlight0         app: frontend\cf1\highlight2 
\par \cf0\highlight0     spec:\cf1\highlight2 
\par \cf0\highlight0       containers:\cf1\highlight2 
\par \cf0\highlight0       - name: frontend\cf1\highlight2 
\par \cf0\highlight0         image: skarlso/kube-frontend-alpine:1.1.0\cf1\highlight2 
\par \cf0\highlight0         imagePullPolicy: Always\cf1\highlight2 
\par \cf0\highlight0         env:\cf1\highlight2 
\par \cf0\highlight0         - name: MYSQL_CONNECTION\cf1\highlight2 
\par \cf0\highlight0           value: "mysql.face.svc.cluster.local"\cf1\highlight2 
\par \cf0\highlight0         - name: MYSQL_USERPASSWORD\cf1\highlight2 
\par \cf0\highlight0           valueFrom:\cf1\highlight2 
\par \cf0\highlight0             secretKeyRef:\cf1\highlight2 
\par \cf0\highlight0               name: kube-face-secret\cf1\highlight2 
\par \cf0\highlight0               key: mysql_userpassword\cf1\highlight2 
\par \cf0\highlight0         - name: MYSQL_PORT\cf1\highlight2 
\par \cf0\highlight0           value: "3306"\cf1\highlight2 
\par \cf0\highlight0         - name: MYSQL_DBNAME\cf1\highlight2 
\par \cf0\highlight0           value: kube\cf1\highlight2 
\par \cf0\highlight0         - name: FRONTEND_PORT\cf1\highlight2 
\par \cf0\highlight0           value: "8081"\cf1\highlight2 
\par \cf0\highlight0         ports:\cf1\highlight2 
\par \cf0\highlight0         - containerPort: 8081\cf1\highlight2 
\par \cf0\highlight0           hostPort: 8081\cf1\highlight2 
\par \cf0\highlight0 ' | /root/go/src/github.com/kubeedge/_output/local/bin/kubectl/vanilla/kubectl delete --kubeconfig=/root/edgecluster.kubeconfig -f - ) failed: exitcode: 1, output (Error fr): error when deleting "STDIN": deployments.apps "frontend" not found\cf1\highlight2 
\par \cf0\highlight0 ), error: exit status 1\cf1\highlight2 
\par \cf0\highlight0 E1222 05:05:56.031168   26253 clusterd.go:172] handle mission failed: Failed to delete mission resource-frontend-deployment: Command (/root/go/src/github.com/kubeedge/_outpl/vanilla/kubectl delete mission resource-frontend-deployment --kubeconfig=/root/edgecluster.kubeconfig) failed: exitcode: 1, output (Error from server (NotFound): missionsdge.io "resource-frontend-deployment" not found\cf1\highlight2 
\par \cf0\highlight0 ), error: exit status 1\cf1\highlight2 
\par \cf0\highlight0 I1222 05:05:56.839112   26253 mission_deployer.go:125] Mission resource-nsqlookup-deployment is created\cf1\highlight2 
\par \cf0\highlight0 E1222 05:05:57.860519   26253 mission_deployer.go:60] Failed to deploy the resource of mission resource-nsqlookup-deployment: Command (printf 'apiVersion: apps/v1\cf1\highlight2 
\par \cf0\highlight0 kind: Deployment\cf1\highlight2 
\par \cf0\highlight0 metadata:\cf1\highlight2 
\par \cf0\highlight0   name: nsqlookup\cf1\highlight2 
\par \cf0\highlight0   namespace: face\cf1\highlight2 
\par \cf0\highlight0 spec:\cf1\highlight2 
\par \cf0\highlight0   selector:\cf1\highlight2 
\par \cf0\highlight0     matchLabels:\cf1\highlight2 
\par \cf0\highlight0       app: nsqlookup\cf1\highlight2 
\par \cf0\highlight0   replicas: 1\cf1\highlight2 
\par \cf0\highlight0   template:\cf1\highlight2 
\par \cf0\highlight0     metadata:\cf1\highlight2 
\par \cf0\highlight0       labels:\cf1\highlight2 
\par \cf0\highlight0         app: nsqlookup\cf1\highlight2 
\par \cf0\highlight0     spec:\cf1\highlight2 
\par \cf0\highlight0       containers:\cf1\highlight2 
\par \cf0\highlight0       - name: nsqlookup\cf1\highlight2 
\par \cf0\highlight0         image: nsqio/nsq\cf1\highlight2 
\par \cf0\highlight0         imagePullPolicy: Always\cf1\highlight2 
\par \cf0\highlight0         command: ["/nsqlookupd"]\cf1\highlight2 
\par \cf0\highlight0         args: ["--broadcast-address=nsqlookup.face.svc.cluster.local"]\cf1\highlight2 
\par \cf0\highlight0         ports:\cf1\highlight2 
\par \cf0\highlight0         - containerPort: 4160\cf1\highlight2 
\par \cf0\highlight0           hostPort: 4160\cf1\highlight2 
\par \cf0\highlight0         - containerPort: 4161\cf1\highlight2 
\par \cf0\highlight0           hostPort: 4161\cf1\highlight2 
\par \cf0\highlight0 ' | /root/go/src/github.com/kubeedge/_output/local/bin/kubectl/vanilla/kubectl apply --kubeconfig=/root/edgecluster.kubeconfig -f - ) failed: exitcode: 1, output (Error fro: error when creating "STDIN": namespaces "face" not found\cf1\highlight2 
\par \cf0\highlight0 ), error: exit status 1\cf1\highlight2 
\par \cf0\highlight0 E1222 05:05:59.180941   26253 mission_deployer.go:156] Failed to revert the content of mission resource-face-recog-svc: Command (printf 'apiVersion: v1\cf1\highlight2 
\par \cf0\highlight0 kind: Service\cf1\highlight2 
\par \cf0\highlight0 metadata:\cf1\highlight2 
\par \cf0\highlight0   name: face-recog\cf1\highlight2 
\par \cf0\highlight0   namespace: face\cf1\highlight2 
\par \cf0\highlight0 spec:\cf1\highlight2 
\par \cf0\highlight0   ports:\cf1\highlight2 
\par \cf0\highlight0   - protocol: TCP\cf1\highlight2 
\par \cf0\highlight0     port: 50051\cf1\highlight2 
\par \cf0\highlight0     targetPort: 50051\cf1\highlight2 
\par \cf0\highlight0   selector:\cf1\highlight2 
\par \cf0\highlight0     app: face-recog\cf1\highlight2 
\par \cf0\highlight0   clusterIP: None\cf1\highlight2 
\par \cf0\highlight0 ' | /root/go/src/github.com/kubeedge/_output/local/bin/kubectl/vanilla/kubectl delete --kubeconfig=/root/edgecluster.kubeconfig -f - ) failed: exitcode: 1, output (Error fr): error when deleting "STDIN": services "face-recog" not found\cf1\highlight2 
\par \cf0\highlight0 ), error: exit status 1\cf1\highlight2 
\par \cf0\highlight0 E1222 05:05:59.821183   26253 clusterd.go:172] handle mission failed: Failed to delete mission resource-face-recog-svc: Command (/root/go/src/github.com/kubeedge/_output/loilla/kubectl delete mission resource-face-recog-svc --kubeconfig=/root/edgecluster.kubeconfig) failed: exitcode: 1, output (Error from server (NotFound): missions.edgeclustsource-face-recog-svc" not found\cf1\highlight2 
\par \cf0\highlight0 ), error: exit status 1\cf1\highlight2 
\par \cf0\highlight0 E1222 05:06:00.368292   26253 mission_deployer.go:156] Failed to revert the content of mission resource-nsqd-deployment: Command (printf 'apiVersion: apps/v1\cf1\highlight2 
\par \cf0\highlight0 kind: Deployment\cf1\highlight2 
\par \cf0\highlight0 metadata:\cf1\highlight2 
\par \cf0\highlight0   name: nsqd\cf1\highlight2 
\par \cf0\highlight0   namespace: face\cf1\highlight2 
\par \cf0\highlight0 spec:\cf1\highlight2 
\par \cf0\highlight0   selector:\cf1\highlight2 
\par \cf0\highlight0     matchLabels:\cf1\highlight2 
\par \cf0\highlight0       app: nsqd\cf1\highlight2 
\par \cf0\highlight0   replicas: 1\cf1\highlight2 
\par \cf0\highlight0   template:\cf1\highlight2 
\par \cf0\highlight0     metadata:\cf1\highlight2 
\par \cf0\highlight0       labels:\cf1\highlight2 
\par \cf0\highlight0         app: nsqd\cf1\highlight2 
\par \cf0\highlight0     spec:\cf1\highlight2 
\par \cf0\highlight0       containers:\cf1\highlight2 
\par \cf0\highlight0       # Note, since nsq persists information on disk via .dat file it might be advicable to store those\cf1\highlight2 
\par \cf0\highlight0       # in a mount.\cf1\highlight2 
\par \cf0\highlight0       - name: nsqd\cf1\highlight2 
\par \cf0\highlight0         image: nsqio/nsq\cf1\highlight2 
\par \cf0\highlight0         ports:\cf1\highlight2 
\par \cf0\highlight0         - containerPort: 4150\cf1\highlight2 
\par \cf0\highlight0           hostPort: 4150\cf1\highlight2 
\par \cf0\highlight0         - containerPort: 4151\cf1\highlight2 
\par \cf0\highlight0           hostPort: 4151\cf1\highlight2 
\par \cf0\highlight0         env:\cf1\highlight2 
\par \cf0\highlight0         - name: NSQLOOKUP_ADDRESS\cf1\highlight2 
\par \cf0\highlight0           value: nsqlookup.face.svc.cluster.local\cf1\highlight2 
\par \cf0\highlight0         - name: NSQ_BROADCAST_ADDRESS\cf1\highlight2 
\par \cf0\highlight0           value: nsqd.face.svc.cluster.local\cf1\highlight2 
\par \cf0\highlight0         command: ["/nsqd"]\cf1\highlight2 
\par \cf0\highlight0         args: ["--lookupd-tcp-address=$(NSQLOOKUP_ADDRESS):4160", "--broadcast-address=$(NSQ_BROADCAST_ADDRESS)"]\cf1\highlight2 
\par \cf0\highlight0 ' | /root/go/src/github.com/kubeedge/_output/local/bin/kubectl/vanilla/kubectl delete --kubeconfig=/root/edgecluster.kubeconfig -f - ) failed: exitcode: 1, output (Error fr): error when deleting "STDIN": deployments.apps "nsqd" not found\cf1\highlight2 
\par \cf0\highlight0 ), error: exit status 1\cf1\highlight2 
\par \cf0\highlight0 E1222 05:06:00.971297   26253 clusterd.go:172] handle mission failed: Failed to delete mission resource-nsqd-deployment: Command (/root/go/src/github.com/kubeedge/_output/lnilla/kubectl delete mission resource-nsqd-deployment --kubeconfig=/root/edgecluster.kubeconfig) failed: exitcode: 1, output (Error from server (NotFound): missions.edgecluresource-nsqd-deployment" not found\cf1\highlight2 
\par \cf0\highlight0 ), error: exit status 1\cf1\highlight2 
\par \cf0\highlight0 ^C\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge#\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# nohup  _output/local/bin/cloudcore > cloudcore.logs 2>&1 &\cf1\highlight2 
\par \cf0\highlight0 [2] 30913\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# tail -f cloudcore.logs\cf1\highlight2 
\par \cf0\highlight0 W1222 05:06:31.762143   30913 channelq.go:321] nodeStore for edge node node-f not found and created now\cf1\highlight2 
\par \cf0\highlight0 W1222 05:06:31.762282   30913 channelq.go:307] nodeListQueue for edge node node-e not found and created now\cf1\highlight2 
\par \cf0\highlight0 W1222 05:06:31.762415   30913 channelq.go:335] nodeListStore for edge node node-e not found and created now\cf1\highlight2 
\par \cf0\highlight0 W1222 05:06:31.762464   30913 channelq.go:307] nodeListQueue for edge node node-f not found and created now\cf1\highlight2 
\par \cf0\highlight0 W1222 05:06:31.762585   30913 channelq.go:335] nodeListStore for edge node node-f not found and created now\cf1\highlight2 
\par \cf0\highlight0 I1222 05:06:31.868070   30913 signcerts.go:100] Succeed to creating token\cf1\highlight2 
\par \cf0\highlight0 I1222 05:06:31.870360   30913 server.go:44] start unix domain socket server\cf1\highlight2 
\par \cf0\highlight0 I1222 05:06:31.870571   30913 uds.go:71] listening on: //var/lib/kubeedge/kubeedge.sock\cf1\highlight2 
\par \cf0\highlight0 I1222 05:06:31.871185   30913 server.go:64] Starting cloudhub websocket server\cf1\highlight2 
\par \cf0\highlight0 I1222 05:06:33.661325   30913 upstream.go:63] Start upstream devicecontroller\cf1\highlight2 
\par \cf0\highlight0 ^C\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get edgecluster\cf1\highlight2 
\par \cf0\highlight0 NAME     LASTHEARBEAT   HEALTHSTATUS   SUBEDGECLUSTERS   RECEIVED_MISSIONS                                MATCHED_MISSIONS\cf1\highlight2 
\par \cf0\highlight0 node-e   2s             healthy\cf1\highlight2 
\par \cf0\highlight0 node-f   8s             healthy                          ["resource-unknown-pvc","resource-unknown-pv"]   ["resource-unknown-pvc","resource-unknown-pv"]\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ^C\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get edgecluster\cf1\highlight2 
\par \cf0\highlight0 NAME     LASTHEARBEAT   HEALTHSTATUS   SUBEDGECLUSTERS   RECEIVED_MISSIONS          MATCHED_MISSIONS\cf1\highlight2 
\par \cf0\highlight0 node-e   1s             healthy\cf1\highlight2 
\par \cf0\highlight0 node-f   6s             healthy                          ["resource-unknown-pvc"]   ["resource-unknown-pvc"]\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          18h\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          18h\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          18h\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          18h\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          18h\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          18h\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          18h\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          18h\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get edgecluster\cf1\highlight2 
\par \cf0\highlight0 NAME     LASTHEARBEAT   HEALTHSTATUS   SUBEDGECLUSTERS   RECEIVED_MISSIONS          MATCHED_MISSIONS\cf1\highlight2 
\par \cf0\highlight0 node-e   2s             healthy\cf1\highlight2 
\par \cf0\highlight0 node-f   5s             healthy                          ["resource-unknown-pvc"]   ["resource-unknown-pvc"]\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get missio\cf1\highlight2 
\par \cf0\highlight0 error: the server doesn't have a resource type "missio"\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get mission\cf1\highlight2 
\par \cf0\highlight0 No resources found\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get missions\cf1\highlight2 
\par \cf0\highlight0 No resources found\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          18h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          18h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          18h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          18h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          18h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          18h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          18h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          18h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          18h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          18h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          18h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          18h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          18h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          18h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          18h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          18h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          18h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          18h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          18h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          18h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          18h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          18h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          18h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          18h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          18h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          18h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          18h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          18h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          18h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          18h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          18h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          18h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          18h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          18h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          18h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          18h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          18h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          18h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          18h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          18h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          18h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          18h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          18h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          18h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          18h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          18h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          18h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          18h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                                   READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 face          receiver-deployment-74b5c7d449-5d4qh   0/1     Pending   0          1s    <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5               1/1     Running   0          18h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2               1/1     Running   0          18h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d                  1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d         1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                       1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d                  1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                        2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                                   READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 face          receiver-deployment-74b5c7d449-5d4qh   0/1     Pending   0          2s    <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5               1/1     Running   0          18h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2               1/1     Running   0          18h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d                  1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d         1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                       1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d                  1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                        2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                                   READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 face          receiver-deployment-74b5c7d449-5d4qh   0/1     Pending   0          4s    <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5               1/1     Running   0          18h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2               1/1     Running   0          18h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d                  1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d         1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                       1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d                  1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                        2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                                   READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 face          frontend-56b6fd5f8c-dhm2q              0/1     Pending   0          10s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          receiver-deployment-74b5c7d449-5d4qh   0/1     Pending   0          16s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5               1/1     Running   0          18h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2               1/1     Running   0          18h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d                  1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d         1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                       1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d                  1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                        2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                                   READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 face          frontend-56b6fd5f8c-dhm2q              0/1     Pending   0          11s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-vntkj             0/1     Pending   0          1s    <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          receiver-deployment-74b5c7d449-5d4qh   0/1     Pending   0          17s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5               1/1     Running   0          18h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2               1/1     Running   0          18h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d                  1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d         1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                       1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d                  1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                        2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                                   READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 face          frontend-56b6fd5f8c-dhm2q              0/1     Pending   0          13s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-vntkj             0/1     Pending   0          3s    <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          receiver-deployment-74b5c7d449-5d4qh   0/1     Pending   0          19s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5               1/1     Running   0          18h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2               1/1     Running   0          18h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                            1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d                  1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d         1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                       1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d                  1/1     Running   0          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                        2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                                          READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 face          frontend-56b6fd5f8c-dhm2q                     0/1     Pending   0          55s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          image-processor-deployment-7d6d54d996-qpl59   0/1     Pending   0          25s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-fdm8v                        0/1     Pending   0          35s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-vntkj                    0/1     Pending   0          45s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          receiver-deployment-74b5c7d449-5d4qh          0/1     Pending   0          61s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5                      1/1     Running   0          19h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2                      1/1     Running   0          19h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                                   1/1     Running   0          19h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d                         1/1     Running   0          19h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d                1/1     Running   0          19h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                              1/1     Running   0          19h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d                         1/1     Running   0          19h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                               2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                                          READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 face          frontend-56b6fd5f8c-dhm2q                     0/1     Pending   0          58s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          image-processor-deployment-7d6d54d996-qpl59   0/1     Pending   0          28s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-fdm8v                        0/1     Pending   0          38s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-vntkj                    0/1     Pending   0          48s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          receiver-deployment-74b5c7d449-5d4qh          0/1     Pending   0          64s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5                      1/1     Running   0          19h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2                      1/1     Running   0          19h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                                   1/1     Running   0          19h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d                         1/1     Running   0          19h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d                1/1     Running   0          19h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                              1/1     Running   0          19h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d                         1/1     Running   0          19h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                               2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                                          READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 face          frontend-56b6fd5f8c-dhm2q                     0/1     Pending   0          59s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          image-processor-deployment-7d6d54d996-qpl59   0/1     Pending   0          29s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-fdm8v                        0/1     Pending   0          39s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-vntkj                    0/1     Pending   0          49s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          receiver-deployment-74b5c7d449-5d4qh          0/1     Pending   0          65s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5                      1/1     Running   0          19h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2                      1/1     Running   0          19h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                                   1/1     Running   0          19h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d                         1/1     Running   0          19h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d                1/1     Running   0          19h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                              1/1     Running   0          19h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d                         1/1     Running   0          19h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                               2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                                          READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 face          frontend-56b6fd5f8c-dhm2q                     0/1     Pending   0          61s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          image-processor-deployment-7d6d54d996-qpl59   0/1     Pending   0          31s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-fdm8v                        0/1     Pending   0          41s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-vntkj                    0/1     Pending   0          51s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          receiver-deployment-74b5c7d449-5d4qh          0/1     Pending   0          67s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5                      1/1     Running   0          19h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2                      1/1     Running   0          19h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                                   1/1     Running   0          19h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d                         1/1     Running   0          19h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d                1/1     Running   0          19h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                              1/1     Running   0          19h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d                         1/1     Running   0          19h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                               2/2     Running   1          18h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                                          READY   STATUS    RESTARTS   AGE     IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 face          face-recog-698dc6b88f-hl4vq                   0/1     Pending   0          113s    <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          frontend-56b6fd5f8c-dhm2q                     0/1     Pending   0          3m34s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          image-processor-deployment-7d6d54d996-qpl59   0/1     Pending   0          3m4s    <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-fdm8v                        0/1     Pending   0          3m14s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          nsqd-54667b87f4-2v9kb                         0/1     Pending   0          11s     <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-vntkj                    0/1     Pending   0          3m24s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          receiver-deployment-74b5c7d449-5d4qh          0/1     Pending   0          3m40s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5                      1/1     Running   0          19h     10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2                      1/1     Running   0          19h     10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                                   1/1     Running   0          19h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d                         1/1     Running   0          19h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d                1/1     Running   0          19h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                              1/1     Running   0          19h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d                         1/1     Running   0          19h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                               2/2     Running   1          19h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# mkdir -p /tmp/mysql_people\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                                          READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 face          face-recog-698dc6b88f-hl4vq                   0/1     Pending   0          46m   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          frontend-56b6fd5f8c-dhm2q                     0/1     Pending   0          48m   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          image-processor-deployment-7d6d54d996-qpl59   0/1     Pending   0          47m   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-fdm8v                        0/1     Pending   0          47m   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          nsqd-54667b87f4-2v9kb                         0/1     Pending   0          44m   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-vntkj                    0/1     Pending   0          48m   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          receiver-deployment-74b5c7d449-5d4qh          0/1     Pending   0          48m   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5                      1/1     Running   0          19h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2                      1/1     Running   0          19h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                                   1/1     Running   0          19h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d                         1/1     Running   0          19h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d                1/1     Running   0          19h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                              1/1     Running   0          19h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d                         1/1     Running   0          19h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                               2/2     Running   1          19h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get edgecluster\cf1\highlight2 
\par \cf0\highlight0 NAME     LASTHEARBEAT   HEALTHSTATUS   SUBEDGECLUSTERS   RECEIVED_MISSIONS   MATCHED_MISSIONS\cf1\highlight2 
\par \cf0\highlight0 node-e   4m39s          disconnected\cf1\highlight2 
\par \cf0\highlight0 node-f   27m            disconnected\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get edgecluster\cf1\highlight2 
\par \cf0\highlight0 NAME     LASTHEARBEAT   HEALTHSTATUS   SUBEDGECLUSTERS   RECEIVED_MISSIONS   MATCHED_MISSIONS\cf1\highlight2 
\par \cf0\highlight0 node-e   4m47s          disconnected\cf1\highlight2 
\par \cf0\highlight0 node-f   28m            disconnected\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get edgecluster\cf1\highlight2 
\par \cf0\highlight0 NAME     LASTHEARBEAT   HEALTHSTATUS   SUBEDGECLUSTERS   RECEIVED_MISSIONS   MATCHED_MISSIONS\cf1\highlight2 
\par \cf0\highlight0 node-e   4m48s          disconnected\cf1\highlight2 
\par \cf0\highlight0 node-f   28m            disconnected\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get edgecluster\cf1\highlight2 
\par \cf0\highlight0 NAME     LASTHEARBEAT   HEALTHSTATUS   SUBEDGECLUSTERS   RECEIVED_MISSIONS   MATCHED_MISSIONS\cf1\highlight2 
\par \cf0\highlight0 node-e   4m49s          disconnected\cf1\highlight2 
\par \cf0\highlight0 node-f   28m            disconnected\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# tail -f cloudcore.logs\cf1\highlight2 
\par \cf0\highlight0 W1222 05:21:28.863987   30913 upstream.go:751] message: 2ead6da3-9603-466b-adf3-bbd1b95e92aa process failure, mission resource-unknown-pvc not found\cf1\highlight2 
\par \cf0\highlight0 W1222 05:21:56.029471   30913 upstream.go:751] message: f891a5f8-3dcb-4bd3-a6ac-c758e5fe8d24 process failure, mission resource-unknown-pv not found\cf1\highlight2 
\par \cf0\highlight0 W1222 05:22:38.020511   30913 upstream.go:751] message: 602a0fae-156a-4085-bcae-94c590e57e47 process failure, mission resource-unknown-pvc not found\cf1\highlight2 
\par \cf0\highlight0 W1222 05:23:04.642498   30913 upstream.go:751] message: 5e72623e-c532-45d7-ad77-da1a53e214d6 process failure, mission resource-unknown-pv not found\cf1\highlight2 
\par \cf0\highlight0 W1222 05:23:34.633372   30913 upstream.go:751] message: e49ce75a-13b1-45c1-bbf3-18d17cfaeed3 process failure, mission resource-unknown-pvc not found\cf1\highlight2 
\par \cf0\highlight0 E1222 05:24:28.516004   30913 objectsync.go:115] The ObjectResourceVersion is empty in status of objectsync: node-e.311932f5-ffd3-4135-88c8-46aeb56fec35\cf1\highlight2 
\par \cf0\highlight0 W1222 05:24:30.018388   30913 upstream.go:751] message: 5e246f31-9635-4618-a5a7-64e32e50d9bc process failure, mission resource-unknown-pvc not found\cf1\highlight2 
\par \cf0\highlight0 E1222 05:24:30.156111   30913 objectsync.go:115] The ObjectResourceVersion is empty in status of objectsync: node-f.311932f5-ffd3-4135-88c8-46aeb56fec35\cf1\highlight2 
\par \cf0\highlight0 W1222 05:29:00.359682   30913 upstream.go:767] message: 471c28c3-14e0-401a-9117-1aeaad12496d process failure, update mission failed with error: Operation cannot be fulfilled on missions.edgeclusters.kubeedge.io "resource-image-processor-deployment": the object has been modified; please apply your changes to the latest version and try again, name: resource-image-processor-deployment\cf1\highlight2 
\par \cf0\highlight0 W1222 05:31:54.998860   30913 upstream.go:767] message: d256a4ae-01de-4559-87da-ec069f77bea8 process failure, update mission failed with error: Operation cannot be fulfilled on missions.edgeclusters.kubeedge.io "resource-unknown-pv": the object has been modified; please apply your changes to the latest version and try again, name: resource-unknown-pv\cf1\highlight2 
\par \cf0\highlight0 ^C\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ps -ef | grep cloudcore\cf1\highlight2 
\par \cf0\highlight0 root      4163  1899  0 06:17 pts/0    00:00:00 grep --color=auto \cf9 cloudcore\cf1\highlight2 
\par \cf0\highlight0 root     30913  1899  3 05:06 pts/0    00:02:29 _output/local/bin/\cf9 cloudcore\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kill -9 30913\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# rm -rf cloudcore.logs\cf1\highlight2 
\par \cf0\highlight0 [2]+  Killed                  nohup _output/local/bin/cloudcore > cloudcore.logs 2>&1\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ps -ef | grep edgecore\cf1\highlight2 
\par \cf0\highlight0 root      5486  1899  0 06:17 pts/0    00:00:00 grep --color=auto \cf9 edgecore\cf1\highlight2 
\par \cf0\highlight0 root     26253  1899  6 05:05 pts/0    00:04:34 _output/local/bin/\cf9 edgecore\cf0  --edgecluster\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kil -9 26253\cf1\highlight2 
\par 
\par \cf0\highlight0 Command 'kil' not found, did you mean:\cf1\highlight2 
\par 
\par \cf0\highlight0   command 'cil' from deb cil\cf1\highlight2 
\par \cf0\highlight0   command 'pil' from deb picolisp\cf1\highlight2 
\par \cf0\highlight0   command 'kid' from deb python-kid\cf1\highlight2 
\par \cf0\highlight0   command 'kile' from deb kile\cf1\highlight2 
\par \cf0\highlight0   command 'kig' from deb kig\cf1\highlight2 
\par \cf0\highlight0   command 'uil' from deb uil\cf1\highlight2 
\par \cf0\highlight0   command 'kic' from deb kic\cf1\highlight2 
\par \cf0\highlight0   command 'kill' from deb procps\cf1\highlight2 
\par 
\par \cf0\highlight0 Try: apt install <deb name>\cf1\highlight2 
\par 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kill -9 26253\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# rm -rf edgecore.logs\cf1\highlight2 
\par \cf0\highlight0 [1]+  Killed                  nohup _output/local/bin/edgecore --edgecluster > edgecore.logs 2>&1\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ls\cf1\highlight2 
\par \cf0\highlight0 admin.conf   \cf12 build\cf0          \cf12 cloud\cf0                CONTRIBUTING.md     \cf12 edge\cf0       external-dependency.md  \cf12 hack\cf0      \cf12 LICENSES\cf0         Makefile  OWNERS     README_zh.md  \cf12 vendor\cf1\highlight2 
\par \cf0\highlight0 ADOPTERS.md  \cf12 CHANGELOG\cf0      CODE_OF_CONDUCT.md  database_setup.sql  \cf12 edgemesh\cf0   go.mod                  \cf12 keadm\cf0     MAINTAINERS     \cf12 mappers\cf0    \cf12 pkg\cf0         \cf12 staging\cf1\highlight2 
\par \cf12\highlight0 binaries\cf0      CHANGELOG.md  \cf12 common\cf0               \cf12 docs\cf0                 \cf12 edgesite\cf0   go.sum                  LICENSE  MAINTAINERS.md  \cf12 _output\cf0    README.md  \cf12 tests\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# nohup  _output/local/bin/edgecore --edgecluster > edgecore.logs 2>&1 &\cf1\highlight2 
\par \cf0\highlight0 [1] 7420\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# TAIL -F EDGEC\cf1\highlight2 
\par \cf0\highlight0 TAIL: command not found\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# tail -f edgecore.logs\cf1\highlight2 
\par \cf0\highlight0     "lastheartbeat": "2021-12-22T05:45:38Z"\cf1\highlight2 
\par \cf0\highlight0   \}\cf1\highlight2 
\par \cf0\highlight0\}\cf1\highlight2 
\par \cf0\highlight0 ), error: invalid character '\{' after top-level value\cf1\highlight2 
\par \cf0\highlight0 E1222 06:20:17.955387    7420 imitator.go:108] [imitator] delete error: <nil>\cf1\highlight2 
\par \cf0\highlight0 E1222 06:20:18.667565    7420 imitator.go:108] [imitator] delete error: <nil>\cf1\highlight2 
\par \cf0\highlight0 E1222 06:20:19.429749    7420 imitator.go:108] [imitator] delete error: <nil>\cf1\highlight2 
\par \cf0\highlight0 E1222 06:20:20.507256    7420 imitator.go:108] [imitator] delete error: <nil>\cf1\highlight2 
\par \cf0\highlight0 E1222 06:20:21.592892    7420 imitator.go:108] [imitator] delete error: <nil>\cf1\highlight2 
\par \cf0\highlight0 E1222 06:20:22.734151    7420 imitator.go:108] [imitator] delete error: <nil>\cf1\highlight2 
\par \cf0\highlight0 E1222 06:20:23.856482    7420 imitator.go:108] [imitator] delete error: <nil>\cf1\highlight2 
\par \cf0\highlight0 E1222 06:20:25.027927    7420 imitator.go:108] [imitator] delete error: <nil>\cf1\highlight2 
\par \cf0\highlight0 E1222 06:20:26.259730    7420 imitator.go:108] [imitator] delete error: <nil>\cf1\highlight2 
\par \cf0\highlight0 E1222 06:20:26.859961    7420 edgecluster_state_reporter.go:226] Error in unmarshall edgecluster state json: (\{\cf1\highlight2 
\par \cf0\highlight0   "node-e": \{\cf1\highlight2 
\par \cf0\highlight0     "activemissions": [\cf1\highlight2 
\par \cf0\highlight0       "resource-frontend-svc",\cf1\highlight2 
\par \cf0\highlight0       "command-label-node",\cf1\highlight2 
\par \cf0\highlight0       "command-cp-unknown",\cf1\highlight2 
\par \cf0\highlight0       "resource-known-pvc",\cf1\highlight2 
\par \cf0\highlight0       "resource-known-pv",\cf1\highlight2 
\par \cf0\highlight0       "resource-receiver-svc",\cf1\highlight2 
\par \cf0\highlight0       "command-create-ns-face",\cf1\highlight2 
\par \cf0\highlight0       "resource-secret",\cf1\highlight2 
\par \cf0\highlight0       "resource-face-recog-deployment",\cf1\highlight2 
\par \cf0\highlight0       "resource-image-processor-deployment",\cf1\highlight2 
\par \cf0\highlight0       "resource-unknown-pv",\cf1\highlight2 
\par \cf0\highlight0       "resource-face-recog-svc",\cf1\highlight2 
\par \cf0\highlight0       "command-create-configmap",\cf1\highlight2 
\par \cf0\highlight0       "resource-unknown-pvc",\cf1\highlight2 
\par \cf0\highlight0       "resource-frontend-deployment",\cf1\highlight2 
\par \cf0\highlight0       "resource-mysql-deployment",\cf1\highlight2 
\par \cf0\highlight0       "resource-mysql-pv",\cf1\highlight2 
\par \cf0\highlight0       "resource-nsqlookup-deployment",\cf1\highlight2 
\par \cf0\highlight0       "resource-nsqd-svc",\cf1\highlight2 
\par \cf0\highlight0       "resource-mysql-pvc",\cf1\highlight2 
\par \cf0\highlight0       "resource-receiver-deployment"\cf1\highlight2 
\par \cf0\highlight0     ],\cf1\highlight2 
\par \cf0\highlight0     "healthstatus": "healthy",\cf1\highlight2 
\par \cf0\highlight0     "lastheartbeat": "2021-12-22T06:17:22Z",\cf1\highlight2 
\par \cf0\highlight0     "nodes": [\cf1\highlight2 
\par \cf0\highlight0       "node-e"\cf1\highlight2 
\par \cf0\highlight0     ],\cf1\highlight2 
\par \cf0\highlight0     "receivedmissions": [\cf1\highlight2 
\par \cf0\highlight0       "resource-frontend-svc",\cf1\highlight2 
\par \cf0\highlight0       "command-label-node",\cf1\highlight2 
\par \cf0\highlight0       "command-cp-unknown",\cf1\highlight2 
\par \cf0\highlight0       "resource-known-pvc",\cf1\highlight2 
\par \cf0\highlight0       "resource-known-pv",\cf1\highlight2 
\par \cf0\highlight0       "resource-receiver-svc",\cf1\highlight2 
\par \cf0\highlight0       "command-create-ns-face",\cf1\highlight2 
\par \cf0\highlight0       "resource-secret",\cf1\highlight2 
\par \cf0\highlight0       "command-receiver-port-forward",\cf1\highlight2 
\par \cf0\highlight0       "command-frontend-port-forward",\cf1\highlight2 
\par \cf0\highlight0       "resource-face-recog-deployment",\cf1\highlight2 
\par \cf0\highlight0       "resource-image-processor-deployment",\cf1\highlight2 
\par \cf0\highlight0       "resource-unknown-pv",\cf1\highlight2 
\par \cf0\highlight0       "resource-face-recog-svc",\cf1\highlight2 
\par \cf0\highlight0       "command-create-configmap",\cf1\highlight2 
\par \cf0\highlight0       "resource-unknown-pvc",\cf1\highlight2 
\par \cf0\highlight0       "resource-frontend-deployment",\cf1\highlight2 
\par \cf0\highlight0       "command-mysql-port-forward",\cf1\highlight2 
\par \cf0\highlight0       "resource-mysql-deployment",\cf1\highlight2 
\par \cf0\highlight0       "resource-mysql-pv",\cf1\highlight2 
\par \cf0\highlight0       "resource-nsqlookup-deployment",\cf1\highlight2 
\par \cf0\highlight0       "resource-nsqd-svc",\cf1\highlight2 
\par \cf0\highlight0       "resource-mysql-pvc",\cf1\highlight2 
\par \cf0\highlight0       "resource-receiver-deployment"\cf1\highlight2 
\par \cf0\highlight0     ]\cf1\highlight2 
\par \cf0\highlight0   \}\cf1\highlight2 
\par \cf0\highlight0\}\cf1\highlight2 
\par \cf0\highlight0\{\cf1\highlight2 
\par \cf0\highlight0   "node-f": \{\cf1\highlight2 
\par \cf0\highlight0     "healthstatus": "disconnected",\cf1\highlight2 
\par \cf0\highlight0     "lastheartbeat": "2021-12-22T05:45:38Z"\cf1\highlight2 
\par \cf0\highlight0   \}\cf1\highlight2 
\par \cf0\highlight0\}\cf1\highlight2 
\par \cf0\highlight0 ), error: invalid character '\{' after top-level value\cf1\highlight2 
\par \cf0\highlight0 E1222 06:20:27.401607    7420 imitator.go:108] [imitator] delete error: <nil>\cf1\highlight2 
\par \cf0\highlight0 E1222 06:20:28.660072    7420 imitator.go:108] [imitator] delete error: <nil>\cf1\highlight2 
\par \cf0\highlight0 E1222 06:20:29.947784    7420 imitator.go:108] [imitator] delete error: <nil>\cf1\highlight2 
\par \cf0\highlight0 E1222 06:20:31.188328    7420 imitator.go:108] [imitator] delete error: <nil>\cf1\highlight2 
\par \cf0\highlight0 E1222 06:20:32.349519    7420 imitator.go:108] [imitator] delete error: <nil>\cf1\highlight2 
\par \cf0\highlight0 I1222 06:20:32.406755    7420 mission_deployer.go:158] The content of mission command-cp-known is reverted.\cf1\highlight2 
\par \cf0\highlight0 ^C\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# nohup  _output/local/bin/cloudcore > cloudcore.logs 2>&1 &\cf1\highlight2 
\par \cf0\highlight0 [2] 11753\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# tail -f cloudcore.logs\cf1\highlight2 
\par \cf0\highlight0 I1222 06:20:56.464394   11753 synccontroller.go:148] ObjectSync node-f.e60aa94e-0f30-4d84-a9d0-9f9a411b29a5 will be deleted since node node-f has been deleted\cf1\highlight2 
\par \cf0\highlight0 I1222 06:20:56.476350   11753 synccontroller.go:148] ObjectSync node-f.5a101a0d-42bc-4591-8395-b66b9dd5cb98 will be deleted since node node-f has been deleted\cf1\highlight2 
\par \cf0\highlight0 I1222 06:20:56.489402   11753 synccontroller.go:148] ObjectSync node-e.aaf7dbd1-0cf1-4e48-92ad-080009f77f71 will be deleted since node node-e has been deleted\cf1\highlight2 
\par \cf0\highlight0 I1222 06:20:56.504663   11753 synccontroller.go:148] ObjectSync node-e.268f17c6-377e-4f4a-9467-632c1b915b74 will be deleted since node node-e has been deleted\cf1\highlight2 
\par \cf0\highlight0 I1222 06:20:56.522606   11753 synccontroller.go:148] ObjectSync node-f.ed97f892-f958-4339-b19d-397e52cd32bc will be deleted since node node-f has been deleted\cf1\highlight2 
\par \cf0\highlight0 I1222 06:20:56.546543   11753 synccontroller.go:148] ObjectSync node-e.bf36eaf8-3c51-47db-a5e7-2d818beb065c will be deleted since node node-e has been deleted\cf1\highlight2 
\par \cf0\highlight0 I1222 06:20:56.569821   11753 synccontroller.go:148] ObjectSync node-f.05ed32f5-986a-42f0-9065-031ad832dff5 will be deleted since node node-f has been deleted\cf1\highlight2 
\par \cf0\highlight0 I1222 06:20:56.617842   11753 synccontroller.go:148] ObjectSync node-e.d4fdaaa5-02d4-4a55-911d-fce8d4017cac will be deleted since node node-e has been deleted\cf1\highlight2 
\par \cf0\highlight0 E1222 06:20:56.762346   11753 objectsync.go:60] failed to get obj(gvr:edgeclusters.kubeedge.io/v1, Resource=missions,namespace:default,name:command-cp-known), missions.edgeclusters.kubeedge.io "command-cp-known" not found\cf1\highlight2 
\par \cf0\highlight0 I1222 06:20:57.188722   11753 upstream.go:63] Start upstream devicecontroller\cf1\highlight2 
\par \cf0\highlight0 ^C\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get edgecluster\cf1\highlight2 
\par \cf0\highlight0 NAME     LASTHEARBEAT   HEALTHSTATUS   SUBEDGECLUSTERS   RECEIVED_MISSIONS                                                                                   MATCHED_MISSIONS\cf1\highlight2 
\par \cf0\highlight0 node-e   4s             healthy\cf1\highlight2 
\par \cf0\highlight0 node-f   9s             healthy                          ["resource-unknown-pv","resource-receiver-svc","resource-nsqlookup-deployment","resource-secret"]   ["resource-unknown-pv","resource-receiver-svc","resource-nsqlookup-deployment","resource-secret"]\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get edgecluster\cf1\highlight2 
\par \cf0\highlight0 NAME     LASTHEARBEAT   HEALTHSTATUS   SUBEDGECLUSTERS   RECEIVED_MISSIONS                                                                                                                                   MATCHED_MISSIONS\cf1\highlight2 
\par \cf0\highlight0 node-e   4s             healthy                                                                                                                                             \cf1\highlight2 
\par \cf0\highlight0 node-f   10s            healthy                          ["resource-secret","resource-mysql-service","resource-unknown-pvc","resource-unknown-pv","resource-receiver-svc","resource-nsqlookup-deployment"]   ["resource-secret","resource-mysql-service","resource-unknown-pvc","resource-unknown-pv","resource-receiver-svc","resource-nsqlookup-deployment"]\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get edgecluster\cf1\highlight2 
\par \cf0\highlight0 NAME     LASTHEARBEAT   HEALTHSTATUS   SUBEDGECLUSTERS   RECEIVED_MISSIONS                                                                                                           MATCHED_MISSIONS\cf1\highlight2 
\par \cf0\highlight0 node-e   5s             healthy                                                                                                                                             \cf1\highlight2 
\par \cf0\highlight0 node-f   9s             healthy                          ["resource-nsqlookup-deployment","resource-secret","resource-mysql-service","resource-unknown-pvc","resource-unknown-pv"]   ["resource-nsqlookup-deployment","resource-secret","resource-mysql-service","resource-unknown-pvc","resource-unknown-pv"]\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get edgecluster\cf1\highlight2 
\par \cf0\highlight0 NAME     LASTHEARBEAT   HEALTHSTATUS   SUBEDGECLUSTERS   RECEIVED_MISSIONS                                                                                                           MATCHED_MISSIONS\cf1\highlight2 
\par \cf0\highlight0 node-e   9s             healthy                                                                                                                                             \cf1\highlight2 
\par \cf0\highlight0 node-f   13s            healthy                          ["resource-nsqlookup-deployment","resource-secret","resource-mysql-service","resource-unknown-pvc","resource-unknown-pv"]   ["resource-nsqlookup-deployment","resource-secret","resource-mysql-service","resource-unknown-pvc","resource-unknown-pv"]\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get edgecluster\cf1\highlight2 
\par \cf0\highlight0 NAME     LASTHEARBEAT   HEALTHSTATUS   SUBEDGECLUSTERS   RECEIVED_MISSIONS                                                                                                           MATCHED_MISSIONS\cf1\highlight2 
\par \cf0\highlight0 node-e   11s            healthy                                                                                                                                             \cf1\highlight2 
\par \cf0\highlight0 node-f   2s             healthy                          ["resource-unknown-pv","resource-nsqlookup-deployment","resource-secret","resource-mysql-service","resource-unknown-pvc"]   ["resource-unknown-pv","resource-nsqlookup-deployment","resource-secret","resource-mysql-service","resource-unknown-pvc"]\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get edgecluster\cf1\highlight2 
\par \cf0\highlight0 NAME     LASTHEARBEAT   HEALTHSTATUS   SUBEDGECLUSTERS   RECEIVED_MISSIONS                                                                                                             MATCHED_MISSIONS\cf1\highlight2 
\par \cf0\highlight0 node-e   9s             healthy                          ["resource-unknown-pv","resource-nsqd-svc","resource-nsqlookup-service","resource-nsqd-deployment","resource-receiver-svc"]   ["resource-unknown-pv","resource-nsqd-svc","resource-nsqlookup-service","resource-nsqd-deployment","resource-receiver-svc"]\cf1\highlight2 
\par \cf0\highlight0 node-f   2s             healthy                          ["resource-nsqlookup-deployment","resource-mysql-service","resource-unknown-pvc"]                                             ["resource-nsqlookup-deployment","resource-mysql-service","resource-unknown-pvc"]\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get edgecluster\cf1\highlight2 
\par \cf0\highlight0 NAME     LASTHEARBEAT   HEALTHSTATUS   SUBEDGECLUSTERS   RECEIVED_MISSIONS                                                                                                             MATCHED_MISSIONS\cf1\highlight2 
\par \cf0\highlight0 node-e   10s            healthy                          ["resource-unknown-pv","resource-nsqd-svc","resource-nsqlookup-service","resource-nsqd-deployment","resource-receiver-svc"]   ["resource-unknown-pv","resource-nsqd-svc","resource-nsqlookup-service","resource-nsqd-deployment","resource-receiver-svc"]\cf1\highlight2 
\par \cf0\highlight0 node-f   3s             healthy                          ["resource-nsqlookup-deployment","resource-mysql-service","resource-unknown-pvc"]                                             ["resource-nsqlookup-deployment","resource-mysql-service","resource-unknown-pvc"]\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get edgecluster\cf1\highlight2 
\par \cf0\highlight0 NAME     LASTHEARBEAT   HEALTHSTATUS   SUBEDGECLUSTERS   RECEIVED_MISSIONS                                                                                                             MATCHED_MISSIONS\cf1\highlight2 
\par \cf0\highlight0 node-e   11s            healthy                          ["resource-unknown-pv","resource-nsqd-svc","resource-nsqlookup-service","resource-nsqd-deployment","resource-receiver-svc"]   ["resource-unknown-pv","resource-nsqd-svc","resource-nsqlookup-service","resource-nsqd-deployment","resource-receiver-svc"]\cf1\highlight2 
\par \cf0\highlight0 node-f   4s             healthy                          ["resource-nsqlookup-deployment","resource-mysql-service","resource-unknown-pvc"]                                             ["resource-nsqlookup-deployment","resource-mysql-service","resource-unknown-pvc"]\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get edgecluster\cf1\highlight2 
\par \cf0\highlight0 NAME     LASTHEARBEAT   HEALTHSTATUS   SUBEDGECLUSTERS   RECEIVED_MISSIONS                                                                                                                                                                                                                                                                                                                                                 MATCHED_MISSIONS\cf1\highlight2 
\par \cf0\highlight0 node-e   11s            healthy                          ["resource-unknown-pv","command-cp-unknown","resource-receiver-svc","command-create-configmap","resource-mysql-service","command-create-ns-face","command-frontend-port-forward","command-label-node","resource-nsqlookup-deployment","resource-secret","command-cp-known","resource-unknown-pvc","command-mysql-port-forward","command-receiver-port-forward"]   ["resource-unknown-pv","command-cp-unknown","resource-receiver-svc","command-create-configmap","resource-mysql-service","command-create-ns-face","command-label-node","resource-nsqlookup-deployment","resource-secret","command-cp-known","resource-unknown-pvc"]\cf1\highlight2 
\par \cf0\highlight0 node-f   7s             healthy                          ["resource-nsqlookup-deployment","resource-mysql-service","resource-unknown-pvc"]                                                                                                                                                                                                                                                                                 ["resource-nsqlookup-deployment","resource-mysql-service","resource-unknown-pvc"]\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get edgecluster\cf1\highlight2 
\par \cf0\highlight0 NAME     LASTHEARBEAT   HEALTHSTATUS   SUBEDGECLUSTERS   RECEIVED_MISSIONS                                                                                                                                                                                                                                                                                                                                                 MATCHED_MISSIONS\cf1\highlight2 
\par \cf0\highlight0 node-e   14s            healthy                          ["resource-unknown-pv","command-cp-unknown","resource-receiver-svc","command-create-configmap","resource-mysql-service","command-create-ns-face","command-frontend-port-forward","command-label-node","resource-nsqlookup-deployment","resource-secret","command-cp-known","resource-unknown-pvc","command-mysql-port-forward","command-receiver-port-forward"]   ["resource-unknown-pv","command-cp-unknown","resource-receiver-svc","command-create-configmap","resource-mysql-service","command-create-ns-face","command-label-node","resource-nsqlookup-deployment","resource-secret","command-cp-known","resource-unknown-pvc"]\cf1\highlight2 
\par \cf0\highlight0 node-f   10s            healthy                          ["resource-nsqlookup-deployment","resource-mysql-service","resource-unknown-pvc"]                                                                                                                                                                                                                                                                                 ["resource-nsqlookup-deployment","resource-mysql-service","resource-unknown-pvc"]\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get edgecluster\cf1\highlight2 
\par \cf0\highlight0 NAME     LASTHEARBEAT   HEALTHSTATUS   SUBEDGECLUSTERS   RECEIVED_MISSIONS                                                                                                                                                                                                                                                                                                                         MATCHED_MISSIONS\cf1\highlight2 
\par \cf0\highlight0 node-e   1s             healthy                          ["command-label-node","resource-nsqlookup-deployment","resource-secret","command-cp-known","resource-unknown-pvc","command-mysql-port-forward","command-receiver-port-forward","resource-unknown-pv","command-cp-unknown","command-create-configmap","resource-mysql-service","command-create-ns-face","command-frontend-port-forward"]   ["command-label-node","resource-nsqlookup-deployment","resource-secret","command-cp-known","resource-unknown-pvc","resource-unknown-pv","command-cp-unknown","command-create-configmap","resource-mysql-service","command-create-ns-face"]\cf1\highlight2 
\par \cf0\highlight0 node-f   12s            healthy                          ["resource-nsqlookup-deployment","resource-mysql-service","resource-unknown-pvc"]                                                                                                                                                                                                                                                         ["resource-nsqlookup-deployment","resource-mysql-service","resource-unknown-pvc"]\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get edgecluster\cf1\highlight2 
\par \cf0\highlight0 NAME     LASTHEARBEAT   HEALTHSTATUS   SUBEDGECLUSTERS   RECEIVED_MISSIONS                                                                                                                                                                                                                                                                                                                         MATCHED_MISSIONS\cf1\highlight2 
\par \cf0\highlight0 node-e   2s             healthy                          ["command-label-node","resource-nsqlookup-deployment","resource-secret","command-cp-known","resource-unknown-pvc","command-mysql-port-forward","command-receiver-port-forward","resource-unknown-pv","command-cp-unknown","command-create-configmap","resource-mysql-service","command-create-ns-face","command-frontend-port-forward"]   ["command-label-node","resource-nsqlookup-deployment","resource-secret","command-cp-known","resource-unknown-pvc","resource-unknown-pv","command-cp-unknown","command-create-configmap","resource-mysql-service","command-create-ns-face"]\cf1\highlight2 
\par \cf0\highlight0 node-f   1s             healthy                          ["resource-nsqlookup-deployment","resource-mysql-service","resource-unknown-pvc"]                                                                                                                                                                                                                                                         ["resource-nsqlookup-deployment","resource-mysql-service","resource-unknown-pvc"]\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get edgecluster\cf1\highlight2 
\par \cf0\highlight0 NAME     LASTHEARBEAT   HEALTHSTATUS   SUBEDGECLUSTERS   RECEIVED_MISSIONS                                                                                                                                                                                                                                                                                 MATCHED_MISSIONS\cf1\highlight2 
\par \cf0\highlight0 node-e   0s             healthy                          ["resource-nsqlookup-deployment","command-label-node","resource-unknown-pvc","command-mysql-port-forward","command-receiver-port-forward","command-cp-known","command-cp-unknown","resource-mysql-service","command-create-ns-face","command-frontend-port-forward","command-create-configmap"]   ["resource-nsqlookup-deployment","command-label-node","resource-unknown-pvc","command-cp-known","command-cp-unknown","resource-mysql-service","command-create-ns-face","command-create-configmap"]\cf1\highlight2 
\par \cf0\highlight0 node-f   0s             healthy                          ["resource-nsqlookup-deployment","resource-mysql-service","resource-unknown-pvc"]                                                                                                                                                                                                                 ["resource-nsqlookup-deployment","resource-mysql-service","resource-unknown-pvc"]\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get edgecluster\cf1\highlight2 
\par \cf0\highlight0 NAME     LASTHEARBEAT   HEALTHSTATUS   SUBEDGECLUSTERS   RECEIVED_MISSIONS                                                                                                                                                                                                                                                                                 MATCHED_MISSIONS\cf1\highlight2 
\par \cf0\highlight0 node-e   3s             healthy                          ["resource-nsqlookup-deployment","command-label-node","resource-unknown-pvc","command-mysql-port-forward","command-receiver-port-forward","command-cp-known","command-cp-unknown","resource-mysql-service","command-create-ns-face","command-frontend-port-forward","command-create-configmap"]   ["resource-nsqlookup-deployment","command-label-node","resource-unknown-pvc","command-cp-known","command-cp-unknown","resource-mysql-service","command-create-ns-face","command-create-configmap"]\cf1\highlight2 
\par \cf0\highlight0 node-f   3s             healthy                          ["resource-nsqlookup-deployment","resource-mysql-service","resource-unknown-pvc"]                                                                                                                                                                                                                 ["resource-nsqlookup-deployment","resource-mysql-service","resource-unknown-pvc"]\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get edgecluster\cf1\highlight2 
\par \cf0\highlight0 NAME     LASTHEARBEAT   HEALTHSTATUS   SUBEDGECLUSTERS   RECEIVED_MISSIONS                                                                                                                                                                                                                                                                                 MATCHED_MISSIONS\cf1\highlight2 
\par \cf0\highlight0 node-e   9s             healthy                          ["command-create-configmap","resource-mysql-service","command-create-ns-face","command-frontend-port-forward","command-label-node","resource-nsqlookup-deployment","command-cp-known","resource-unknown-pvc","command-mysql-port-forward","command-receiver-port-forward","command-cp-unknown"]   ["command-create-configmap","resource-mysql-service","command-create-ns-face","command-label-node","resource-nsqlookup-deployment","command-cp-known","resource-unknown-pvc","command-cp-unknown"]\cf1\highlight2 
\par \cf0\highlight0 node-f   10s            healthy                          ["resource-mysql-service","resource-unknown-pvc","resource-nsqlookup-deployment"]                                                                                                                                                                                                                 ["resource-mysql-service","resource-unknown-pvc","resource-nsqlookup-deployment"]\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# tail -f cloudcore.logs\cf1\highlight2 
\par \cf0\highlight0 W1222 06:26:50.168104   11753 upstream.go:751] message: ef772cf9-c639-443e-99a5-8e703a619c19 process failure, mission resource-mysql-service not found\cf1\highlight2 
\par \cf0\highlight0 W1222 06:27:08.116232   11753 upstream.go:751] message: 8858a374-34f8-499a-91ba-c762a78c614b process failure, mission resource-nsqlookup-deployment not found\cf1\highlight2 
\par \cf0\highlight0 W1222 06:27:11.357906   11753 upstream.go:751] message: 8aa93292-3acc-4abc-9d5d-5ff33c163e62 process failure, mission resource-mysql-service not found\cf1\highlight2 
\par \cf0\highlight0 W1222 06:27:13.332666   11753 upstream.go:751] message: 036dfd9c-a09f-421c-8a97-31adf93972d8 process failure, mission resource-mysql-service not found\cf1\highlight2 
\par \cf0\highlight0 W1222 06:27:13.410917   11753 upstream.go:751] message: 10f71dcb-f625-4ff1-8698-f3b1fe5409cf process failure, mission resource-nsqlookup-deployment not found\cf1\highlight2 
\par \cf0\highlight0 W1222 06:27:17.730712   11753 upstream.go:751] message: 6985ae58-470a-48fa-a60c-101e3854acad process failure, mission command-receiver-port-forward not found\cf1\highlight2 
\par \cf0\highlight0 W1222 06:27:24.118336   11753 upstream.go:751] message: 5518780b-cb3e-48f7-a552-905d5ef68282 process failure, mission command-frontend-port-forward not found\cf1\highlight2 
\par \cf0\highlight0 W1222 06:27:26.007634   11753 upstream.go:751] message: a938b695-d274-4bfb-a132-9d96a90c74c2 process failure, mission resource-mysql-service not found\cf1\highlight2 
\par \cf0\highlight0 W1222 06:27:27.842017   11753 upstream.go:751] message: bbc88df5-c84e-4e58-b3b4-fc23728f4ca6 process failure, mission resource-nsqlookup-deployment not found\cf1\highlight2 
\par \cf0\highlight0 W1222 06:27:33.281301   11753 upstream.go:751] message: c03af723-ef79-4890-83a2-4bd9f0c725c2 process failure, mission resource-unknown-pvc not found\cf1\highlight2 
\par \cf0\highlight0 W1222 06:27:44.552536   11753 upstream.go:751] message: 5e44cc56-7341-425c-abcf-c35a2d1cd377 process failure, mission resource-mysql-service not found\cf1\highlight2 
\par \cf0\highlight0 W1222 06:27:46.492877   11753 upstream.go:751] message: 4a622193-1f1e-4704-8b39-9467b14e1328 process failure, mission resource-mysql-service not found\cf1\highlight2 
\par \cf0\highlight0 W1222 06:27:48.369624   11753 upstream.go:751] message: 71b1b810-d12c-48fb-a501-a6f316ed342c process failure, mission resource-nsqlookup-deployment not found\cf1\highlight2 
\par \cf0\highlight0 ^C\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# Kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 Kubectl: command not found\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# Kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 Kubectl: command not found\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          20h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          20h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-rjmfz           0/1     Pending   0          3s    <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          20h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          20h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-rjmfz           0/1     Pending   0          6s    <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          20h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          20h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-rjmfz           0/1     Pending   0          7s    <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          20h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          20h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-rjmfz           0/1     Pending   0          8s    <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          20h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          20h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-rjmfz           0/1     Pending   0          34s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          20h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          20h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-rjmfz           0/1     Pending   0          35s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          20h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          20h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-rjmfz           0/1     Pending   0          36s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          20h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          20h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-rjmfz           0/1     Pending   0          38s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          20h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          20h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-rjmfz           0/1     Pending   0          64s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          20h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          20h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-rjmfz           0/1     Pending   0          65s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          20h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          20h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-rjmfz           0/1     Pending   0          66s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          20h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          20h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-rjmfz           0/1     Pending   0          94s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          20h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          20h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-rjmfz           0/1     Pending   0          98s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          20h   10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          20h   10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          20h   192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE    IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-rjmfz           0/1     Pending   0          2m8s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-n2sjj       0/1     Pending   0          8s     <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          20h    10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          20h    10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          20h    192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          20h    192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          20h    192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          20h    192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          20h    192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          20h    192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE     IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-rjmfz           0/1     Pending   0          2m12s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-n2sjj       0/1     Pending   0          12s     <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          20h     10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          20h     10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          20h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          20h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          20h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          20h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          20h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          20h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl taint nodes node-d node-role.kubernetes.io/master-\cf1\highlight2 
\par \cf0\highlight0 node/node-d untainted\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE     IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-rjmfz           0/1     Pending   0          3m27s   <none>          <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-n2sjj       1/1     Running   0          87s     10.32.0.4       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5         1/1     Running   0          20h     10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2         1/1     Running   0          20h     10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                      1/1     Running   0          20h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d            1/1     Running   0          20h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d   1/1     Running   0          20h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                 1/1     Running   0          20h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d            1/1     Running   0          20h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                  2/2     Running   1          20h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                                          READY   STATUS              RESTARTS   AGE     IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 face          face-recog-698dc6b88f-4b84p                   0/1     ContainerCreating   0          3m44s   <none>          node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          frontend-56b6fd5f8c-6g5b7                     0/1     ContainerCreating   0          3m31s   <none>          node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          image-processor-deployment-7d6d54d996-zcjws   1/1     Running             0          5m56s   10.32.0.7       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-rjmfz                        0/1     ContainerCreating   0          10m     <none>          node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          nsqd-54667b87f4-6htxs                         1/1     Running             0          6m11s   10.32.0.6       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-n2sjj                    1/1     Running             0          8m21s   10.32.0.4       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          receiver-deployment-74b5c7d449-gr868          1/1     Running             0          6m29s   10.32.0.5       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5                      1/1     Running             0          20h     10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2                      1/1     Running             0          20h     10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                                   1/1     Running             0          20h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d                         1/1     Running             0          20h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d                1/1     Running             0          20h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                              1/1     Running             0          20h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d                         1/1     Running             0          20h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                               2/2     Running             1          20h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ps -ef | grep cloudcore\cf1\highlight2 
\par \cf0\highlight0 root     11753  1899  5 06:20 pts/0    00:01:15 _output/local/bin/\cf9 cloudcore\cf1\highlight2 
\par \cf0\highlight0 root     32306  1899  0 06:42 pts/0    00:00:00 grep --color=auto \cf9 cloudcore\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# tail -f cloudcore.logs\cf1\highlight2 
\par \cf0\highlight0 W1222 06:33:24.416112   11753 upstream.go:751] message: 963b49c5-586f-48e0-88f5-d402bcaae670 process failure, mission resource-nsqlookup-deployment not found\cf1\highlight2 
\par \cf0\highlight0 W1222 06:34:02.409629   11753 upstream.go:751] message: 498d3e45-fa99-4ac9-8261-2ddeec60338c process failure, mission command-mysql-port-forward not found\cf1\highlight2 
\par \cf0\highlight0 W1222 06:34:40.429635   11753 upstream.go:751] message: 4fc8eb72-260e-458c-82ae-221d496034dc process failure, mission command-receiver-port-forward not found\cf1\highlight2 
\par \cf0\highlight0 W1222 06:34:45.443266   11753 upstream.go:751] message: 3b4998be-c10b-48f2-8b4e-4654384e5904 process failure, mission command-frontend-port-forward not found\cf1\highlight2 
\par \cf0\highlight0 W1222 06:35:55.545291   11753 upstream.go:751] message: 9f368563-906d-4a8c-b85b-081ef35c93da process failure, mission command-mysql-port-forward not found\cf1\highlight2 
\par \cf0\highlight0 W1222 06:36:34.460844   11753 upstream.go:751] message: 8ea1737c-c36c-47cf-8af2-14d1dcb59af4 process failure, mission command-receiver-port-forward not found\cf1\highlight2 
\par \cf0\highlight0 W1222 06:36:39.230084   11753 upstream.go:751] message: 682890a8-df20-4dfc-a11c-dd970afb7222 process failure, mission command-frontend-port-forward not found\cf1\highlight2 
\par \cf0\highlight0 W1222 06:37:48.027570   11753 upstream.go:751] message: c85dcb2b-5e53-49b3-8616-e925d04b2478 process failure, mission command-mysql-port-forward not found\cf1\highlight2 
\par \cf0\highlight0 W1222 06:38:25.336808   11753 upstream.go:751] message: d615c10d-c532-4bf5-a209-dea7fab4bda6 process failure, mission command-receiver-port-forward not found\cf1\highlight2 
\par \cf0\highlight0 W1222 06:38:31.283539   11753 upstream.go:751] message: ed3522ee-d6e2-4b32-a470-f528d6c59eec process failure, mission command-frontend-port-forward not found\cf1\highlight2 
\par \cf0\highlight0 ^C\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get edgecluster\cf1\highlight2 
\par \cf0\highlight0 NAME     LASTHEARBEAT   HEALTHSTATUS   SUBEDGECLUSTERS   RECEIVED_MISSIONS                                                                                                                                                                                                                                                                                 MATCHED_MISSIONS\cf1\highlight2 
\par \cf0\highlight0 node-e   8s             healthy                          ["command-cp-known","resource-unknown-pvc","command-mysql-port-forward","command-receiver-port-forward","command-cp-unknown","command-create-configmap","resource-mysql-service","command-create-ns-face","command-frontend-port-forward","command-label-node","resource-nsqlookup-deployment"]   ["command-cp-known","resource-unknown-pvc","command-cp-unknown","command-create-configmap","resource-mysql-service","command-create-ns-face","command-label-node","resource-nsqlookup-deployment"]\cf1\highlight2 
\par \cf0\highlight0 node-f   1s             healthy                          ["resource-nsqlookup-deployment","resource-mysql-service","resource-unknown-pvc"]                                                                                                                                                                                                                 ["resource-nsqlookup-deployment","resource-mysql-service","resource-unknown-pvc"]\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# tail -f cloudcore.logs\cf1\highlight2 
\par \cf0\highlight0 W1222 06:33:24.416112   11753 upstream.go:751] message: 963b49c5-586f-48e0-88f5-d402bcaae670 process failure, mission resource-nsqlookup-deployment not found\cf1\highlight2 
\par \cf0\highlight0 W1222 06:34:02.409629   11753 upstream.go:751] message: 498d3e45-fa99-4ac9-8261-2ddeec60338c process failure, mission command-mysql-port-forward not found\cf1\highlight2 
\par \cf0\highlight0 W1222 06:34:40.429635   11753 upstream.go:751] message: 4fc8eb72-260e-458c-82ae-221d496034dc process failure, mission command-receiver-port-forward not found\cf1\highlight2 
\par \cf0\highlight0 W1222 06:34:45.443266   11753 upstream.go:751] message: 3b4998be-c10b-48f2-8b4e-4654384e5904 process failure, mission command-frontend-port-forward not found\cf1\highlight2 
\par \cf0\highlight0 W1222 06:35:55.545291   11753 upstream.go:751] message: 9f368563-906d-4a8c-b85b-081ef35c93da process failure, mission command-mysql-port-forward not found\cf1\highlight2 
\par \cf0\highlight0 W1222 06:36:34.460844   11753 upstream.go:751] message: 8ea1737c-c36c-47cf-8af2-14d1dcb59af4 process failure, mission command-receiver-port-forward not found\cf1\highlight2 
\par \cf0\highlight0 W1222 06:36:39.230084   11753 upstream.go:751] message: 682890a8-df20-4dfc-a11c-dd970afb7222 process failure, mission command-frontend-port-forward not found\cf1\highlight2 
\par \cf0\highlight0 W1222 06:37:48.027570   11753 upstream.go:751] message: c85dcb2b-5e53-49b3-8616-e925d04b2478 process failure, mission command-mysql-port-forward not found\cf1\highlight2 
\par \cf0\highlight0 W1222 06:38:25.336808   11753 upstream.go:751] message: d615c10d-c532-4bf5-a209-dea7fab4bda6 process failure, mission command-receiver-port-forward not found\cf1\highlight2 
\par \cf0\highlight0 W1222 06:38:31.283539   11753 upstream.go:751] message: ed3522ee-d6e2-4b32-a470-f528d6c59eec process failure, mission command-frontend-port-forward not found\cf1\highlight2 
\par \cf0\highlight0 ^C\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# ps -ef | grep cloudcore\cf1\highlight2 
\par \cf0\highlight0 root      6207  1899  0 06:44 pts/0    00:00:00 grep --color=auto \cf9 cloudcore\cf1\highlight2 
\par \cf0\highlight0 root     11753  1899  5 06:20 pts/0    00:01:20 _output/local/bin/\cf9 cloudcore\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kill -9 11753\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# rm -rf cloudcore.logs\cf1\highlight2 
\par \cf0\highlight0 [2]+  Killed                  nohup _output/local/bin/cloudcore > cloudcore.logs 2>&1\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# nohup  _output/local/bin/cloudcore > cloudcore.logs 2>&1 &\cf1\highlight2 
\par \cf0\highlight0 [2] 8495\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# tail -f cloudcore.logs\cf1\highlight2 
\par \cf0\highlight0 I1222 06:44:50.187458    8495 synccontroller.go:148] ObjectSync node-e.a8433e03-e0b0-4c96-867b-09f8a7d4f9c5 will be deleted since node node-e has been deleted\cf1\highlight2 
\par \cf0\highlight0 I1222 06:44:50.203830    8495 synccontroller.go:148] ObjectSync node-e.9ff11146-9b89-4ace-8f88-7b2d46b34066 will be deleted since node node-e has been deleted\cf1\highlight2 
\par \cf0\highlight0 I1222 06:44:50.229388    8495 synccontroller.go:148] ObjectSync node-e.cdadaa03-efe1-4bef-9d2b-5d58cfe4c5e0 will be deleted since node node-e has been deleted\cf1\highlight2 
\par \cf0\highlight0 I1222 06:44:50.406323    8495 upstream.go:63] Start upstream devicecontroller\cf1\highlight2 
\par \cf0\highlight0 I1222 06:44:51.908977    8495 channelq.go:227] Message queue and store for edge node node-e are already exist\cf1\highlight2 
\par \cf0\highlight0 I1222 06:44:51.909114    8495 messagehandler.go:293] edge node node-e for project e632aba927ea4ac2b575ec1603d56f10 connected\cf1\highlight2 
\par \cf0\highlight0 W1222 06:44:51.909237    8495 upstream.go:188] parse message: 6d4101ba-ae6c-47cc-b15f-74797aaed1b7 resource type with error: resource type not found,\cf1\highlight2 
\par \cf0\highlight0 I1222 06:44:51.986094    8495 channelq.go:227] Message queue and store for edge node node-f are already exist\cf1\highlight2 
\par \cf0\highlight0 I1222 06:44:51.986197    8495 messagehandler.go:293] edge node node-f for project e632aba927ea4ac2b575ec1603d56f10 connected\cf1\highlight2 
\par \cf0\highlight0 W1222 06:44:51.986265    8495 upstream.go:188] parse message: 0a8e442a-078d-43a9-8fa0-68d014098bca resource type with error: resource type not found,\cf1\highlight2 
\par \cf0\highlight0 ^C\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get edgecluster\cf1\highlight2 
\par \cf0\highlight0 NAME     LASTHEARBEAT   HEALTHSTATUS   SUBEDGECLUSTERS   RECEIVED_MISSIONS                                                                                                                                                                                                                                                                                 MATCHED_MISSIONS\cf1\highlight2 
\par \cf0\highlight0 node-e   4s             healthy                          ["command-cp-unknown","command-frontend-port-forward","command-create-configmap","resource-mysql-service","command-create-ns-face","command-label-node","resource-nsqlookup-deployment","command-receiver-port-forward","command-cp-known","resource-unknown-pvc","command-mysql-port-forward"]   ["command-cp-unknown","command-create-configmap","resource-mysql-service","command-create-ns-face","command-label-node","resource-nsqlookup-deployment","command-cp-known","resource-unknown-pvc"]\cf1\highlight2 
\par \cf0\highlight0 node-f   12s            healthy                          ["resource-mysql-service","resource-unknown-pvc","resource-nsqlookup-deployment"]                                                                                                                                                                                                                 ["resource-mysql-service","resource-unknown-pvc","resource-nsqlookup-deployment"]\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get edgecluster\cf1\highlight2 
\par \cf0\highlight0 NAME     LASTHEARBEAT   HEALTHSTATUS   SUBEDGECLUSTERS   RECEIVED_MISSIONS                                                                                                                                                                                                                                                                                 MATCHED_MISSIONS\cf1\highlight2 
\par \cf0\highlight0 node-e   7s             healthy                          ["command-cp-unknown","command-frontend-port-forward","command-create-configmap","resource-mysql-service","command-create-ns-face","command-label-node","resource-nsqlookup-deployment","command-receiver-port-forward","command-cp-known","resource-unknown-pvc","command-mysql-port-forward"]   ["command-cp-unknown","command-create-configmap","resource-mysql-service","command-create-ns-face","command-label-node","resource-nsqlookup-deployment","command-cp-known","resource-unknown-pvc"]\cf1\highlight2 
\par \cf0\highlight0 node-f   2s             healthy                          ["resource-nsqlookup-deployment","resource-mysql-service","resource-unknown-pvc"]                                                                                                                                                                                                                 ["resource-nsqlookup-deployment","resource-mysql-service","resource-unknown-pvc"]\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                                          READY   STATUS             RESTARTS   AGE     IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 face          face-recog-698dc6b88f-4b84p                   0/1     CrashLoopBackOff   4          7m55s   10.32.0.8       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          frontend-56b6fd5f8c-6g5b7                     1/1     Running            0          7m42s   10.32.0.9       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          image-processor-deployment-7d6d54d996-zcjws   1/1     Running            0          10m     10.32.0.7       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-rjmfz                        1/1     Running            0          14m     10.32.0.10      node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          nsqd-54667b87f4-6htxs                         1/1     Running            0          10m     10.32.0.6       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-n2sjj                    1/1     Running            0          12m     10.32.0.4       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          receiver-deployment-74b5c7d449-gr868          1/1     Running            0          10m     10.32.0.5       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5                      1/1     Running            0          20h     10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2                      1/1     Running            0          20h     10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                                   1/1     Running            0          20h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d                         1/1     Running            0          20h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d                1/1     Running            0          20h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                              1/1     Running            0          20h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d                         1/1     Running            0          20h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                               2/2     Running            1          20h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# kubectl get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                                          READY   STATUS             RESTARTS   AGE     IP              NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 face          face-recog-698dc6b88f-4b84p                   0/1     CrashLoopBackOff   4          8m10s   10.32.0.8       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          frontend-56b6fd5f8c-6g5b7                     1/1     Running            0          7m57s   10.32.0.9       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          image-processor-deployment-7d6d54d996-zcjws   1/1     Running            0          10m     10.32.0.7       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-rjmfz                        1/1     Running            0          14m     10.32.0.10      node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          nsqd-54667b87f4-6htxs                         1/1     Running            0          10m     10.32.0.6       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-n2sjj                    1/1     Running            0          12m     10.32.0.4       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 face          receiver-deployment-74b5c7d449-gr868          1/1     Running            0          10m     10.32.0.5       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-jf6n5                      1/1     Running            0          20h     10.32.0.3       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-npvb2                      1/1     Running            0          20h     10.32.0.2       node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-d                                   1/1     Running            0          20h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-d                         1/1     Running            0          20h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-d                1/1     Running            0          20h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-pw55z                              1/1     Running            0          20h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-d                         1/1     Running            0          20h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   weave-net-6l2zr                               2/2     Running            1          20h     192.168.1.210   node-d   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# tail -f cloudcore.logs\cf1\highlight2 
\par \cf0\highlight0 I1222 06:44:50.187458    8495 synccontroller.go:148] ObjectSync node-e.a8433e03-e0b0-4c96-867b-09f8a7d4f9c5 will be deleted since node node-e has been deleted\cf1\highlight2 
\par \cf0\highlight0 I1222 06:44:50.203830    8495 synccontroller.go:148] ObjectSync node-e.9ff11146-9b89-4ace-8f88-7b2d46b34066 will be deleted since node node-e has been deleted\cf1\highlight2 
\par \cf0\highlight0 I1222 06:44:50.229388    8495 synccontroller.go:148] ObjectSync node-e.cdadaa03-efe1-4bef-9d2b-5d58cfe4c5e0 will be deleted since node node-e has been deleted\cf1\highlight2 
\par \cf0\highlight0 I1222 06:44:50.406323    8495 upstream.go:63] Start upstream devicecontroller\cf1\highlight2 
\par \cf0\highlight0 I1222 06:44:51.908977    8495 channelq.go:227] Message queue and store for edge node node-e are already exist\cf1\highlight2 
\par \cf0\highlight0 I1222 06:44:51.909114    8495 messagehandler.go:293] edge node node-e for project e632aba927ea4ac2b575ec1603d56f10 connected\cf1\highlight2 
\par \cf0\highlight0 W1222 06:44:51.909237    8495 upstream.go:188] parse message: 6d4101ba-ae6c-47cc-b15f-74797aaed1b7 resource type with error: resource type not found,\cf1\highlight2 
\par \cf0\highlight0 I1222 06:44:51.986094    8495 channelq.go:227] Message queue and store for edge node node-f are already exist\cf1\highlight2 
\par \cf0\highlight0 I1222 06:44:51.986197    8495 messagehandler.go:293] edge node node-f for project e632aba927ea4ac2b575ec1603d56f10 connected\cf1\highlight2 
\par \cf0\highlight0 W1222 06:44:51.986265    8495 upstream.go:188] parse message: 0a8e442a-078d-43a9-8fa0-68d014098bca resource type with error: resource type not found,\cf1\highlight2 
\par \cf0\highlight0 ^[[A^C\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge# tail -f cloudcore.logs\cf1\highlight2 
\par \cf0\highlight0 I1222 06:44:50.187458    8495 synccontroller.go:148] ObjectSync node-e.a8433e03-e0b0-4c96-867b-09f8a7d4f9c5 will be deleted since node node-e has been deleted\cf1\highlight2 
\par \cf0\highlight0 I1222 06:44:50.203830    8495 synccontroller.go:148] ObjectSync node-e.9ff11146-9b89-4ace-8f88-7b2d46b34066 will be deleted since node node-e has been deleted\cf1\highlight2 
\par \cf0\highlight0 I1222 06:44:50.229388    8495 synccontroller.go:148] ObjectSync node-e.cdadaa03-efe1-4bef-9d2b-5d58cfe4c5e0 will be deleted since node node-e has been deleted\cf1\highlight2 
\par \cf0\highlight0 I1222 06:44:50.406323    8495 upstream.go:63] Start upstream devicecontroller\cf1\highlight2 
\par \cf0\highlight0 I1222 06:44:51.908977    8495 channelq.go:227] Message queue and store for edge node node-e are already exist\cf1\highlight2 
\par \cf0\highlight0 I1222 06:44:51.909114    8495 messagehandler.go:293] edge node node-e for project e632aba927ea4ac2b575ec1603d56f10 connected\cf1\highlight2 
\par \cf0\highlight0 W1222 06:44:51.909237    8495 upstream.go:188] parse message: 6d4101ba-ae6c-47cc-b15f-74797aaed1b7 resource type with error: resource type not found,\cf1\highlight2 
\par \cf0\highlight0 I1222 06:44:51.986094    8495 channelq.go:227] Message queue and store for edge node node-f are already exist\cf1\highlight2 
\par \cf0\highlight0 I1222 06:44:51.986197    8495 messagehandler.go:293] edge node node-f for project e632aba927ea4ac2b575ec1603d56f10 connected\cf1\highlight2 
\par \cf0\highlight0 W1222 06:44:51.986265    8495 upstream.go:188] parse message: 0a8e442a-078d-43a9-8fa0-68d014098bca resource type with error: resource type not found,\cf1\highlight2 
\par \cf0\highlight0 ^C\cf1\highlight2 
\par \cf0\highlight0 root@node-d:~/go/src/github.com/kubeedge#\cf1\highlight2 
\par \pard\cf0\highlight0\f2 
\par }
 