{\rtf1\ansi\ansicpg1252\deff0\deflang1033{\fonttbl{\f0\fmodern Consolas;}{\f1\fmodern\fcharset0 Consolas;}{\f2\fnil\fcharset129 Courier New;}}
{\colortbl ;\red20\green20\blue20;\red142\green142\blue142;\red16\green158\blue98;\red148\green146\blue12;\red170\green51\blue170;\red56\green136\blue159;\red170\green85\blue85;\red18\green124\blue155;}
\viewkind4\uc1\pard\f0\fs20     \u9484?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9488?\cf1\highlight2 
\par \cf0\highlight0     \u9474?                 \cf3\f1\bullet  MobaXterm Personal Edition v21.4 \bullet\cf0\f0                  \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?               \cf4 (SSH client, X server and network tools)\cf0                \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?                                                                      \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474? \u8594? SSH session to \cf5 root@192.168.1.211                        \cf0           \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?\f1    \bullet  Direct SSH      :  \cf3\f0 v\cf0                                              \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?\f1    \bullet  SSH compression :  \cf3\f0 v\cf0                                              \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?\f1    \bullet  SSH-browser     :  \cf3\f0 v\cf0                                              \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?\f1    \bullet  X11-forwarding  :  \cf3\f0 v\cf0   (remote display is forwarded through SSH)  \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?                                                                      \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474? \u8594? For more info, ctrl+click on \cf6\ul help\cf0\ulnone  or visit our \cf6\ul website\cf0\ulnone .            \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9492?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9496?\cf1\highlight2 
\par 
\par \cf0\highlight0 Welcome to Ubuntu 18.04.5 LTS (GNU/Linux 4.15.0-159-generic x86_64)\cf1\highlight2 
\par 
\par \cf0\highlight0  * Documentation:  https://help.ubuntu.com\cf1\highlight2 
\par \cf0\highlight0  * Management:     https://landscape.canonical.com\cf1\highlight2 
\par \cf0\highlight0  * Support:        https://ubuntu.com/advantage\cf1\highlight2 
\par 
\par \cf0\highlight0   System information as of Thu Dec  9 05:12:10 UTC 2021\cf1\highlight2 
\par 
\par \cf0\highlight0   System load:  0.03              Processes:             313\cf1\highlight2 
\par \cf0\highlight0   Usage of /:   8.7% of 72.83GB   Users logged in:       0\cf1\highlight2 
\par \cf0\highlight0   Memory usage: 3%                IP address for ens160: 192.168.1.211\cf1\highlight2 
\par \cf0\highlight0   Swap usage:   0%\cf1\highlight2 
\par 
\par \cf0\highlight0  * Super-optimized for small spaces - read how we shrank the memory\cf1\highlight2 
\par \cf0\highlight0    footprint of MicroK8s to make it the smallest full K8s around.\cf1\highlight2 
\par 
\par \cf0\highlight0    https://ubuntu.com/blog/microk8s-memory-optimisation\cf1\highlight2 
\par 
\par \cf0\highlight0  * Canonical Livepatch is available for installation.\cf1\highlight2 
\par \cf0\highlight0    - Reduce system reboots and improve kernel security. Activate at:\cf1\highlight2 
\par \cf0\highlight0      https://ubuntu.com/livepatch\cf1\highlight2 
\par 
\par \cf0\highlight0 20 updates can be applied immediately.\cf1\highlight2 
\par \cf0\highlight0 3 of these updates are standard security updates.\cf1\highlight2 
\par \cf0\highlight0 To see these additional updates run: apt list --upgradable\cf1\highlight2 
\par 
\par \cf0\highlight0 New release '20.04.3 LTS' available.\cf1\highlight2 
\par \cf0\highlight0 Run 'do-release-upgrade' to upgrade to it.\cf1\highlight2 
\par 
\par 
\par \cf0\highlight0 *** System restart required ***\cf1\highlight2 
\par \cf0\highlight0 Last login: Thu Dec  9 05:05:54 2021 from 192.168.1.1\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | tee /etc/apt/sources.list.d/docker.list > /dev/null\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# apt-get update -y\cf1\highlight2 
\par \cf0\highlight0 Get:1 https://download.docker.com/linux/ubuntu bionic InRelease [64.4 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:2 https://download.docker.com/linux/ubuntu bionic/stable amd64 Packages [21.8 kB]\cf1\highlight2 
\par \cf0\highlight0 Hit:3 http://in.archive.ubuntu.com/ubuntu bionic InRelease\cf1\highlight2 
\par \cf0\highlight0 Hit:4 http://in.archive.ubuntu.com/ubuntu bionic-updates InRelease\cf1\highlight2 
\par \cf0\highlight0 Hit:5 http://in.archive.ubuntu.com/ubuntu bionic-backports InRelease\cf1\highlight2 
\par \cf0\highlight0 Hit:6 http://in.archive.ubuntu.com/ubuntu bionic-security InRelease\cf1\highlight2 
\par \cf0\highlight0 Fetched 86.3 kB in 1s (83.8 kB/s)\cf1\highlight2 
\par \cf0\highlight0 Reading package lists... Done\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# apt-get install -y docker-ce docker-ce-cli containerd.io\cf1\highlight2 
\par \cf0\highlight0 Reading package lists... Done\cf1\highlight2 
\par \cf0\highlight0 Building dependency tree\cf1\highlight2 
\par \cf0\highlight0 Reading state information... Done\cf1\highlight2 
\par \cf0\highlight0 The following additional packages will be installed:\cf1\highlight2 
\par \cf0\highlight0   dbus-user-session docker-ce-rootless-extras docker-scan-plugin libltdl7 pigz\cf1\highlight2 
\par \cf0\highlight0 Suggested packages:\cf1\highlight2 
\par \cf0\highlight0   aufs-tools cgroupfs-mount | cgroup-lite\cf1\highlight2 
\par \cf0\highlight0 Recommended packages:\cf1\highlight2 
\par \cf0\highlight0   slirp4netns\cf1\highlight2 
\par \cf0\highlight0 The following NEW packages will be installed:\cf1\highlight2 
\par \cf0\highlight0   containerd.io dbus-user-session docker-ce docker-ce-cli docker-ce-rootless-extras docker-scan-plugin libltdl7 pigz\cf1\highlight2 
\par \cf0\highlight0 0 upgraded, 8 newly installed, 0 to remove and 18 not upgraded.\cf1\highlight2 
\par \cf0\highlight0 Need to get 95.3 MB of archives.\cf1\highlight2 
\par \cf0\highlight0 After this operation, 403 MB of additional disk space will be used.\cf1\highlight2 
\par \cf0\highlight0 Get:1 http://in.archive.ubuntu.com/ubuntu bionic/universe amd64 pigz amd64 2.4-1 [57.4 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:2 https://download.docker.com/linux/ubuntu bionic/stable amd64 containerd.io amd64 1.4.12-1 [23.7 MB]\cf1\highlight2 
\par \cf0\highlight0 Get:3 http://in.archive.ubuntu.com/ubuntu bionic-updates/main amd64 dbus-user-session amd64 1.12.2-1ubuntu1.2 [9,392 B]\cf1\highlight2 
\par \cf0\highlight0 Get:4 http://in.archive.ubuntu.com/ubuntu bionic/main amd64 libltdl7 amd64 2.4.6-2 [38.8 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:5 https://download.docker.com/linux/ubuntu bionic/stable amd64 docker-ce-cli amd64 5:20.10.11~3-0~ubuntu-bionic [38.8 MB]\cf1\highlight2 
\par \cf0\highlight0 Get:6 https://download.docker.com/linux/ubuntu bionic/stable amd64 docker-ce amd64 5:20.10.11~3-0~ubuntu-bionic [21.2 MB]\cf1\highlight2 
\par \cf0\highlight0 Get:7 https://download.docker.com/linux/ubuntu bionic/stable amd64 docker-ce-rootless-extras amd64 5:20.10.11~3-0~ubuntu-bionic [7,920 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:8 https://download.docker.com/linux/ubuntu bionic/stable amd64 docker-scan-plugin amd64 0.9.0~ubuntu-bionic [3,518 kB]\cf1\highlight2 
\par \cf0\highlight0 Fetched 95.3 MB in 9s (10.3 MB/s)\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package pigz.\cf1\highlight2 
\par \cf0\highlight0 (Reading database ... 108238 files and directories currently installed.)\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../0-pigz_2.4-1_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking pigz (2.4-1) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package containerd.io.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../1-containerd.io_1.4.12-1_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking containerd.io (1.4.12-1) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package dbus-user-session.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../2-dbus-user-session_1.12.2-1ubuntu1.2_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking dbus-user-session (1.12.2-1ubuntu1.2) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package docker-ce-cli.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../3-docker-ce-cli_5%3a20.10.11~3-0~ubuntu-bionic_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking docker-ce-cli (5:20.10.11~3-0~ubuntu-bionic) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package docker-ce.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../4-docker-ce_5%3a20.10.11~3-0~ubuntu-bionic_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking docker-ce (5:20.10.11~3-0~ubuntu-bionic) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package docker-ce-rootless-extras.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../5-docker-ce-rootless-extras_5%3a20.10.11~3-0~ubuntu-bionic_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking docker-ce-rootless-extras (5:20.10.11~3-0~ubuntu-bionic) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package docker-scan-plugin.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../6-docker-scan-plugin_0.9.0~ubuntu-bionic_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking docker-scan-plugin (0.9.0~ubuntu-bionic) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package libltdl7:amd64.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../7-libltdl7_2.4.6-2_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking libltdl7:amd64 (2.4.6-2) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up containerd.io (1.4.12-1) ...\cf1\highlight2 
\par \cf0\highlight0 Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service \u8594? /lib/systemd/system/containerd.service.\cf1\highlight2 
\par \cf0\highlight0 /usr/sbin/policy-rc.d returned 101, not running 'start containerd.service'\cf1\highlight2 
\par \cf0\highlight0 Setting up docker-scan-plugin (0.9.0~ubuntu-bionic) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up dbus-user-session (1.12.2-1ubuntu1.2) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up libltdl7:amd64 (2.4.6-2) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up docker-ce-cli (5:20.10.11~3-0~ubuntu-bionic) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up pigz (2.4-1) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up docker-ce (5:20.10.11~3-0~ubuntu-bionic) ...\cf1\highlight2 
\par \cf0\highlight0 Created symlink /etc/systemd/system/multi-user.target.wants/docker.service \u8594? /lib/systemd/system/docker.service.\cf1\highlight2 
\par \cf0\highlight0 Created symlink /etc/systemd/system/sockets.target.wants/docker.socket \u8594? /lib/systemd/system/docker.socket.\cf1\highlight2 
\par \cf0\highlight0 invoke-rc.d: policy-rc.d denied execution of start.\cf1\highlight2 
\par \cf0\highlight0 Setting up docker-ce-rootless-extras (5:20.10.11~3-0~ubuntu-bionic) ...\cf1\highlight2 
\par \cf0\highlight0 Processing triggers for systemd (237-3ubuntu10.52) ...\cf1\highlight2 
\par \cf0\highlight0 Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\cf1\highlight2 
\par \cf0\highlight0 Processing triggers for ureadahead (0.100.0-21) ...\cf1\highlight2 
\par \cf0\highlight0 Processing triggers for libc-bin (2.27-3ubuntu1.4) ...\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# mkdir /etc/docker\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# cat <<EOF | tee /etc/docker/daemon.json\cf1\highlight2 
\par \cf0\highlight0 > \{\cf1\highlight2 
\par \cf0\highlight0 >\cf1\highlight2 
\par \cf0\highlight0 > "exec-opts": ["native.cgroupdriver=systemd"],\cf1\highlight2 
\par \cf0\highlight0 >\cf1\highlight2 
\par \cf0\highlight0 > "log-driver": "json-file",\cf1\highlight2 
\par \cf0\highlight0 >\cf1\highlight2 
\par \cf0\highlight0 > "log-opts": \{\cf1\highlight2 
\par \cf0\highlight0 >\cf1\highlight2 
\par \cf0\highlight0 > "max-size": "100m"\cf1\highlight2 
\par \cf0\highlight0 >\cf1\highlight2 
\par \cf0\highlight0 > \},\cf1\highlight2 
\par \cf0\highlight0 >\cf1\highlight2 
\par \cf0\highlight0 > "storage-driver": "overlay2"\cf1\highlight2 
\par \cf0\highlight0 >\cf1\highlight2 
\par \cf0\highlight0 > \}\cf1\highlight2 
\par \cf0\highlight0 >\cf1\highlight2 
\par \cf0\highlight0 > EOF\cf1\highlight2 
\par \cf0\highlight0\{\cf1\highlight2 
\par 
\par \cf0\highlight0 "exec-opts": ["native.cgroupdriver=systemd"],\cf1\highlight2 
\par 
\par \cf0\highlight0 "log-driver": "json-file",\cf1\highlight2 
\par 
\par \cf0\highlight0 "log-opts": \{\cf1\highlight2 
\par 
\par \cf0\highlight0 "max-size": "100m"\cf1\highlight2 
\par 
\par \cf0\highlight0\},\cf1\highlight2 
\par 
\par \cf0\highlight0 "storage-driver": "overlay2"\cf1\highlight2 
\par 
\par \cf0\highlight0\}\cf1\highlight2 
\par 
\par \cf0\highlight0 root@node-b:~# systemctl enable docker\cf1\highlight2 
\par \cf0\highlight0 Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.\cf1\highlight2 
\par \cf0\highlight0 Executing: /lib/systemd/systemd-sysv-install enable docker\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# systemctl daemon-reload\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# systemctl restart docker\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# systemctl status docker\cf1\highlight2 
\par \cf3\highlight0\u9679?\cf0  docker.service - Docker Application Container Engine\cf1\highlight2 
\par \cf0\highlight0    Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled)\cf1\highlight2 
\par \cf0\highlight0    Active: \cf3 active (running)\cf0  since Thu 2021-12-09 05:14:33 UTC; 365ms ago\cf1\highlight2 
\par \cf0\highlight0      Docs: https://docs.docker.com\cf1\highlight2 
\par \cf0\highlight0  Main PID: 23938 (dockerd)\cf1\highlight2 
\par \cf0\highlight0     Tasks: 14\cf1\highlight2 
\par \cf0\highlight0    CGroup: /system.slice/docker.service\cf1\highlight2 
\par \cf0\highlight0            \u9492?\u9472?23938 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock\cf1\highlight2 
\par 
\par \cf0\highlight0 Dec 09 05:14:32 node-b dockerd[23938]: time="2021-12-09T05:14:32.528621193Z" level=info msg="ClientConn switching balancer to \\"pick_firs\cf1\highlight2 
\par \cf0\highlight0 Dec 09 05:14:32 node-b dockerd[23938]: time="2021-12-09T05:14:32.651492253Z" level=warning msg="Your kernel does not support swap memory\cf1\highlight2 
\par \cf0\highlight0 Dec 09 05:14:32 node-b dockerd[23938]: time="2021-12-09T05:14:32.651843989Z" level=warning msg="Your kernel does not support CPU realtime\cf1\highlight2 
\par \cf0\highlight0 Dec 09 05:14:32 node-b dockerd[23938]: time="2021-12-09T05:14:32.652332825Z" level=info msg="Loading containers: start."\cf1\highlight2 
\par \cf0\highlight0 Dec 09 05:14:33 node-b dockerd[23938]: time="2021-12-09T05:14:33.038123366Z" level=info msg="Default bridge (docker0) is assigned with an\cf1\highlight2 
\par \cf0\highlight0 Dec 09 05:14:33 node-b dockerd[23938]: time="2021-12-09T05:14:33.312150977Z" level=info msg="Loading containers: done."\cf1\highlight2 
\par \cf0\highlight0 Dec 09 05:14:33 node-b dockerd[23938]: time="2021-12-09T05:14:33.373063960Z" level=info msg="Docker daemon" commit=847da18 graphdriver(s)\cf1\highlight2 
\par \cf0\highlight0 Dec 09 05:14:33 node-b dockerd[23938]: time="2021-12-09T05:14:33.373531743Z" level=info msg="Daemon has completed initialization"\cf1\highlight2 
\par \cf0\highlight0 Dec 09 05:14:33 node-b systemd[1]: Started Docker Application Container Engine.\cf1\highlight2 
\par \cf0\highlight0 Dec 09 05:14:33 node-b dockerd[23938]: time="2021-12-09T05:14:33.445352999Z" level=info msg="API listen on /var/run/docker.sock"\cf1\highlight2 
\par 
\par \cf0\highlight0 root@node-b:~# swapoff -a\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# cat <<EOF | tee /etc/modules-load.d/k8s.conf\cf1\highlight2 
\par \cf0\highlight0 > br_netfilter\cf1\highlight2 
\par \cf0\highlight0 >\cf1\highlight2 
\par \cf0\highlight0 > EOF\cf1\highlight2 
\par \cf0\highlight0 br_netfilter\cf1\highlight2 
\par 
\par \cf0\highlight0 root@node-b:~# cat <<EOF | tee /etc/sysctl.d/k8s.conf\cf1\highlight2 
\par \cf0\highlight0 > net.bridge.bridge-nf-call-ip6tables = 1\cf1\highlight2 
\par \cf0\highlight0 >\cf1\highlight2 
\par \cf0\highlight0 > net.bridge.bridge-nf-call-iptables = 1\cf1\highlight2 
\par \cf0\highlight0 >\cf1\highlight2 
\par \cf0\highlight0 > EOF\cf1\highlight2 
\par \cf0\highlight0 net.bridge.bridge-nf-call-ip6tables = 1\cf1\highlight2 
\par 
\par \cf0\highlight0 net.bridge.bridge-nf-call-iptables = 1\cf1\highlight2 
\par 
\par \cf0\highlight0 root@node-b:~# sysctl --system\cf1\highlight2 
\par \cf0\highlight0 * Applying /etc/sysctl.d/10-console-messages.conf ...\cf1\highlight2 
\par \cf0\highlight0 kernel.printk = 4 4 1 7\cf1\highlight2 
\par \cf0\highlight0 * Applying /etc/sysctl.d/10-ipv6-privacy.conf ...\cf1\highlight2 
\par \cf0\highlight0 net.ipv6.conf.all.use_tempaddr = 2\cf1\highlight2 
\par \cf0\highlight0 net.ipv6.conf.default.use_tempaddr = 2\cf1\highlight2 
\par \cf0\highlight0 * Applying /etc/sysctl.d/10-kernel-hardening.conf ...\cf1\highlight2 
\par \cf0\highlight0 kernel.kptr_restrict = 1\cf1\highlight2 
\par \cf0\highlight0 * Applying /etc/sysctl.d/10-link-restrictions.conf ...\cf1\highlight2 
\par \cf0\highlight0 fs.protected_hardlinks = 1\cf1\highlight2 
\par \cf0\highlight0 fs.protected_symlinks = 1\cf1\highlight2 
\par \cf0\highlight0 * Applying /etc/sysctl.d/10-lxd-inotify.conf ...\cf1\highlight2 
\par \cf0\highlight0 fs.inotify.max_user_instances = 1024\cf1\highlight2 
\par \cf0\highlight0 * Applying /etc/sysctl.d/10-magic-sysrq.conf ...\cf1\highlight2 
\par \cf0\highlight0 kernel.sysrq = 176\cf1\highlight2 
\par \cf0\highlight0 * Applying /etc/sysctl.d/10-network-security.conf ...\cf1\highlight2 
\par \cf0\highlight0 net.ipv4.conf.default.rp_filter = 1\cf1\highlight2 
\par \cf0\highlight0 net.ipv4.conf.all.rp_filter = 1\cf1\highlight2 
\par \cf0\highlight0 net.ipv4.tcp_syncookies = 1\cf1\highlight2 
\par \cf0\highlight0 * Applying /etc/sysctl.d/10-ptrace.conf ...\cf1\highlight2 
\par \cf0\highlight0 kernel.yama.ptrace_scope = 1\cf1\highlight2 
\par \cf0\highlight0 * Applying /etc/sysctl.d/10-zeropage.conf ...\cf1\highlight2 
\par \cf0\highlight0 vm.mmap_min_addr = 65536\cf1\highlight2 
\par \cf0\highlight0 * Applying /usr/lib/sysctl.d/50-default.conf ...\cf1\highlight2 
\par \cf0\highlight0 net.ipv4.conf.all.promote_secondaries = 1\cf1\highlight2 
\par \cf0\highlight0 net.core.default_qdisc = fq_codel\cf1\highlight2 
\par \cf0\highlight0 * Applying /etc/sysctl.d/99-sysctl.conf ...\cf1\highlight2 
\par \cf0\highlight0 * Applying /etc/sysctl.d/k8s.conf ...\cf1\highlight2 
\par \cf0\highlight0 net.bridge.bridge-nf-call-ip6tables = 1\cf1\highlight2 
\par \cf0\highlight0 net.bridge.bridge-nf-call-iptables = 1\cf1\highlight2 
\par \cf0\highlight0 * Applying /etc/sysctl.conf ...\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# apt-get update -y\cf1\highlight2 
\par \cf0\highlight0 Hit:1 https://download.docker.com/linux/ubuntu bionic InRelease\cf1\highlight2 
\par \cf0\highlight0 Hit:2 http://in.archive.ubuntu.com/ubuntu bionic InRelease\cf1\highlight2 
\par \cf0\highlight0 Hit:3 http://in.archive.ubuntu.com/ubuntu bionic-updates InRelease\cf1\highlight2 
\par \cf0\highlight0 Hit:4 http://in.archive.ubuntu.com/ubuntu bionic-backports InRelease\cf1\highlight2 
\par \cf0\highlight0 Hit:5 http://in.archive.ubuntu.com/ubuntu bionic-security InRelease\cf1\highlight2 
\par \cf0\highlight0 Reading package lists... Done\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# apt-get install -y apt-transport-https ca-certificates curl\cf1\highlight2 
\par \cf0\highlight0 Reading package lists... Done\cf1\highlight2 
\par \cf0\highlight0 Building dependency tree\cf1\highlight2 
\par \cf0\highlight0 Reading state information... Done\cf1\highlight2 
\par \cf0\highlight0 ca-certificates is already the newest version (20210119~18.04.2).\cf1\highlight2 
\par \cf0\highlight0 curl is already the newest version (7.58.0-2ubuntu3.16).\cf1\highlight2 
\par \cf0\highlight0 apt-transport-https is already the newest version (1.6.14).\cf1\highlight2 
\par \cf0\highlight0 0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | tee /etc/apt/sources.list.d/kubernetes.list\cf1\highlight2 
\par \cf0\highlight0 deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# apt-get update -y\cf1\highlight2 
\par \cf0\highlight0 Hit:1 http://in.archive.ubuntu.com/ubuntu bionic InRelease\cf1\highlight2 
\par \cf0\highlight0 Hit:2 https://download.docker.com/linux/ubuntu bionic InRelease\cf1\highlight2 
\par \cf0\highlight0 Hit:3 http://in.archive.ubuntu.com/ubuntu bionic-updates InRelease\cf1\highlight2 
\par \cf0\highlight0 Hit:4 http://in.archive.ubuntu.com/ubuntu bionic-backports InRelease\cf1\highlight2 
\par \cf0\highlight0 Hit:5 http://in.archive.ubuntu.com/ubuntu bionic-security InRelease\cf1\highlight2 
\par \cf0\highlight0 Get:6 https://packages.cloud.google.com/apt kubernetes-xenial InRelease [9,383 B]\cf1\highlight2 
\par \cf0\highlight0 Get:7 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 Packages [51.5 kB]\cf1\highlight2 
\par \cf0\highlight0 Fetched 60.9 kB in 2s (39.3 kB/s)\cf1\highlight2 
\par \cf0\highlight0 Reading package lists... Done\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# apt install -y kubeadm=1.21.1-00 kubelet=1.21.1-00 kubectl=1.21.1-00 --allow-change-held-packages\cf1\highlight2 
\par \cf0\highlight0 Reading package lists... Done\cf1\highlight2 
\par \cf0\highlight0 Building dependency tree\cf1\highlight2 
\par \cf0\highlight0 Reading state information... Done\cf1\highlight2 
\par \cf0\highlight0 The following additional packages will be installed:\cf1\highlight2 
\par \cf0\highlight0   conntrack cri-tools kubernetes-cni socat\cf1\highlight2 
\par \cf0\highlight0 The following NEW packages will be installed:\cf1\highlight2 
\par \cf0\highlight0   conntrack cri-tools kubeadm kubectl kubelet kubernetes-cni socat\cf1\highlight2 
\par \cf0\highlight0 0 upgraded, 7 newly installed, 0 to remove and 18 not upgraded.\cf1\highlight2 
\par \cf0\highlight0 Need to get 73.5 MB of archives.\cf1\highlight2 
\par \cf0\highlight0 After this operation, 316 MB of additional disk space will be used.\cf1\highlight2 
\par \cf0\highlight0 Get:1 http://in.archive.ubuntu.com/ubuntu bionic/main amd64 conntrack amd64 1:1.4.4+snapshot20161117-6ubuntu2 [30.6 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:2 http://in.archive.ubuntu.com/ubuntu bionic/main amd64 socat amd64 1.7.3.2-2ubuntu2 [342 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:3 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 cri-tools amd64 1.19.0-00 [11.2 MB]\cf1\highlight2 
\par \cf0\highlight0 Get:4 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubernetes-cni amd64 0.8.7-00 [25.0 MB]\cf1\highlight2 
\par \cf0\highlight0 Get:5 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubelet amd64 1.21.1-00 [18.8 MB]\cf1\highlight2 
\par \cf0\highlight0 Get:6 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubectl amd64 1.21.1-00 [9,225 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:7 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubeadm amd64 1.21.1-00 [8,985 kB]\cf1\highlight2 
\par \cf0\highlight0 Fetched 73.5 MB in 21s (3,545 kB/s)\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package conntrack.\cf1\highlight2 
\par \cf0\highlight0 (Reading database ... 108502 files and directories currently installed.)\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../0-conntrack_1%3a1.4.4+snapshot20161117-6ubuntu2_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking conntrack (1:1.4.4+snapshot20161117-6ubuntu2) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package cri-tools.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../1-cri-tools_1.19.0-00_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking cri-tools (1.19.0-00) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package kubernetes-cni.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../2-kubernetes-cni_0.8.7-00_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking kubernetes-cni (0.8.7-00) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package socat.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../3-socat_1.7.3.2-2ubuntu2_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking socat (1.7.3.2-2ubuntu2) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package kubelet.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../4-kubelet_1.21.1-00_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking kubelet (1.21.1-00) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package kubectl.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../5-kubectl_1.21.1-00_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking kubectl (1.21.1-00) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package kubeadm.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../6-kubeadm_1.21.1-00_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking kubeadm (1.21.1-00) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up conntrack (1:1.4.4+snapshot20161117-6ubuntu2) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up kubernetes-cni (0.8.7-00) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up cri-tools (1.19.0-00) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up socat (1.7.3.2-2ubuntu2) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up kubelet (1.21.1-00) ...\cf1\highlight2 
\par \cf0\highlight0 Created symlink /etc/systemd/system/multi-user.target.wants/kubelet.service \u8594? /lib/systemd/system/kubelet.service.\cf1\highlight2 
\par \cf0\highlight0 /usr/sbin/policy-rc.d returned 101, not running 'start kubelet.service'\cf1\highlight2 
\par \cf0\highlight0 Setting up kubectl (1.21.1-00) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up kubeadm (1.21.1-00) ...\cf1\highlight2 
\par \cf0\highlight0 Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# apt-mark hold kubelet kubeadm kubectl\cf1\highlight2 
\par \cf0\highlight0 kubelet set on hold.\cf1\highlight2 
\par \cf0\highlight0 kubeadm set on hold.\cf1\highlight2 
\par \cf0\highlight0 kubectl set on hold.\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# systemctl enable --now kubelet\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# systemctl status kubelet\cf1\highlight2 
\par \cf0\highlight0\u9679? kubelet.service - kubelet: The Kubernetes Node Agent\cf1\highlight2 
\par \cf0\highlight0    Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)\cf1\highlight2 
\par \cf0\highlight0   Drop-In: /etc/systemd/system/kubelet.service.d\cf1\highlight2 
\par \cf0\highlight0            \u9492?\u9472?10-kubeadm.conf\cf1\highlight2 
\par \cf0\highlight0    Active: activating (auto-restart) (Result: exit-code) since Thu 2021-12-09 05:18:23 UTC; 3s ago\cf1\highlight2 
\par \cf0\highlight0      Docs: https://kubernetes.io/docs/home/\cf1\highlight2 
\par \cf0\highlight0   Process: 25261 ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS \cf7 (code\cf1\highlight2 
\par \cf0\highlight0  Main PID: 25261 (code=exited, status=1/FAILURE)\cf1\highlight2 
\par 
\par \cf0\highlight0 root@node-b:~# kubeadm init --pod-network-cidr=10.244.0.0/16\cf1\highlight2 
\par \cf0\highlight0 I1209 05:19:05.000426   25406 version.go:254] remote version is much newer: v1.23.0; falling back to: stable-1.21\cf1\highlight2 
\par \cf0\highlight0 [init] Using Kubernetes version: v1.21.7\cf1\highlight2 
\par \cf0\highlight0 [preflight] Running pre-flight checks\cf1\highlight2 
\par \cf0\highlight0 [preflight] Pulling images required for setting up a Kubernetes cluster\cf1\highlight2 
\par \cf0\highlight0 [preflight] This might take a minute or two, depending on the speed of your internet connection\cf1\highlight2 
\par \cf0\highlight0 [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'\cf1\highlight2 
\par \cf0\highlight0 [certs] Using certificateDir folder "/etc/kubernetes/pki"\cf1\highlight2 
\par \cf0\highlight0 [certs] Generating "ca" certificate and key\cf1\highlight2 
\par \cf0\highlight0 [certs] Generating "apiserver" certificate and key\cf1\highlight2 
\par \cf0\highlight0 [certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local node-b] and IPs [10.96.0.1 192.168.1.211]\cf1\highlight2 
\par \cf0\highlight0 [certs] Generating "apiserver-kubelet-client" certificate and key\cf1\highlight2 
\par \cf0\highlight0 [certs] Generating "front-proxy-ca" certificate and key\cf1\highlight2 
\par \cf0\highlight0 [certs] Generating "front-proxy-client" certificate and key\cf1\highlight2 
\par \cf0\highlight0 [certs] Generating "etcd/ca" certificate and key\cf1\highlight2 
\par \cf0\highlight0 [certs] Generating "etcd/server" certificate and key\cf1\highlight2 
\par \cf0\highlight0 [certs] etcd/server serving cert is signed for DNS names [localhost node-b] and IPs [192.168.1.211 127.0.0.1 ::1]\cf1\highlight2 
\par \cf0\highlight0 [certs] Generating "etcd/peer" certificate and key\cf1\highlight2 
\par \cf0\highlight0 [certs] etcd/peer serving cert is signed for DNS names [localhost node-b] and IPs [192.168.1.211 127.0.0.1 ::1]\cf1\highlight2 
\par \cf0\highlight0 [certs] Generating "etcd/healthcheck-client" certificate and key\cf1\highlight2 
\par \cf0\highlight0 [certs] Generating "apiserver-etcd-client" certificate and key\cf1\highlight2 
\par \cf0\highlight0 [certs] Generating "sa" key and public key\cf1\highlight2 
\par \cf0\highlight0 [kubeconfig] Using kubeconfig folder "/etc/kubernetes"\cf1\highlight2 
\par \cf0\highlight0 [kubeconfig] Writing "admin.conf" kubeconfig file\cf1\highlight2 
\par \cf0\highlight0 [kubeconfig] Writing "kubelet.conf" kubeconfig file\cf1\highlight2 
\par \cf0\highlight0 [kubeconfig] Writing "controller-manager.conf" kubeconfig file\cf1\highlight2 
\par \cf0\highlight0 [kubeconfig] Writing "scheduler.conf" kubeconfig file\cf1\highlight2 
\par \cf0\highlight0 [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"\cf1\highlight2 
\par \cf0\highlight0 [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"\cf1\highlight2 
\par \cf0\highlight0 [kubelet-start] Starting the kubelet\cf1\highlight2 
\par \cf0\highlight0 [control-plane] Using manifest folder "/etc/kubernetes/manifests"\cf1\highlight2 
\par \cf0\highlight0 [control-plane] Creating static Pod manifest for "kube-apiserver"\cf1\highlight2 
\par \cf0\highlight0 [control-plane] Creating static Pod manifest for "kube-controller-manager"\cf1\highlight2 
\par \cf0\highlight0 [control-plane] Creating static Pod manifest for "kube-scheduler"\cf1\highlight2 
\par \cf0\highlight0 [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"\cf1\highlight2 
\par \cf0\highlight0 [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s\cf1\highlight2 
\par \cf0\highlight0 [apiclient] All control plane components are healthy after 18.504408 seconds\cf1\highlight2 
\par \cf0\highlight0 [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace\cf1\highlight2 
\par \cf0\highlight0 [kubelet] Creating a ConfigMap "kubelet-config-1.21" in namespace kube-system with the configuration for the kubelets in the cluster\cf1\highlight2 
\par \cf0\highlight0 [upload-certs] Skipping phase. Please see --upload-certs\cf1\highlight2 
\par \cf0\highlight0 [mark-control-plane] Marking the node node-b as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]\cf1\highlight2 
\par \cf0\highlight0 [mark-control-plane] Marking the node node-b as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]\cf1\highlight2 
\par \cf0\highlight0 [bootstrap-token] Using token: c6wqnt.szluv0w9420gk86e\cf1\highlight2 
\par \cf0\highlight0 [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles\cf1\highlight2 
\par \cf0\highlight0 [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes\cf1\highlight2 
\par \cf0\highlight0 [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\cf1\highlight2 
\par \cf0\highlight0 [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\cf1\highlight2 
\par \cf0\highlight0 [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\cf1\highlight2 
\par \cf0\highlight0 [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace\cf1\highlight2 
\par \cf0\highlight0 [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key\cf1\highlight2 
\par \cf0\highlight0 [addons] Applied essential addon: CoreDNS\cf1\highlight2 
\par \cf0\highlight0 [addons] Applied essential addon: kube-proxy\cf1\highlight2 
\par 
\par \cf0\highlight0 Your Kubernetes control-plane has initialized successfully!\cf1\highlight2 
\par 
\par \cf0\highlight0 To start using your cluster, you need to run the following as a regular user:\cf1\highlight2 
\par 
\par \cf0\highlight0   mkdir -p $HOME/.kube\cf1\highlight2 
\par \cf0\highlight0   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\cf1\highlight2 
\par \cf0\highlight0   sudo chown $(id -u):$(id -g) $HOME/.kube/config\cf1\highlight2 
\par 
\par \cf0\highlight0 Alternatively, if you are the root user, you can run:\cf1\highlight2 
\par 
\par \cf0\highlight0   export KUBECONFIG=/etc/kubernetes/admin.conf\cf1\highlight2 
\par 
\par \cf0\highlight0 You should now deploy a pod network to the cluster.\cf1\highlight2 
\par \cf0\highlight0 Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:\cf1\highlight2 
\par \cf0\highlight0   https://kubernetes.io/docs/concepts/cluster-administration/addons/\cf1\highlight2 
\par 
\par \cf0\highlight0 Then you can join any number of worker nodes by running the following on each as root:\cf1\highlight2 
\par 
\par \cf0\highlight0 kubeadm join 192.168.1.211:6443 --token c6wqnt.szluv0w9420gk86e \\\cf1\highlight2 
\par \cf0\highlight0         --discovery-token-ca-cert-hash sha256:3d249e0ad2c33ef8a1afa2b59b988a74438166f92ea7310571e4af917d2538f7\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# mkdir -p $HOME/.kube\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# chown $(id -u):$(id -g) $HOME/.kube/config\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\cf1\highlight2 
\par \cf4\highlight0 Warning:\cf0  policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+\cf1\highlight2 
\par \cf0\highlight0 podsecuritypolicy.policy/psp.flannel.unprivileged created\cf1\highlight2 
\par \cf0\highlight0 clusterrole.rbac.authorization.k8s.io/flannel created\cf1\highlight2 
\par \cf0\highlight0 clusterrolebinding.rbac.authorization.k8s.io/flannel created\cf1\highlight2 
\par \cf0\highlight0 serviceaccount/flannel created\cf1\highlight2 
\par \cf0\highlight0 configmap/kube-flannel-cfg created\cf1\highlight2 
\par \cf0\highlight0 daemonset.apps/kube-flannel-ds created\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# kubectl get nodes\cf1\highlight2 
\par \cf0\highlight0 NAME     STATUS     ROLES                  AGE   VERSION\cf1\highlight2 
\par \cf0\highlight0 node-b   NotReady   control-plane,master   79s   v1.21.1\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# kubectl get nodes\cf1\highlight2 
\par \cf0\highlight0 NAME     STATUS   ROLES                  AGE     VERSION\cf1\highlight2 
\par \cf0\highlight0 node-b   Ready    control-plane,master   2m20s   v1.21.1\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# sudo wget https://storage.googleapis.com/golang/go1.15.4.linux-amd64.tar.gz\cf1\highlight2 
\par \cf0\highlight0 --2021-12-09 05:23:45--  https://storage.googleapis.com/golang/go1.15.4.linux-amd64.tar.gz\cf1\highlight2 
\par \cf0\highlight0 Resolving storage.googleapis.com (storage.googleapis.com)... 216.58.203.48, 142.250.67.144, 142.250.67.176, ...\cf1\highlight2 
\par \cf0\highlight0 Connecting to storage.googleapis.com (storage.googleapis.com)|216.58.203.48|:443... connected.\cf1\highlight2 
\par \cf0\highlight0 HTTP request sent, awaiting response... 200 OK\cf1\highlight2 
\par \cf0\highlight0 Length: 120906247 (115M) [application/octet-stream]\cf1\highlight2 
\par \cf0\highlight0\f1 Saving to: \lquote go1.15.4.linux-amd64.tar.gz\rquote\cf1\highlight2\f0 
\par 
\par \cf0\highlight0 go1.15.4.linux-amd64.tar.gz        100%[=============================================================>] 115.30M  2.76MB/s    in 32s\cf1\highlight2 
\par 
\par \cf0\highlight0\f1 2021-12-09 05:24:21 (3.55 MB/s) - \lquote go1.15.4.linux-amd64.tar.gz\rquote  saved [120906247/120906247]\cf1\highlight2\f0 
\par 
\par \cf0\highlight0 root@node-b:~# sudo tar -C /usr/local -xzf go1.15.4.linux-amd64.tar.gz\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# sudo echo 'export PATH=$PATH:/usr/local/go/bin' >> $HOME/.profile\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# sudo echo 'export GOPATH=$HOME/gopath' >> $HOME/.profile\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# source $HOME/.profile\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# apt install -y make gcc jq\cf1\highlight2 
\par \cf0\highlight0 Reading package lists... Done\cf1\highlight2 
\par \cf0\highlight0 Building dependency tree\cf1\highlight2 
\par \cf0\highlight0 Reading state information... Done\cf1\highlight2 
\par \cf0\highlight0 make is already the newest version (4.1-9.1ubuntu1).\cf1\highlight2 
\par \cf0\highlight0 make set to manually installed.\cf1\highlight2 
\par \cf0\highlight0 gcc is already the newest version (4:7.4.0-1ubuntu2.3).\cf1\highlight2 
\par \cf0\highlight0 gcc set to manually installed.\cf1\highlight2 
\par \cf0\highlight0 The following NEW packages will be installed:\cf1\highlight2 
\par \cf0\highlight0   jq libjq1 libonig4\cf1\highlight2 
\par \cf0\highlight0 0 upgraded, 3 newly installed, 0 to remove and 21 not upgraded.\cf1\highlight2 
\par \cf0\highlight0 Need to get 276 kB of archives.\cf1\highlight2 
\par \cf0\highlight0 After this operation, 930 kB of additional disk space will be used.\cf1\highlight2 
\par \cf0\highlight0 Get:1 http://in.archive.ubuntu.com/ubuntu bionic/universe amd64 libonig4 amd64 6.7.0-1 [119 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:2 http://in.archive.ubuntu.com/ubuntu bionic/universe amd64 libjq1 amd64 1.5+dfsg-2 [111 kB]\cf1\highlight2 
\par \cf0\highlight0 Get:3 http://in.archive.ubuntu.com/ubuntu bionic/universe amd64 jq amd64 1.5+dfsg-2 [45.6 kB]\cf1\highlight2 
\par \cf0\highlight0 Fetched 276 kB in 1s (349 kB/s)\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package libonig4:amd64.\cf1\highlight2 
\par \cf0\highlight0 (Reading database ... 108576 files and directories currently installed.)\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../libonig4_6.7.0-1_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking libonig4:amd64 (6.7.0-1) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package libjq1:amd64.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../libjq1_1.5+dfsg-2_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking libjq1:amd64 (1.5+dfsg-2) ...\cf1\highlight2 
\par \cf0\highlight0 Selecting previously unselected package jq.\cf1\highlight2 
\par \cf0\highlight0 Preparing to unpack .../jq_1.5+dfsg-2_amd64.deb ...\cf1\highlight2 
\par \cf0\highlight0 Unpacking jq (1.5+dfsg-2) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up libonig4:amd64 (6.7.0-1) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up libjq1:amd64 (1.5+dfsg-2) ...\cf1\highlight2 
\par \cf0\highlight0 Setting up jq (1.5+dfsg-2) ...\cf1\highlight2 
\par \cf0\highlight0 Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\cf1\highlight2 
\par \cf0\highlight0 Processing triggers for libc-bin (2.27-3ubuntu1.4) ...\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# go version\cf1\highlight2 
\par \cf0\highlight0 go version go1.15.4 linux/amd64\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# git clone https://github.com/CentaurusInfra/fornax\cf1\highlight2 
\par \cf0\highlight0 Cloning into 'fornax'...\cf1\highlight2 
\par \cf0\highlight0 remote: Enumerating objects: 50832, done.\cf1\highlight2 
\par \cf0\highlight0 remote: Counting objects: 100% (760/760), done.\cf1\highlight2 
\par \cf0\highlight0 remote: Compressing objects: 100% (432/432), done.\cf1\highlight2 
\par \cf0\highlight0 remote: Total 50832 (delta 360), reused 621 (delta 289), pack-reused 50072\cf1\highlight2 
\par \cf0\highlight0 Receiving objects: 100% (50832/50832), 125.52 MiB | 1.66 MiB/s, done.\cf1\highlight2 
\par \cf0\highlight0 Resolving deltas: 100% (27938/27938), done.\cf1\highlight2 
\par \cf0\highlight0 Checking out files: 100% (8900/8900), done.\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# cd fornax\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# make WHAT=cloudcore\cf1\highlight2 
\par \cf0\highlight0 hack/verify-golang.sh\cf1\highlight2 
\par \cf0\highlight0 go detail version: go version go1.15.4 linux/amd64\cf1\highlight2 
\par \cf0\highlight0 go version: 1.15.4\cf1\highlight2 
\par \cf0\highlight0 KUBEEDGE_OUTPUT_SUBPATH=_output/local hack/make-rules/build.sh cloudcore\cf1\highlight2 
\par \cf0\highlight0 building github.com/kubeedge/kubeedge/cloud/cmd/cloudcore\cf1\highlight2 
\par \cf0\highlight0 + go build -o /root/fornax/_output/local/bin/cloudcore -gcflags= -ldflags '-s -w -buildid= -X github.com/kubeedge/kubeedge/pkg/version.buildDate=2021-12-09T05:32:41Z -X github.com/kubeedge/kubeedge/pkg/version.gitCommit=b992ccac6715efb9df3e8a1a702246b4d24096d1 -X github.com/kubeedge/kubeedge/pkg/version.gitTreeState=clean -X github.com/kubeedge/kubeedge/pkg/version.gitVersion=v0.2-12+b992ccac6715ef -X github.com/kubeedge/kubeedge/pkg/version.gitMajor=0 -X github.com/kubeedge/kubeedge/pkg/version.gitMinor=2+' github.com/kubeedge/kubeedge/cloud/cmd/cloudcore\cf1\highlight2 
\par \cf0\highlight0 + set +x\cf1\highlight2 
\par \cf0\highlight0 yes | cp -r binaries/kubectl _output/local/bin/\cf1\highlight2 
\par \cf0\highlight0 mkdir -p _output/local/bin/crds\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/devices/devices_v1alpha2_device.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/devices/devices_v1alpha2_devicemodel.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/reliablesyncs/cluster_objectsync_v1alpha1.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/reliablesyncs/objectsync_v1alpha1.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/router/router_v1_rule.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/router/router_v1_ruleEndpoint.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/edgecluster/mission_v1.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/edgecluster/edgecluster_v1.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# make WHAT=edgecore\cf1\highlight2 
\par \cf0\highlight0 hack/verify-golang.sh\cf1\highlight2 
\par \cf0\highlight0 go detail version: go version go1.15.4 linux/amd64\cf1\highlight2 
\par \cf0\highlight0 go version: 1.15.4\cf1\highlight2 
\par \cf0\highlight0 KUBEEDGE_OUTPUT_SUBPATH=_output/local hack/make-rules/build.sh edgecore\cf1\highlight2 
\par \cf0\highlight0 building github.com/kubeedge/kubeedge/edge/cmd/edgecore\cf1\highlight2 
\par \cf0\highlight0 + go build -o /root/fornax/_output/local/bin/edgecore -gcflags= -ldflags '-s -w -buildid= -X github.com/kubeedge/kubeedge/pkg/version.buildDate=2021-12-09T05:38:06Z -X github.com/kubeedge/kubeedge/pkg/version.gitCommit=b992ccac6715efb9df3e8a1a702246b4d24096d1 -X github.com/kubeedge/kubeedge/pkg/version.gitTreeState=clean -X github.com/kubeedge/kubeedge/pkg/version.gitVersion=v0.2-12+b992ccac6715ef -X github.com/kubeedge/kubeedge/pkg/version.gitMajor=0 -X github.com/kubeedge/kubeedge/pkg/version.gitMinor=2+' github.com/kubeedge/kubeedge/edge/cmd/edgecore\cf1\highlight2 
\par \cf0\highlight0 + set +x\cf1\highlight2 
\par \cf0\highlight0 yes | cp -r binaries/kubectl _output/local/bin/\cf1\highlight2 
\par \cf0\highlight0 mkdir -p _output/local/bin/crds\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/devices/devices_v1alpha2_device.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/devices/devices_v1alpha2_devicemodel.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/reliablesyncs/cluster_objectsync_v1alpha1.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/reliablesyncs/objectsync_v1alpha1.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/router/router_v1_rule.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/router/router_v1_ruleEndpoint.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/edgecluster/mission_v1.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 yes | cp build/crds/edgecluster/edgecluster_v1.yaml  _output/local/bin/crds/\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# mkdir -p /etc/kubeedge/ca\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# mkdir -p /etc/kubeedge/certs\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# scp -r 192.168.1.210:/etc/kubeedge/ca /etc/kubeedge\cf1\highlight2 
\par \cf0\highlight0 The authenticity of host '192.168.1.210 (192.168.1.210)' can't be established.\cf1\highlight2 
\par \cf0\highlight0 ECDSA key fingerprint is SHA256:CwgX7h/zzN3K9IeiEZ+BGK49rwjYaKdrG6IXDe7N+Sk.\cf1\highlight2 
\par \cf0\highlight0 Are you sure you want to continue connecting (yes/no)? yes\cf1\highlight2 
\par \cf0\highlight0 Warning: Permanently added '192.168.1.210' (ECDSA) to the list of known hosts.\cf1\highlight2 
\par \cf0\highlight0 root@192.168.1.210's password:\cf1\highlight2 
\par \cf0\highlight0 rootCA.key                                                                                             100% 3311     6.0MB/s   00:00\cf1\highlight2 
\par \cf0\highlight0 rootCA.crt                                                                                             100% 2057     4.3MB/s   00:00\cf1\highlight2 
\par \cf0\highlight0 rootCA.srl                                                                                             100%   41   127.4KB/s   00:00\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# scp -r 192.168.1.210:/etc/kubeedge/certd /etc/kubeedge\cf1\highlight2 
\par \cf0\highlight0 root@192.168.1.210's password: root@node-b:~/fornax# scp -r 192.168.1.210:/etc/kubeedge/certs /etc/kubeedge\cf1\highlight2 
\par \cf0\highlight0 root@192.168.1.210's password:\cf1\highlight2 
\par \cf0\highlight0 server.crt                                                                                             100% 1602     2.2MB/s   00:00\cf1\highlight2 
\par \cf0\highlight0 server.csr                                                                                             100%  989     2.7MB/s   00:00\cf1\highlight2 
\par \cf0\highlight0 server.key                                                                                             100% 1675     4.4MB/s   00:00\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# cp /etc/kubernetes/admin.conf /root/.kube/config\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# mkdir -p /etc/kubeedge/config\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# _output/local/bin/cloudcore --minconfig > /etc/kubeedge/config/cloudcore.yaml\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# cp /etc/kubernetes/admin.conf /root/edgecluster.kubeconfig\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# _output/local/bin/edgecore --edgeclusterconfig > /etc/kubeedge/config/edgecore.yaml\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# scp 192.168.1.210:/etc/kubernetes/admin.conf /root\cf1\highlight2 
\par \cf0\highlight0 root@192.168.1.210's password:\cf1\highlight2 
\par \cf0\highlight0 admin.conf                                                                                             100% 5597     3.9MB/s   00:00\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# tests/edgecluster/hack/update_edgecore_config.sh /root/admin.conf\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# chmod a+x tests/edgecluster/hack/update_edgecore_config.sh\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# tests/edgecluster/hack/update_edgecore_config.sh admin.conf\cf1\highlight2 
\par \cf0\highlight0 kubeconfig file (admin.conf) not found!\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# scp 192.168.1.210:/etc/kubernetes/admin.conf /root/fornax\cf1\highlight2 
\par \cf0\highlight0 root@192.168.1.210's password:\cf1\highlight2 
\par \cf0\highlight0 admin.conf                                                                                             100% 5597     5.4MB/s   00:00\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# cd ..\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# rm -rf admin.conf\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~# cd fornax\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# ls\cf1\highlight2 
\par \cf0\highlight0 admin.conf   \cf8 CHANGELOG\cf0            \cf8 common\cf0            \cf8 edgemesh\cf0                 go.sum   \cf8 LICENSES\cf0         \cf8 mappers\cf0   README.md     \cf8 vendor\cf1\highlight2 
\par \cf0\highlight0 ADOPTERS.md  CHANGELOG.md        CONTRIBUTING.md  \cf8 edgesite\cf0                 \cf8 hack\cf0      MAINTAINERS     \cf8 _output\cf0   README_zh.md\cf1\highlight2 
\par \cf8\highlight0 binaries\cf0      \cf8 cloud\cf0                \cf8 docs\cf0              external-dependency.md  \cf8 keadm\cf0     MAINTAINERS.md  OWNERS   \cf8 staging\cf1\highlight2 
\par \cf8\highlight0 build\cf0         CODE_OF_CONDUCT.md  \cf8 edge\cf0              go.mod                  LICENSE  Makefile        \cf8 pkg\cf0       \cf8 tests\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# tests/edgecluster/hack/update_edgecore_config.sh admin.conf\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# chmod a+x tests/edgecluster/hack/update_edgecore_config.sh\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# kubectl apply -f build/crds/devices/devices_v1alpha2_device.yaml\cf1\highlight2 
\par \cf0\highlight0 kubectl apply -f build/crds/reliablesyncs/cluster_objectsync_v1alpha1.yaml\cf1\highlight2 
\par 
\par \cf0\highlight0 kubectl apply -f build/crds/reliablesyncs/objectsync_v1alpha1.yaml\cf1\highlight2 
\par 
\par \cf0\highlight0 kubectl apply -f build/crds/router/router_v1_rule.yaml\cf1\highlight2 
\par 
\par \cf0\highlight0 kubectl apply -f build/crds/router/router_v1_ruleEndpoint.yaml\cf1\highlight2 
\par 
\par \cf0\highlight0 kubectl apply -f build/crds/edgecluster/mission_v1.yaml\cf1\highlight2 
\par 
\par \cf0\highlight0 kubectl apply -f build/crds/edgecluster/edgecluster_v1.yaml\cf1\highlight2 
\par \cf4\highlight0 Warning:\cf0  apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition\cf1\highlight2 
\par \cf0\highlight0 customresourcedefinition.apiextensions.k8s.io/devices.devices.kubeedge.io created\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# kubectl apply -f build/crds/devices/devices_v1alpha2_devicemodel.yaml\cf1\highlight2 
\par \cf4\highlight0 Warning:\cf0  apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition\cf1\highlight2 
\par \cf0\highlight0 customresourcedefinition.apiextensions.k8s.io/devicemodels.devices.kubeedge.io created\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# kubectl apply -f build/crds/reliablesyncs/cluster_objectsync_v1alpha1.yaml\cf1\highlight2 
\par \cf4\highlight0 Warning:\cf0  apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition\cf1\highlight2 
\par \cf0\highlight0 customresourcedefinition.apiextensions.k8s.io/clusterobjectsyncs.reliablesyncs.kubeedge.io created\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# kubectl apply -f build/crds/reliablesyncs/objectsync_v1alpha1.yaml\cf1\highlight2 
\par \cf4\highlight0 Warning:\cf0  apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition\cf1\highlight2 
\par \cf0\highlight0 customresourcedefinition.apiextensions.k8s.io/objectsyncs.reliablesyncs.kubeedge.io created\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# kubectl apply -f build/crds/router/router_v1_rule.yaml\cf1\highlight2 
\par \cf0\highlight0 customresourcedefinition.apiextensions.k8s.io/rules.rules.kubeedge.io created\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# kubectl apply -f build/crds/router/router_v1_ruleEndpoint.yaml\cf1\highlight2 
\par \cf0\highlight0 customresourcedefinition.apiextensions.k8s.io/ruleendpoints.rules.kubeedge.io created\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# kubectl apply -f build/crds/edgecluster/mission_v1.yaml\cf1\highlight2 
\par \cf4\highlight0 Warning:\cf0  apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition\cf1\highlight2 
\par \cf0\highlight0 customresourcedefinition.apiextensions.k8s.io/missions.edgeclusters.kubeedge.io created\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# kubectl apply -f build/crds/edgecluster/edgecluster_v1.yaml\cf1\highlight2 
\par \cf4\highlight0 Warning:\cf0  apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition\cf1\highlight2 
\par \cf0\highlight0 customresourcedefinition.apiextensions.k8s.io/edgeclusters.edgeclusters.kubeedge.io created\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# chmod a+x _output/local/bin/kubectl/vanilla/kubectl\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax#\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# nohup _output/local/bin/edgecore --edgecluster > edgecore.logs 2>&1 &\cf1\highlight2 
\par \cf0\highlight0 [1] 22914\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# tail -f edgecore.logs\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:28.123374   22914 ws.go:46] dial wss://192.168.1.210:10000/e632aba927ea4ac2b575ec1603d56f10/node-b/events successfully\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:28.123525   22914 websocket.go:93] Websocket connect to cloud access successful\cf1\highlight2 
\par \cf0\highlight0 W1209 05:45:28.123575   22914 context_channel.go:335] Failed to get type channel, type:bus\cf1\highlight2 
\par \cf0\highlight0 W1209 05:45:28.123597   22914 context_channel.go:184] Get bad module type:bus when sendToGroup message, do nothing\cf1\highlight2 
\par \cf0\highlight0 W1209 05:45:28.123623   22914 context_channel.go:335] Failed to get type channel, type:twin\cf1\highlight2 
\par \cf0\highlight0 W1209 05:45:28.123634   22914 context_channel.go:184] Get bad module type:twin when sendToGroup message, do nothing\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:28.123712   22914 process.go:411] node connection event occur: cloud_connected\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:28.123765   22914 process.go:411] node connection event occur: cloud_connected\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:29.635410   22914 edgecluster_state_reporter.go:113] Attempting to register edgeCluster (node-b), default/edgeclusterstate/node-b\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:29.647112   22914 edgecluster_state_reporter.go:131] Successfully registered edgeCluster node-b\cf1\highlight2 
\par \cf0\highlight0 ^C\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# nohup _output/local/bin/cloudcore > cloudcore.logs 2>&1 &\cf1\highlight2 
\par \cf0\highlight0 [2] 23944\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# tail -f cloudcore.logs\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:46.496854   23944 upstream.go:123] start upstream controller\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:46.496872   23944 core.go:24] Starting module synccontroller\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:46.496913   23944 core.go:24] Starting module missionstatepruner\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:46.496982   23944 downstream.go:446] start downstream controller\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:46.496913   23944 downstream.go:873] Start downstream devicecontroller\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:46.618558   23944 signcerts.go:100] Succeed to creating token\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:46.618691   23944 server.go:44] start unix domain socket server\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:46.618967   23944 uds.go:71] listening on: //var/lib/kubeedge/kubeedge.sock\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:46.620288   23944 server.go:64] Starting cloudhub websocket server\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:48.498335   23944 upstream.go:63] Start upstream devicecontroller\cf1\highlight2 
\par \cf0\highlight0 ^C\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# tail -f cloudcore.logs\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:46.496913   23944 core.go:24] Starting module missionstatepruner\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:46.496982   23944 downstream.go:446] start downstream controller\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:46.496913   23944 downstream.go:873] Start downstream devicecontroller\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:46.618558   23944 signcerts.go:100] Succeed to creating token\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:46.618691   23944 server.go:44] start unix domain socket server\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:46.618967   23944 uds.go:71] listening on: //var/lib/kubeedge/kubeedge.sock\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:46.620288   23944 server.go:64] Starting cloudhub websocket server\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:48.498335   23944 upstream.go:63] Start upstream devicecontroller\cf1\highlight2 
\par \cf0\highlight0 I1209 05:49:53.722452   23944 messagehandler.go:293] edge node node-c for project e632aba927ea4ac2b575ec1603d56f10 connected\cf1\highlight2 
\par \cf0\highlight0 W1209 05:49:53.722581   23944 upstream.go:188] parse message: 6563f747-3d21-46b9-83b8-7209079d6dfd resource type with error: resource type not found,\cf1\highlight2 
\par \cf0\highlight0 ^C\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# tail -f edgecore.logs\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:28.123374   22914 ws.go:46] dial wss://192.168.1.210:10000/e632aba927ea4ac2b575ec1603d56f10/node-b/events successfully\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:28.123525   22914 websocket.go:93] Websocket connect to cloud access successful\cf1\highlight2 
\par \cf0\highlight0 W1209 05:45:28.123575   22914 context_channel.go:335] Failed to get type channel, type:bus\cf1\highlight2 
\par \cf0\highlight0 W1209 05:45:28.123597   22914 context_channel.go:184] Get bad module type:bus when sendToGroup message, do nothing\cf1\highlight2 
\par \cf0\highlight0 W1209 05:45:28.123623   22914 context_channel.go:335] Failed to get type channel, type:twin\cf1\highlight2 
\par \cf0\highlight0 W1209 05:45:28.123634   22914 context_channel.go:184] Get bad module type:twin when sendToGroup message, do nothing\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:28.123712   22914 process.go:411] node connection event occur: cloud_connected\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:28.123765   22914 process.go:411] node connection event occur: cloud_connected\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:29.635410   22914 edgecluster_state_reporter.go:113] Attempting to register edgeCluster (node-b), default/edgeclusterstate/node-b\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:29.647112   22914 edgecluster_state_reporter.go:131] Successfully registered edgeCluster node-b\cf1\highlight2 
\par \cf0\highlight0 ^C\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# tail -f edgecore.logs\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:28.123374   22914 ws.go:46] dial wss://192.168.1.210:10000/e632aba927ea4ac2b575ec1603d56f10/node-b/events successfully\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:28.123525   22914 websocket.go:93] Websocket connect to cloud access successful\cf1\highlight2 
\par \cf0\highlight0 W1209 05:45:28.123575   22914 context_channel.go:335] Failed to get type channel, type:bus\cf1\highlight2 
\par \cf0\highlight0 W1209 05:45:28.123597   22914 context_channel.go:184] Get bad module type:bus when sendToGroup message, do nothing\cf1\highlight2 
\par \cf0\highlight0 W1209 05:45:28.123623   22914 context_channel.go:335] Failed to get type channel, type:twin\cf1\highlight2 
\par \cf0\highlight0 W1209 05:45:28.123634   22914 context_channel.go:184] Get bad module type:twin when sendToGroup message, do nothing\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:28.123712   22914 process.go:411] node connection event occur: cloud_connected\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:28.123765   22914 process.go:411] node connection event occur: cloud_connected\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:29.635410   22914 edgecluster_state_reporter.go:113] Attempting to register edgeCluster (node-b), default/edgeclusterstate/node-b\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:29.647112   22914 edgecluster_state_reporter.go:131] Successfully registered edgeCluster node-b\cf1\highlight2 
\par \cf0\highlight0 ^C\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# tail -f cloudcore.logs\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:46.496913   23944 core.go:24] Starting module missionstatepruner\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:46.496982   23944 downstream.go:446] start downstream controller\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:46.496913   23944 downstream.go:873] Start downstream devicecontroller\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:46.618558   23944 signcerts.go:100] Succeed to creating token\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:46.618691   23944 server.go:44] start unix domain socket server\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:46.618967   23944 uds.go:71] listening on: //var/lib/kubeedge/kubeedge.sock\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:46.620288   23944 server.go:64] Starting cloudhub websocket server\cf1\highlight2 
\par \cf0\highlight0 I1209 05:45:48.498335   23944 upstream.go:63] Start upstream devicecontroller\cf1\highlight2 
\par \cf0\highlight0 I1209 05:49:53.722452   23944 messagehandler.go:293] edge node node-c for project e632aba927ea4ac2b575ec1603d56f10 connected\cf1\highlight2 
\par \cf0\highlight0 W1209 05:49:53.722581   23944 upstream.go:188] parse message: 6563f747-3d21-46b9-83b8-7209079d6dfd resource type with error: resource type not found,\cf1\highlight2 
\par \cf0\highlight0 ^C\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-pxlm6         1/1     Running   0          93m\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-zgght         1/1     Running   0          93m\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-b                      1/1     Running   0          93m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-b            1/1     Running   0          94m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-b   1/1     Running   0          93m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-flannel-ds-rxgkl            1/1     Running   0          92m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-qv42t                 1/1     Running   0          93m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-b            1/1     Running   0          93m\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-pxlm6         1/1     Running   0          94m\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-zgght         1/1     Running   0          94m\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-b                      1/1     Running   0          94m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-b            1/1     Running   0          94m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-b   1/1     Running   0          94m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-flannel-ds-rxgkl            1/1     Running   0          93m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-qv42t                 1/1     Running   0          94m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-b            1/1     Running   0          94m\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-pxlm6         1/1     Running   0          94m\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-zgght         1/1     Running   0          94m\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-b                      1/1     Running   0          94m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-b            1/1     Running   0          94m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-b   1/1     Running   0          94m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-flannel-ds-rxgkl            1/1     Running   0          93m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-qv42t                 1/1     Running   0          94m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-b            1/1     Running   0          94m\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-pxlm6         1/1     Running   0          94m\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-zgght         1/1     Running   0          94m\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-b                      1/1     Running   0          94m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-b            1/1     Running   0          94m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-b   1/1     Running   0          94m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-flannel-ds-rxgkl            1/1     Running   0          93m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-qv42t                 1/1     Running   0          94m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-b            1/1     Running   0          94m\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-pxlm6         1/1     Running   0          94m\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-zgght         1/1     Running   0          94m\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-b                      1/1     Running   0          94m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-b            1/1     Running   0          94m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-b   1/1     Running   0          94m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-flannel-ds-rxgkl            1/1     Running   0          93m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-qv42t                 1/1     Running   0          94m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-b            1/1     Running   0          94m\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                                   READY   STATUS    RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 face          frontend-56b6fd5f8c-c2n2t              0/1     Pending   0          20s\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-24lgz                 0/1     Pending   0          15s\cf1\highlight2 
\par \cf0\highlight0 face          nsqd-54667b87f4-gz4h2                  0/1     Pending   0          8s\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-8rvmp             0/1     Pending   0          25s\cf1\highlight2 
\par \cf0\highlight0 face          receiver-deployment-74b5c7d449-rfs6f   0/1     Pending   0          31s\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-pxlm6               1/1     Running   0          94m\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-zgght               1/1     Running   0          94m\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-b                            1/1     Running   0          95m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-b                  1/1     Running   0          95m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-b         1/1     Running   0          95m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-flannel-ds-rxgkl                  1/1     Running   0          93m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-qv42t                       1/1     Running   0          94m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-b                  1/1     Running   0          95m\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                                          READY   STATUS    RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 face          face-recog-698dc6b88f-87pnl                   0/1     Pending   0          67s\cf1\highlight2 
\par \cf0\highlight0 face          frontend-56b6fd5f8c-c2n2t                     0/1     Pending   0          104s\cf1\highlight2 
\par \cf0\highlight0 face          image-processor-deployment-7d6d54d996-mkxtl   0/1     Pending   0          69s\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-24lgz                        0/1     Pending   0          99s\cf1\highlight2 
\par \cf0\highlight0 face          nsqd-54667b87f4-gz4h2                         0/1     Pending   0          92s\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-8rvmp                    0/1     Pending   0          109s\cf1\highlight2 
\par \cf0\highlight0 face          receiver-deployment-74b5c7d449-rfs6f          0/1     Pending   0          115s\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-pxlm6                      1/1     Running   0          96m\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-zgght                      1/1     Running   0          96m\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-b                                   1/1     Running   0          96m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-b                         1/1     Running   0          96m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-b                1/1     Running   0          96m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-flannel-ds-rxgkl                         1/1     Running   0          95m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-qv42t                              1/1     Running   0          96m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-b                         1/1     Running   0          96m\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# kubectl describe pod face-recog-698dc6b88f-87pnl -n face\cf1\highlight2 
\par \cf0\highlight0 Name:           face-recog-698dc6b88f-87pnl\cf1\highlight2 
\par \cf0\highlight0 Namespace:      face\cf1\highlight2 
\par \cf0\highlight0 Priority:       0\cf1\highlight2 
\par \cf0\highlight0 Node:           <none>\cf1\highlight2 
\par \cf0\highlight0 Labels:         app=face-recog\cf1\highlight2 
\par \cf0\highlight0                 pod-template-hash=698dc6b88f\cf1\highlight2 
\par \cf0\highlight0 Annotations:    <none>\cf1\highlight2 
\par \cf0\highlight0 Status:         Pending\cf1\highlight2 
\par \cf0\highlight0 IP:\cf1\highlight2 
\par \cf0\highlight0 IPs:            <none>\cf1\highlight2 
\par \cf0\highlight0 Controlled By:  ReplicaSet/face-recog-698dc6b88f\cf1\highlight2 
\par \cf0\highlight0 Containers:\cf1\highlight2 
\par \cf0\highlight0   face-recog:\cf1\highlight2 
\par \cf0\highlight0     Image:      skarlso/kube-face-recog:1.0.0\cf1\highlight2 
\par \cf0\highlight0     Port:       50051/TCP\cf1\highlight2 
\par \cf0\highlight0     Host Port:  50051/TCP\cf1\highlight2 
\par \cf0\highlight0     Environment:\cf1\highlight2 
\par \cf0\highlight0       KNOWN_PEOPLE:  /known_people\cf1\highlight2 
\par \cf0\highlight0     Mounts:\cf1\highlight2 
\par \cf0\highlight0       /known_people from known-people-storage (rw)\cf1\highlight2 
\par \cf0\highlight0       /unknown_people from unknown-people-storage (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-8chh4 (ro)\cf1\highlight2 
\par \cf0\highlight0 Conditions:\cf1\highlight2 
\par \cf0\highlight0   Type           Status\cf1\highlight2 
\par \cf0\highlight0   PodScheduled   False\cf1\highlight2 
\par \cf0\highlight0 Volumes:\cf1\highlight2 
\par \cf0\highlight0   known-people-storage:\cf1\highlight2 
\par \cf0\highlight0     Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\cf1\highlight2 
\par \cf0\highlight0     ClaimName:  face-recognition-pvc-known\cf1\highlight2 
\par \cf0\highlight0     ReadOnly:   false\cf1\highlight2 
\par \cf0\highlight0   unknown-people-storage:\cf1\highlight2 
\par \cf0\highlight0     Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\cf1\highlight2 
\par \cf0\highlight0     ClaimName:  face-recognition-pvc-unknown\cf1\highlight2 
\par \cf0\highlight0     ReadOnly:   false\cf1\highlight2 
\par \cf0\highlight0   kube-api-access-8chh4:\cf1\highlight2 
\par \cf0\highlight0     Type:                    Projected (a volume that contains injected data from multiple sources)\cf1\highlight2 
\par \cf0\highlight0     TokenExpirationSeconds:  3607\cf1\highlight2 
\par \cf0\highlight0     ConfigMapName:           kube-root-ca.crt\cf1\highlight2 
\par \cf0\highlight0     ConfigMapOptional:       <nil>\cf1\highlight2 
\par \cf0\highlight0     DownwardAPI:             true\cf1\highlight2 
\par \cf0\highlight0 QoS Class:                   BestEffort\cf1\highlight2 
\par \cf0\highlight0 Node-Selectors:              <none>\cf1\highlight2 
\par \cf0\highlight0 Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\cf1\highlight2 
\par \cf0\highlight0                              node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\cf1\highlight2 
\par \cf0\highlight0 Events:\cf1\highlight2 
\par \cf0\highlight0   Type     Reason            Age                From               Message\cf1\highlight2 
\par \cf0\highlight0   ----     ------            ----               ----               -------\cf1\highlight2 
\par \cf0\highlight0   Warning  FailedScheduling  22s (x3 over 86s)  default-scheduler  0/1 nodes are available: 1 node(s) had taint \{node-role.kubernetes.io/master: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# kubectl taint nodes node-b node-role.kubernetes.io/master-\cf1\highlight2 
\par \cf0\highlight0 node/node-b untainted\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                                          READY   STATUS              RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 face          face-recog-698dc6b88f-87pnl                   0/1     ContainerCreating   0          2m19s\cf1\highlight2 
\par \cf0\highlight0 face          frontend-56b6fd5f8c-c2n2t                     1/1     Running             0          2m56s\cf1\highlight2 
\par \cf0\highlight0 face          image-processor-deployment-7d6d54d996-mkxtl   0/1     ContainerCreating   0          2m21s\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-24lgz                        0/1     ContainerCreating   0          2m51s\cf1\highlight2 
\par \cf0\highlight0 face          nsqd-54667b87f4-gz4h2                         0/1     ContainerCreating   0          2m44s\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-8rvmp                    0/1     ContainerCreating   0          3m1s\cf1\highlight2 
\par \cf0\highlight0 face          receiver-deployment-74b5c7d449-rfs6f          0/1     ContainerCreating   0          3m7s\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-pxlm6                      1/1     Running             0          97m\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-zgght                      1/1     Running             0          97m\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-b                                   1/1     Running             0          97m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-b                         1/1     Running             0          97m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-b                1/1     Running             0          97m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-flannel-ds-rxgkl                         1/1     Running             0          96m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-qv42t                              1/1     Running             0          97m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-b                         1/1     Running             0          97m\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                                          READY   STATUS              RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 face          face-recog-698dc6b88f-87pnl                   0/1     ContainerCreating   0          3m37s\cf1\highlight2 
\par \cf0\highlight0 face          frontend-56b6fd5f8c-c2n2t                     1/1     Running             0          4m14s\cf1\highlight2 
\par \cf0\highlight0 face          image-processor-deployment-7d6d54d996-mkxtl   1/1     Running             0          3m39s\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-24lgz                        0/1     ContainerCreating   0          4m9s\cf1\highlight2 
\par \cf0\highlight0 face          nsqd-54667b87f4-gz4h2                         1/1     Running             0          4m2s\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-8rvmp                    1/1     Running             0          4m19s\cf1\highlight2 
\par \cf0\highlight0 face          receiver-deployment-74b5c7d449-rfs6f          0/1     ContainerCreating   0          4m25s\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-pxlm6                      1/1     Running             0          98m\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-zgght                      1/1     Running             0          98m\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-b                                   1/1     Running             0          98m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-b                         1/1     Running             0          99m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-b                1/1     Running             0          98m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-flannel-ds-rxgkl                         1/1     Running             0          97m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-qv42t                              1/1     Running             0          98m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-b                         1/1     Running             0          98m\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                                          READY   STATUS              RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 face          face-recog-698dc6b88f-87pnl                   0/1     ContainerCreating   0          4m15s\cf1\highlight2 
\par \cf0\highlight0 face          frontend-56b6fd5f8c-c2n2t                     1/1     Running             0          4m52s\cf1\highlight2 
\par \cf0\highlight0 face          image-processor-deployment-7d6d54d996-mkxtl   1/1     Running             0          4m17s\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-24lgz                        0/1     ContainerCreating   0          4m47s\cf1\highlight2 
\par \cf0\highlight0 face          nsqd-54667b87f4-gz4h2                         1/1     Running             0          4m40s\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-8rvmp                    1/1     Running             0          4m57s\cf1\highlight2 
\par \cf0\highlight0 face          receiver-deployment-74b5c7d449-rfs6f          0/1     ContainerCreating   0          5m3s\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-pxlm6                      1/1     Running             0          99m\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-zgght                      1/1     Running             0          99m\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-b                                   1/1     Running             0          99m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-b                         1/1     Running             0          99m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-b                1/1     Running             0          99m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-flannel-ds-rxgkl                         1/1     Running             0          98m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-qv42t                              1/1     Running             0          99m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-b                         1/1     Running             0          99m\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# kubectl get pvc\cf1\highlight2 
\par \cf0\highlight0 No resources found in default namespace.\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# kubectl get pvc -n face\cf1\highlight2 
\par \cf0\highlight0 NAME                           STATUS   VOLUME                        CAPACITY   ACCESS MODES   STORAGECLASS    AGE\cf1\highlight2 
\par \cf0\highlight0 face-recognition-pvc-known     Bound    face-recognition-pv-known     5Gi        RWO            local-storage   5m8s\cf1\highlight2 
\par \cf0\highlight0 face-recognition-pvc-unknown   Bound    face-recognition-pv-unknown   5Gi        RWO            local-storage   5m26s\cf1\highlight2 
\par \cf0\highlight0 mysql-pv-claim                 Bound    mysql-pv                      5Gi        RWO            local-storage   5m40s\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                                          READY   STATUS              RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 face          face-recog-698dc6b88f-87pnl                   0/1     CrashLoopBackOff    4          8m13s\cf1\highlight2 
\par \cf0\highlight0 face          frontend-56b6fd5f8c-c2n2t                     1/1     Running             0          8m50s\cf1\highlight2 
\par \cf0\highlight0 face          image-processor-deployment-7d6d54d996-mkxtl   1/1     Running             0          8m15s\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-24lgz                        0/1     ContainerCreating   0          8m45s\cf1\highlight2 
\par \cf0\highlight0 face          nsqd-54667b87f4-gz4h2                         1/1     Running             0          8m38s\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-8rvmp                    1/1     Running             0          8m55s\cf1\highlight2 
\par \cf0\highlight0 face          receiver-deployment-74b5c7d449-rfs6f          1/1     Running             0          9m1s\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-pxlm6                      1/1     Running             0          103m\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-zgght                      1/1     Running             0          103m\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-b                                   1/1     Running             0          103m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-b                         1/1     Running             0          103m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-b                1/1     Running             0          103m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-flannel-ds-rxgkl                         1/1     Running             0          102m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-qv42t                              1/1     Running             0          103m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-b                         1/1     Running             0          103m\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                                          READY   STATUS              RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 face          face-recog-698dc6b88f-87pnl                   0/1     CrashLoopBackOff    8          24m\cf1\highlight2 
\par \cf0\highlight0 face          frontend-56b6fd5f8c-c2n2t                     1/1     Running             0          24m\cf1\highlight2 
\par \cf0\highlight0 face          image-processor-deployment-7d6d54d996-mkxtl   1/1     Running             0          24m\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-24lgz                        0/1     ContainerCreating   0          24m\cf1\highlight2 
\par \cf0\highlight0 face          nsqd-54667b87f4-gz4h2                         1/1     Running             0          24m\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-8rvmp                    1/1     Running             0          24m\cf1\highlight2 
\par \cf0\highlight0 face          receiver-deployment-74b5c7d449-rfs6f          1/1     Running             0          24m\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-pxlm6                      1/1     Running             0          119m\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-zgght                      1/1     Running             0          119m\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-b                                   1/1     Running             0          119m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-b                         1/1     Running             0          119m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-b                1/1     Running             0          119m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-flannel-ds-rxgkl                         1/1     Running             0          118m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-qv42t                              1/1     Running             0          119m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-b                         1/1     Running             0          119m\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                                          READY   STATUS              RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 face          face-recog-698dc6b88f-87pnl                   0/1     CrashLoopBackOff    24         107m\cf1\highlight2 
\par \cf0\highlight0 face          frontend-56b6fd5f8c-c2n2t                     1/1     Running             0          108m\cf1\highlight2 
\par \cf0\highlight0 face          image-processor-deployment-7d6d54d996-mkxtl   1/1     Running             0          107m\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-24lgz                        0/1     ContainerCreating   0          108m\cf1\highlight2 
\par \cf0\highlight0 face          nsqd-54667b87f4-gz4h2                         1/1     Running             0          107m\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-8rvmp                    1/1     Running             0          108m\cf1\highlight2 
\par \cf0\highlight0 face          receiver-deployment-74b5c7d449-rfs6f          1/1     Running             0          108m\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-pxlm6                      1/1     Running             0          3h22m\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-zgght                      1/1     Running             0          3h22m\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-b                                   1/1     Running             0          3h22m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-b                         1/1     Running             0          3h23m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-b                1/1     Running             0          3h22m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-flannel-ds-rxgkl                         1/1     Running             0          3h21m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-qv42t                              1/1     Running             0          3h22m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-b                         1/1     Running             0          3h22m\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                                          READY   STATUS              RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 face          face-recog-698dc6b88f-87pnl                   0/1     CrashLoopBackOff    24         108m\cf1\highlight2 
\par \cf0\highlight0 face          frontend-56b6fd5f8c-c2n2t                     1/1     Running             0          109m\cf1\highlight2 
\par \cf0\highlight0 face          image-processor-deployment-7d6d54d996-mkxtl   1/1     Running             0          108m\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-24lgz                        0/1     ContainerCreating   0          109m\cf1\highlight2 
\par \cf0\highlight0 face          nsqd-54667b87f4-gz4h2                         1/1     Running             0          108m\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-8rvmp                    1/1     Running             0          109m\cf1\highlight2 
\par \cf0\highlight0 face          receiver-deployment-74b5c7d449-rfs6f          1/1     Running             0          109m\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-pxlm6                      1/1     Running             0          3h23m\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-zgght                      1/1     Running             0          3h23m\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-b                                   1/1     Running             0          3h23m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-b                         1/1     Running             0          3h24m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-b                1/1     Running             0          3h23m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-flannel-ds-rxgkl                         1/1     Running             0          3h22m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-qv42t                              1/1     Running             0          3h23m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-b                         1/1     Running             0          3h23m\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# tail -f edgecore.logs\cf1\highlight2 
\par \cf0\highlight0 I1209 08:44:54.442811   22914 mission_deployer.go:168] Mission command-create-configmap deleted successfully\cf1\highlight2 
\par \cf0\highlight0 E1209 08:45:04.910696   22914 mission_deployer.go:156] Failed to revert the content of mission command-create-ns-face: Command (/root/fornax/_output/local/bin/kubectl/vanilla/kubectl delete ns face --kubeconfig /root/edgecluster.kubeconfig) failed: exitcode: -1, output (namespace "face" deleted\cf1\highlight2 
\par \cf0\highlight0 ), error: signal: killed\cf1\highlight2 
\par \cf0\highlight0 I1209 08:45:05.289257   22914 mission_deployer.go:168] Mission command-create-ns-face deleted successfully\cf1\highlight2 
\par \cf0\highlight0 I1209 08:45:05.918490   22914 mission_deployer.go:168] Mission command-frontend-port-forward deleted successfully\cf1\highlight2 
\par \cf0\highlight0 E1209 08:45:06.993332   22914 mission_deployer.go:156] Failed to revert the content of mission command-label-node: Command (/root/fornax/_output/local/bin/kubectl/vanilla/kubectl label nodes  $(/root/fornax/_output/local/bin/kubectl/vanilla/kubectl get nodes -o json --kubeconfig /root/edgecluster.kubeconfig | jq -r .items[0].metadata.name)  local-pvc=true- --kubeconfig /root/edgecluster.kubeconfig) failed: exitcode: 1, output (error: invalid label value: "local-pvc=true-": a valid label must be an empty string or consist of alphanumeric characters, '-', '_' or '.', and must start and end with an alphanumeric character (e.g. 'MyValue',  or 'my_value',  or '12345', regex used for validation is '(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])?')\cf1\highlight2 
\par \cf0\highlight0 ), error: exit status 1\cf1\highlight2 
\par \cf0\highlight0 I1209 08:45:07.337472   22914 mission_deployer.go:168] Mission command-label-node deleted successfully\cf1\highlight2 
\par \cf0\highlight0 I1209 08:45:08.019180   22914 mission_deployer.go:168] Mission command-mysql-port-forward deleted successfully\cf1\highlight2 
\par \cf0\highlight0 I1209 08:45:08.682343   22914 mission_deployer.go:168] Mission command-receiver-port-forward deleted successfully\cf1\highlight2 
\par \cf0\highlight0 E1209 08:45:09.452389   22914 mission_deployer.go:156] Failed to revert the content of mission resource-face-recog-deployment: Command (printf 'apiVersion: apps/v1\cf1\highlight2 
\par \cf0\highlight0 kind: Deployment\cf1\highlight2 
\par \cf0\highlight0 metadata:\cf1\highlight2 
\par \cf0\highlight0   name: face-recog\cf1\highlight2 
\par \cf0\highlight0   namespace: face\cf1\highlight2 
\par \cf0\highlight0 spec:\cf1\highlight2 
\par \cf0\highlight0   selector:\cf1\highlight2 
\par \cf0\highlight0     matchLabels:\cf1\highlight2 
\par \cf0\highlight0       app: face-recog\cf1\highlight2 
\par \cf0\highlight0   replicas: 1\cf1\highlight2 
\par \cf0\highlight0   template:\cf1\highlight2 
\par \cf0\highlight0     metadata:\cf1\highlight2 
\par \cf0\highlight0       labels:\cf1\highlight2 
\par \cf0\highlight0         app: face-recog\cf1\highlight2 
\par \cf0\highlight0     spec:\cf1\highlight2 
\par \cf0\highlight0       containers:\cf1\highlight2 
\par \cf0\highlight0       - name: face-recog\cf1\highlight2 
\par \cf0\highlight0         image: skarlso/kube-face-recog:1.0.0\cf1\highlight2 
\par \cf0\highlight0         imagePullPolicy: Always\cf1\highlight2 
\par \cf0\highlight0         ports:\cf1\highlight2 
\par \cf0\highlight0         - containerPort: 50051\cf1\highlight2 
\par \cf0\highlight0           hostPort: 50051\cf1\highlight2 
\par \cf0\highlight0           name: face-recog\cf1\highlight2 
\par \cf0\highlight0         env:\cf1\highlight2 
\par \cf0\highlight0         - name: KNOWN_PEOPLE\cf1\highlight2 
\par \cf0\highlight0           value: "/known_people"\cf1\highlight2 
\par \cf0\highlight0         volumeMounts:\cf1\highlight2 
\par \cf0\highlight0         - name: known-people-storage\cf1\highlight2 
\par \cf0\highlight0           mountPath: /known_people\cf1\highlight2 
\par \cf0\highlight0         - name: unknown-people-storage\cf1\highlight2 
\par \cf0\highlight0           mountPath: /unknown_people\cf1\highlight2 
\par \cf0\highlight0       volumes:\cf1\highlight2 
\par \cf0\highlight0       - name: known-people-storage\cf1\highlight2 
\par \cf0\highlight0         persistentVolumeClaim:\cf1\highlight2 
\par \cf0\highlight0           claimName: face-recognition-pvc-known\cf1\highlight2 
\par \cf0\highlight0       - name: unknown-people-storage\cf1\highlight2 
\par \cf0\highlight0         persistentVolumeClaim:\cf1\highlight2 
\par \cf0\highlight0           claimName: face-recognition-pvc-unknown\cf1\highlight2 
\par \cf0\highlight0 ' | /root/fornax/_output/local/bin/kubectl/vanilla/kubectl delete --kubeconfig=/root/edgecluster.kubeconfig -f - ) failed: exitcode: 1, output (Error from server (NotFound): error when deleting "STDIN": deployments.apps "face-recog" not found\cf1\highlight2 
\par \cf0\highlight0 ), error: exit status 1\cf1\highlight2 
\par \cf0\highlight0 I1209 08:45:09.815480   22914 mission_deployer.go:168] Mission resource-face-recog-deployment deleted successfully\cf1\highlight2 
\par \cf0\highlight0 E1209 08:45:10.538471   22914 mission_deployer.go:156] Failed to revert the content of mission resource-face-recog-svc: Command (printf 'apiVersion: v1\cf1\highlight2 
\par \cf0\highlight0 kind: Service\cf1\highlight2 
\par \cf0\highlight0 metadata:\cf1\highlight2 
\par \cf0\highlight0   name: face-recog\cf1\highlight2 
\par \cf0\highlight0   namespace: face\cf1\highlight2 
\par \cf0\highlight0 spec:\cf1\highlight2 
\par \cf0\highlight0   ports:\cf1\highlight2 
\par \cf0\highlight0   - protocol: TCP\cf1\highlight2 
\par \cf0\highlight0     port: 50051\cf1\highlight2 
\par \cf0\highlight0     targetPort: 50051\cf1\highlight2 
\par \cf0\highlight0   selector:\cf1\highlight2 
\par \cf0\highlight0     app: face-recog\cf1\highlight2 
\par \cf0\highlight0   clusterIP: None\cf1\highlight2 
\par \cf0\highlight0 ' | /root/fornax/_output/local/bin/kubectl/vanilla/kubectl delete --kubeconfig=/root/edgecluster.kubeconfig -f - ) failed: exitcode: 1, output (Error from server (NotFound): error when deleting "STDIN": services "face-recog" not found\cf1\highlight2 
\par \cf0\highlight0 ), error: exit status 1\cf1\highlight2 
\par \cf0\highlight0 I1209 08:45:10.984719   22914 mission_deployer.go:168] Mission resource-face-recog-svc deleted successfully\cf1\highlight2 
\par \cf0\highlight0 E1209 08:45:11.650534   22914 mission_deployer.go:156] Failed to revert the content of mission resource-frontend-deployment: Command (printf 'apiVersion: apps/v1\cf1\highlight2 
\par \cf0\highlight0 kind: Deployment\cf1\highlight2 
\par \cf0\highlight0 metadata:\cf1\highlight2 
\par \cf0\highlight0   name: frontend\cf1\highlight2 
\par \cf0\highlight0   namespace: face\cf1\highlight2 
\par \cf0\highlight0 spec:\cf1\highlight2 
\par \cf0\highlight0   selector:\cf1\highlight2 
\par \cf0\highlight0     matchLabels:\cf1\highlight2 
\par \cf0\highlight0       app: frontend\cf1\highlight2 
\par \cf0\highlight0   replicas: 1\cf1\highlight2 
\par \cf0\highlight0   template:\cf1\highlight2 
\par \cf0\highlight0     metadata:\cf1\highlight2 
\par \cf0\highlight0       labels:\cf1\highlight2 
\par \cf0\highlight0         app: frontend\cf1\highlight2 
\par \cf0\highlight0     spec:\cf1\highlight2 
\par \cf0\highlight0       containers:\cf1\highlight2 
\par \cf0\highlight0       - name: frontend\cf1\highlight2 
\par \cf0\highlight0         image: skarlso/kube-frontend-alpine:1.1.0\cf1\highlight2 
\par \cf0\highlight0         imagePullPolicy: Always\cf1\highlight2 
\par \cf0\highlight0         env:\cf1\highlight2 
\par \cf0\highlight0         - name: MYSQL_CONNECTION\cf1\highlight2 
\par \cf0\highlight0           value: "mysql.face.svc.cluster.local"\cf1\highlight2 
\par \cf0\highlight0         - name: MYSQL_USERPASSWORD\cf1\highlight2 
\par \cf0\highlight0           valueFrom:\cf1\highlight2 
\par \cf0\highlight0             secretKeyRef:\cf1\highlight2 
\par \cf0\highlight0               name: kube-face-secret\cf1\highlight2 
\par \cf0\highlight0               key: mysql_userpassword\cf1\highlight2 
\par \cf0\highlight0         - name: MYSQL_PORT\cf1\highlight2 
\par \cf0\highlight0           value: "3306"\cf1\highlight2 
\par \cf0\highlight0         - name: MYSQL_DBNAME\cf1\highlight2 
\par \cf0\highlight0           value: kube\cf1\highlight2 
\par \cf0\highlight0         - name: FRONTEND_PORT\cf1\highlight2 
\par \cf0\highlight0           value: "8081"\cf1\highlight2 
\par \cf0\highlight0         ports:\cf1\highlight2 
\par \cf0\highlight0         - containerPort: 8081\cf1\highlight2 
\par \cf0\highlight0           hostPort: 8081\cf1\highlight2 
\par \cf0\highlight0 ' | /root/fornax/_output/local/bin/kubectl/vanilla/kubectl delete --kubeconfig=/root/edgecluster.kubeconfig -f - ) failed: exitcode: 1, output (Error from server (NotFound): error when deleting "STDIN": deployments.apps "frontend" not found\cf1\highlight2 
\par \cf0\highlight0 ), error: exit status 1\cf1\highlight2 
\par \cf0\highlight0 I1209 08:45:12.156945   22914 mission_deployer.go:168] Mission resource-frontend-deployment deleted successfully\cf1\highlight2 
\par \cf0\highlight0 E1209 08:45:12.889026   22914 mission_deployer.go:156] Failed to revert the content of mission resource-frontend-svc: Command (printf 'apiVersion: v1\cf1\highlight2 
\par \cf0\highlight0 kind: Service\cf1\highlight2 
\par \cf0\highlight0 metadata:\cf1\highlight2 
\par \cf0\highlight0   name: frontend\cf1\highlight2 
\par \cf0\highlight0   namespace: face\cf1\highlight2 
\par \cf0\highlight0 spec:\cf1\highlight2 
\par \cf0\highlight0   ports:\cf1\highlight2 
\par \cf0\highlight0   - port: 8081\cf1\highlight2 
\par \cf0\highlight0   selector:\cf1\highlight2 
\par \cf0\highlight0     app: frontend\cf1\highlight2 
\par \cf0\highlight0   type: NodePort\cf1\highlight2 
\par \cf0\highlight0 ' | /root/fornax/_output/local/bin/kubectl/vanilla/kubectl delete --kubeconfig=/root/edgecluster.kubeconfig -f - ) failed: exitcode: 1, output (Error from server (NotFound): error when deleting "STDIN": services "frontend" not found\cf1\highlight2 
\par \cf0\highlight0 ), error: exit status 1\cf1\highlight2 
\par \cf0\highlight0 I1209 08:45:13.189625   22914 mission_deployer.go:168] Mission resource-frontend-svc deleted successfully\cf1\highlight2 
\par \cf0\highlight0 E1209 08:45:13.820501   22914 mission_deployer.go:156] Failed to revert the content of mission resource-image-processor-deployment: Command (printf 'apiVersion: apps/v1\cf1\highlight2 
\par \cf0\highlight0 kind: Deployment\cf1\highlight2 
\par \cf0\highlight0 metadata:\cf1\highlight2 
\par \cf0\highlight0   name: image-processor-deployment\cf1\highlight2 
\par \cf0\highlight0   namespace: face\cf1\highlight2 
\par \cf0\highlight0 spec:\cf1\highlight2 
\par \cf0\highlight0   selector:\cf1\highlight2 
\par \cf0\highlight0     matchLabels:\cf1\highlight2 
\par \cf0\highlight0       app: image-processor\cf1\highlight2 
\par \cf0\highlight0   replicas: 1\cf1\highlight2 
\par \cf0\highlight0   template:\cf1\highlight2 
\par \cf0\highlight0     metadata:\cf1\highlight2 
\par \cf0\highlight0       labels:\cf1\highlight2 
\par \cf0\highlight0         app: image-processor\cf1\highlight2 
\par \cf0\highlight0     spec:\cf1\highlight2 
\par \cf0\highlight0       containers:\cf1\highlight2 
\par \cf0\highlight0       - name: image-processor\cf1\highlight2 
\par \cf0\highlight0         image: skarlso/kube-processor-alpine:1.0.13\cf1\highlight2 
\par \cf0\highlight0         imagePullPolicy: IfNotPresent\cf1\highlight2 
\par \cf0\highlight0         args:\cf1\highlight2 
\par \cf0\highlight0           - --db-host=mysql.face.svc.cluster.local\cf1\highlight2 
\par \cf0\highlight0           - --db-username-password=$(MYSQL_USERPASSWORD)\cf1\highlight2 
\par \cf0\highlight0           - --db-port=3306\cf1\highlight2 
\par \cf0\highlight0           - --db-dbname=kube\cf1\highlight2 
\par \cf0\highlight0           - --nsq-lookup-address=nsqlookup.face.svc.cluster.local:4161\cf1\highlight2 
\par \cf0\highlight0           - --grpc-address=face-recog.face.svc.cluster.local:50051\cf1\highlight2 
\par \cf0\highlight0         env:\cf1\highlight2 
\par \cf0\highlight0         - name: MYSQL_USERPASSWORD\cf1\highlight2 
\par \cf0\highlight0           valueFrom:\cf1\highlight2 
\par \cf0\highlight0             secretKeyRef:\cf1\highlight2 
\par \cf0\highlight0               name: kube-face-secret\cf1\highlight2 
\par \cf0\highlight0               key: mysql_userpassword\cf1\highlight2 
\par \cf0\highlight0 ' | /root/fornax/_output/local/bin/kubectl/vanilla/kubectl delete --kubeconfig=/root/edgecluster.kubeconfig -f - ) failed: exitcode: 1, output (Error from server (NotFound): error when deleting "STDIN": deployments.apps "image-processor-deployment" not found\cf1\highlight2 
\par \cf0\highlight0 ), error: exit status 1\cf1\highlight2 
\par \cf0\highlight0 I1209 08:45:14.221463   22914 mission_deployer.go:168] Mission resource-image-processor-deployment deleted successfully\cf1\highlight2 
\par \cf0\highlight0 I1209 08:45:15.044687   22914 mission_deployer.go:158] The content of mission resource-known-pv is reverted.\cf1\highlight2 
\par \cf0\highlight0 I1209 08:45:15.433605   22914 mission_deployer.go:168] Mission resource-known-pv deleted successfully\cf1\highlight2 
\par \cf0\highlight0 ^C\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-pxlm6         1/1     Running   0          3h24m\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-zgght         1/1     Running   0          3h24m\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-b                      1/1     Running   0          3h24m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-b            1/1     Running   0          3h24m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-b   1/1     Running   0          3h24m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-flannel-ds-rxgkl            1/1     Running   0          3h23m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-qv42t                 1/1     Running   0          3h24m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-b            1/1     Running   0          3h24m\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# tail -f edgecore.logs\cf1\highlight2 
\par \cf0\highlight0 I1209 09:44:13.276351   22914 mission_deployer.go:125] Mission resource-mysql-pv is created\cf1\highlight2 
\par \cf0\highlight0 I1209 09:44:14.399674   22914 mission_deployer.go:125] Mission command-create-ns-face is created\cf1\highlight2 
\par \cf0\highlight0 I1209 09:44:15.597159   22914 mission_deployer.go:125] Mission resource-mysql-pvc is created\cf1\highlight2 
\par \cf0\highlight0 I1209 09:44:16.839175   22914 mission_deployer.go:125] Mission command-cp-unknown is created\cf1\highlight2 
\par \cf0\highlight0 I1209 09:44:20.428198   22914 mission_deployer.go:125] Mission resource-receiver-deployment is created\cf1\highlight2 
\par \cf0\highlight0 I1209 09:44:21.939643   22914 mission_deployer.go:125] Mission command-frontend-port-forward is created\cf1\highlight2 
\par \cf0\highlight0 I1209 09:44:22.833309   22914 mission_deployer.go:125] Mission command-label-node is created\cf1\highlight2 
\par \cf0\highlight0 I1209 09:44:24.091238   22914 mission_deployer.go:125] Mission resource-mysql-service is created\cf1\highlight2 
\par \cf0\highlight0 I1209 09:44:25.679071   22914 mission_deployer.go:125] Mission resource-nsqlookup-deployment is created\cf1\highlight2 
\par \cf0\highlight0 I1209 09:44:28.128589   22914 mission_deployer.go:125] Mission resource-nsqlookup-service is created\cf1\highlight2 
\par \cf0\highlight0 I1209 09:44:29.901129   22914 mission_deployer.go:125] Mission resource-image-processor-deployment is created\cf1\highlight2 
\par \cf0\highlight0 I1209 09:44:31.723692   22914 mission_deployer.go:125] Mission resource-face-recog-deployment is created\cf1\highlight2 
\par \cf0\highlight0 I1209 09:44:33.807979   22914 mission_deployer.go:125] Mission command-receiver-port-forward is created\cf1\highlight2 
\par \cf0\highlight0 I1209 09:44:35.106806   22914 mission_deployer.go:125] Mission resource-face-recog-svc is created\cf1\highlight2 
\par \cf0\highlight0 I1209 09:44:36.575754   22914 mission_deployer.go:125] Mission resource-frontend-deployment is created\cf1\highlight2 
\par \cf0\highlight0 ^C\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                                          READY   STATUS                       RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 face          face-recog-698dc6b88f-6tgdm                   0/1     Pending                      0          9s\cf1\highlight2 
\par \cf0\highlight0 face          frontend-56b6fd5f8c-n2nj9                     0/1     ContainerCreating            0          4s\cf1\highlight2 
\par \cf0\highlight0 face          image-processor-deployment-7d6d54d996-v2ff4   0/1     CreateContainerConfigError   0          11s\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-6g6qp                    1/1     Running                      0          15s\cf1\highlight2 
\par \cf0\highlight0 face          receiver-deployment-74b5c7d449-7n7lf          0/1     CreateContainerConfigError   0          20s\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-pxlm6                      1/1     Running                      0          4h23m\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-zgght                      1/1     Running                      0          4h23m\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-b                                   1/1     Running                      0          4h23m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-b                         1/1     Running                      0          4h23m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-b                1/1     Running                      0          4h23m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-flannel-ds-rxgkl                         1/1     Running                      0          4h22m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-qv42t                              1/1     Running                      0          4h23m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-b                         1/1     Running                      0          4h23m\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax# kubectl get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                                          READY   STATUS              RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 face          face-recog-698dc6b88f-6tgdm                   0/1     Pending             0          60s\cf1\highlight2 
\par \cf0\highlight0 face          frontend-56b6fd5f8c-n2nj9                     1/1     Running             0          55s\cf1\highlight2 
\par \cf0\highlight0 face          image-processor-deployment-7d6d54d996-v2ff4   1/1     Running             0          62s\cf1\highlight2 
\par \cf0\highlight0 face          mysql-67ff5f6bf4-mkkjf                        0/1     ContainerCreating   0          49s\cf1\highlight2 
\par \cf0\highlight0 face          nsqd-54667b87f4-r79dx                         0/1     ContainerCreating   0          6s\cf1\highlight2 
\par \cf0\highlight0 face          nsqlookup-56768d5bd8-6g6qp                    1/1     Running             0          66s\cf1\highlight2 
\par \cf0\highlight0 face          receiver-deployment-74b5c7d449-7n7lf          1/1     Running             0          71s\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-pxlm6                      1/1     Running             0          4h24m\cf1\highlight2 
\par \cf0\highlight0 kube-system   coredns-558bd4d5db-zgght                      1/1     Running             0          4h24m\cf1\highlight2 
\par \cf0\highlight0 kube-system   etcd-node-b                                   1/1     Running             0          4h24m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-apiserver-node-b                         1/1     Running             0          4h24m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-controller-manager-node-b                1/1     Running             0          4h24m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-flannel-ds-rxgkl                         1/1     Running             0          4h23m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-proxy-qv42t                              1/1     Running             0          4h24m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-scheduler-node-b                         1/1     Running             0          4h24m\cf1\highlight2 
\par \cf0\highlight0 root@node-b:~/fornax#\cf1\highlight2 
\par \pard\cf0\highlight0\f2 
\par }
 