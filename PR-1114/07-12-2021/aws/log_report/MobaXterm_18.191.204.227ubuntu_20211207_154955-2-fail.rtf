{\rtf1\ansi\ansicpg1252\deff0\deflang1033{\fonttbl{\f0\fmodern Consolas;}{\f1\fmodern\fcharset0 Consolas;}{\f2\fnil\fcharset129 Courier New;}}
{\colortbl ;\red20\green20\blue20;\red142\green142\blue142;\red16\green158\blue98;\red148\green146\blue12;\red170\green51\blue170;\red56\green136\blue159;\red94\green147\blue162;\red18\green124\blue155;\red170\green64\blue64;\red85\green85\blue85;}
\viewkind4\uc1\pard\f0\fs20 Authenticating with public key "Imported-Openssh-Key: C:\\\\Users\\\\prajwal.akhuj\\\\Downloads\\\\prajwal-arktos-key.pem"\cf1\highlight2 
\par \cf0\highlight0     \u9484?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9488?\cf1\highlight2 
\par \cf0\highlight0     \u9474?                 \cf3\f1\bullet  MobaXterm Personal Edition v21.4 \bullet\cf0\f0                  \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?               \cf4 (SSH client, X server and network tools)\cf0                \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?                                                                      \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474? \u8594? SSH session to \cf5 ubuntu@18.191.204.227                     \cf0           \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?\f1    \bullet  Direct SSH      :  \cf3\f0 v\cf0                                              \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?\f1    \bullet  SSH compression :  \cf3\f0 v\cf0                                              \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?\f1    \bullet  SSH-browser     :  \cf3\f0 v\cf0                                              \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?\f1    \bullet  X11-forwarding  :  \cf3\f0 v\cf0   (remote display is forwarded through SSH)  \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?                                                                      \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474? \u8594? For more info, ctrl+click on \cf6\ul help\cf0\ulnone  or visit our \cf6\ul website\cf0\ulnone .            \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9492?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9496?\cf1\highlight2 
\par 
\par \cf0\highlight0 Welcome to Ubuntu 18.04.6 LTS (GNU/Linux 5.6.0-rc2 x86_64)\cf1\highlight2 
\par 
\par \cf0\highlight0  * Documentation:  https://help.ubuntu.com\cf1\highlight2 
\par \cf0\highlight0  * Management:     https://landscape.canonical.com\cf1\highlight2 
\par \cf0\highlight0  * Support:        https://ubuntu.com/advantage\cf1\highlight2 
\par 
\par \cf0\highlight0   System information as of Tue Dec  7 05:54:58 UTC 2021\cf1\highlight2 
\par 
\par \cf0\highlight0   System load:  1.91                Processes:              193\cf1\highlight2 
\par \cf0\highlight0   Usage of /:   13.7% of 116.27GB   Users logged in:        1\cf1\highlight2 
\par \cf0\highlight0   Memory usage: 10%                 IP address for eth0:    172.31.19.33\cf1\highlight2 
\par \cf0\highlight0   Swap usage:   0%                  IP address for docker0: 172.17.0.1\cf1\highlight2 
\par 
\par 
\par \cf0\highlight0 29 updates can be applied immediately.\cf1\highlight2 
\par \cf0\highlight0 25 of these updates are standard security updates.\cf1\highlight2 
\par \cf0\highlight0 To see these additional updates run: apt list --upgradable\cf1\highlight2 
\par 
\par \cf0\highlight0 New release '20.04.3 LTS' available.\cf1\highlight2 
\par \cf0\highlight0 Run 'do-release-upgrade' to upgrade to it.\cf1\highlight2 
\par 
\par 
\par \cf0\highlight0 Last login: Tue Dec  7 05:36:38 2021 from 114.143.207.106\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:~$ history\cf1\highlight2 
\par \cf0\highlight0     1  vi /etc/hsotename\cf1\highlight2 
\par \cf0\highlight0     2  vi /etc/hostname\cf1\highlight2 
\par \cf0\highlight0     3  chmod 777 /etc/hostname\cf1\highlight2 
\par \cf0\highlight0     4  sudo -i\cf1\highlight2 
\par \cf0\highlight0     5  history\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:~$ sudo -i\cf1\highlight2 
\par \cf0\highlight0 root@arktos:~/go/src/k8s.io/arktos# ls\cf1\highlight2 
\par \cf7\highlight0 BUILD.bazel\cf0       \cf7 Makefile\cf0                   \cf7 WORKSPACE\cf0             \cf8 build\cf0                go.mod      \cf8 perf-tests\cf0         \cf8 test\cf1\highlight2 
\par \cf8\highlight0 CHANGELOG\cf0         \cf7 Makefile.generated_files\cf0   \cf8 _output\cf0               \cf8 cluster\cf0              go.sum      \cf8 pkg\cf0                \cf8 third_party\cf1\highlight2 
\par \cf0\highlight0 CONTRIBUTING.md  OWNERS                    \cf8 api\cf0                   \cf8 cmd\cf0                  \cf8 hack\cf0         \cf8 plugin\cf0             \cf8 translations\cf1\highlight2 
\par \cf8\highlight0 Godeps\cf0            README.md                 aws_build_version    code-of-conduct.md  \cf8 logo\cf0         readthedocs.yaml  \cf8 vendor\cf1\highlight2 
\par \cf0\highlight0 LICENSE          SUPPORT.md                aws_kube_params.inc  \cf8 docs\cf0                 mkdocs.yml  \cf8 staging\cf1\highlight2 
\par \cf0\highlight0 root@arktos:~/go/src/k8s.io/arktos# history\cf1\highlight2 
\par \cf0\highlight0     1  vi /etc/hostname\cf1\highlight2 
\par \cf0\highlight0     2  init 6\cf1\highlight2 
\par \cf0\highlight0     3  uname -a\cf1\highlight2 
\par \cf0\highlight0     4  wget https://raw.githubusercontent.com/CentaurusInfra/mizar/dev-next/kernelupdate.sh\cf1\highlight2 
\par \cf0\highlight0     5  sudo bash kernelupdate.sh\cf1\highlight2 
\par \cf0\highlight0     6  ls\cf1\highlight2 
\par \cf0\highlight0     7  history\cf1\highlight2 
\par \cf0\highlight0 root@arktos:~/go/src/k8s.io/arktos# sudo su\cf1\highlight2 
\par \cf0\highlight0 root@arktos:~/go/src/k8s.io/arktos# su ubunut\cf1\highlight2 
\par \cf0\highlight0 No passwd entry for user 'ubunut'\cf1\highlight2 
\par \cf0\highlight0 root@arktos:~/go/src/k8s.io/arktos# su ubuntu\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ ./cluster/kubectl.sh get nodes -Ao wide\cf1\highlight2 
\par \cf0\highlight0 ./cluster/../cluster/../cluster/gce/../../cluster/gce/../../cluster/common.sh: line 30: /root/go/src/k8s.io/arktos/hack/lib/util.sh: Permission denied\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ cd /root\cf1\highlight2 
\par \cf0\highlight0 bash: cd: /root: Permission denied\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$  export KUBECONFIG=/var/run/kubernetes/admin.kubeconfig\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ ./cluster/kubectl.sh get nodes -Ao wide\cf1\highlight2 
\par \cf0\highlight0 ./cluster/../cluster/../cluster/gce/../../cluster/gce/../../cluster/common.sh: line 30: /root/go/src/k8s.io/arktos/hack/lib/util.sh: Permission denied\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo vi /root/go/src/k8s.io/arktos/hack/lib/util.sh\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo chmod 777 /root/go/src/k8s.io/arktos/hack/lib/util.sh\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ ./cluster/kubectl.sh get nodes -Ao wide\cf1\highlight2 
\par \cf0\highlight0 ./cluster/../cluster/../cluster/gce/../../cluster/gce/../../cluster/common.sh: line 30: /root/go/src/k8s.io/arktos/hack/lib/util.sh: Permission denied\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ ./cluster/kubectl.sh get nodes\cf1\highlight2 
\par \cf0\highlight0 ./cluster/../cluster/../cluster/gce/../../cluster/gce/../../cluster/common.sh: line 30: /root/go/src/k8s.io/arktos/hack/lib/util.sh: Permission denied\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo vi /root/go/src/k8s.io/arktos/hack/lib/util.sh\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ kubectl get nodes\cf1\highlight2 
\par \cf0\highlight0 kubectl: command not found\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ ./cluster/kubectl.sh get nodes\cf1\highlight2 
\par \cf0\highlight0 ./cluster/../cluster/../cluster/gce/../../cluster/gce/../../cluster/common.sh: line 30: /root/go/src/k8s.io/arktos/hack/lib/util.sh: Permission denied\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo vi ./cluster/../cluster/../cluster/gce/../../cluster/gce/../../cluster/common.sh\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ /cluster/kubectl.sh get nodes\cf1\highlight2 
\par \cf0\highlight0 bash: /cluster/kubectl.sh: No such file or directory\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ ./cluster/kubectl.sh get nodes\cf1\highlight2 
\par \cf0\highlight0 ./cluster/../cluster/../cluster/gce/../../cluster/gce/../../cluster/common.sh: line 30: /root/go/src/k8s.io/arktos/hack/lib/util.sh: Permission denied\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo chmod 777 ./cluster/../cluster/../cluster/gce/../../cluster/gce/../../cluster/common.sh\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ ./cluster/kubectl.sh get nodes\cf1\highlight2 
\par \cf0\highlight0 ./cluster/../cluster/../cluster/gce/../../cluster/gce/../../cluster/common.sh: line 30: /root/go/src/k8s.io/arktos/hack/lib/util.sh: Permission denied\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo -i\cf1\highlight2 
\par \cf0\highlight0 root@arktos:~/go/src/k8s.io/arktos# ./cluster/kubectl.sh get nodes\cf1\highlight2 
\par \cf0\highlight0 NAME     STATUS   ROLES    AGE   VERSION\cf1\highlight2 
\par \cf0\highlight0 arktos   Ready    <none>   47m   v0.9.0\cf1\highlight2 
\par \cf0\highlight0 root@arktos:~/go/src/k8s.io/arktos# ./cluster/kubectl.sh get nodes -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAME     STATUS   ROLES    AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION   CONTAINER-RUNTIME\cf1\highlight2 
\par \cf0\highlight0 arktos   Ready    <none>   48m   v0.9.0    172.31.19.33   <none>        Ubuntu 18.04.6 LTS   5.6.0-rc2        containerd://1.4.0-beta.1-29-g70b0d3cf\cf1\highlight2 
\par \cf0\highlight0 root@arktos:~/go/src/k8s.io/arktos# sudo su\cf1\highlight2 
\par \cf0\highlight0 root@arktos:~/go/src/k8s.io/arktos# su ubuntu\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get nodes -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAME     STATUS   ROLES    AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION   CONTAINER-RUNTIME\cf1\highlight2 
\par \cf0\highlight0 arktos   Ready    <none>   49m   v0.9.0    172.31.19.33   <none>        Ubuntu 18.04.6 LTS   5.6.0-rc2        containerd://1.4.0-beta.1-29-g70b0d3cf\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo /cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 sudo: /cluster/kubectl.sh: command not found\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-9zm46                5447211320762283802   1/1     Running   0          52m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-p979h   2737426404035080618   1/1     Running   0          52m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-2m9zz         7431288247843844650   0/3     Pending   0          52m   <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh describe pod kube-dns-554c5866fc-2m9zz\cf1\highlight2 
\par \cf0\highlight0 Error from server (NotFound): pods "kube-dns-554c5866fc-2m9zz" not found\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh describe pod kube-dns-554c5866fc-2m9zz -n kube-system\cf1\highlight2 
\par \cf0\highlight0 Name:                 kube-dns-554c5866fc-2m9zz\cf1\highlight2 
\par \cf0\highlight0 Namespace:            kube-system\cf1\highlight2 
\par \cf0\highlight0 Tenant:               system\cf1\highlight2 
\par \cf0\highlight0 Priority:             2000000000\cf1\highlight2 
\par \cf0\highlight0 Priority Class Name:  system-cluster-critical\cf1\highlight2 
\par \cf0\highlight0 Node:                 <none>\cf1\highlight2 
\par \cf0\highlight0 Labels:               k8s-app=kube-dns\cf1\highlight2 
\par \cf0\highlight0                       pod-template-hash=554c5866fc\cf1\highlight2 
\par \cf0\highlight0 Annotations:          kopf.zalando.org/last-handled-configuration:\cf1\highlight2 
\par \cf0\highlight0                         \{"spec":\{"volumes":[\{"name":"kube-dns-config","configMap":\{"name":"kube-dns","defaultMode":420,"optional":true\}\},\{"name":"kube-dns-token-6...\cf1\highlight2 
\par \cf0\highlight0                       prometheus.io/port: 10054\cf1\highlight2 
\par \cf0\highlight0                       prometheus.io/scrape: true\cf1\highlight2 
\par \cf0\highlight0                       scheduler.alpha.kubernetes.io/critical-pod:\cf1\highlight2 
\par \cf0\highlight0                       seccomp.security.alpha.kubernetes.io/pod: docker/default\cf1\highlight2 
\par \cf0\highlight0 Status:               Pending\cf1\highlight2 
\par \cf0\highlight0 IP:\cf1\highlight2 
\par \cf0\highlight0 Controlled By:        ReplicaSet/kube-dns-554c5866fc\cf1\highlight2 
\par \cf0\highlight0 Containers:\cf1\highlight2 
\par \cf0\highlight0   kubedns:\cf1\highlight2 
\par \cf0\highlight0     Image:       k8s.gcr.io/k8s-dns-kube-dns:1.14.13\cf1\highlight2 
\par \cf0\highlight0     Ports:       10053/UDP, 10053/TCP, 10055/TCP\cf1\highlight2 
\par \cf0\highlight0     Host Ports:  0/UDP, 0/TCP, 0/TCP\cf1\highlight2 
\par \cf0\highlight0     Args:\cf1\highlight2 
\par \cf0\highlight0       --domain=cluster.local.\cf1\highlight2 
\par \cf0\highlight0       --dns-port=10053\cf1\highlight2 
\par \cf0\highlight0       --config-dir=/kube-dns-config\cf1\highlight2 
\par \cf0\highlight0       --v=2\cf1\highlight2 
\par \cf0\highlight0     Limits:\cf1\highlight2 
\par \cf0\highlight0       memory:  170Mi\cf1\highlight2 
\par \cf0\highlight0     Requests:\cf1\highlight2 
\par \cf0\highlight0       cpu:      100m\cf1\highlight2 
\par \cf0\highlight0       memory:   70Mi\cf1\highlight2 
\par \cf0\highlight0     Liveness:   http-get http://:10054/healthcheck/kubedns delay=60s timeout=5s period=10s #success=1 #failure=5\cf1\highlight2 
\par \cf0\highlight0     Readiness:  http-get http://:8081/readiness delay=3s timeout=5s period=10s #success=1 #failure=3\cf1\highlight2 
\par \cf0\highlight0     Environment:\cf1\highlight2 
\par \cf0\highlight0       PROMETHEUS_PORT:  10055\cf1\highlight2 
\par \cf0\highlight0     Mounts:\cf1\highlight2 
\par \cf0\highlight0       /kube-dns-config from kube-dns-config (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/run/secrets/kubernetes.io/serviceaccount from kube-dns-token-6s4bc (ro)\cf1\highlight2 
\par \cf0\highlight0   dnsmasq:\cf1\highlight2 
\par \cf0\highlight0     Image:       k8s.gcr.io/k8s-dns-dnsmasq-nanny:1.14.13\cf1\highlight2 
\par \cf0\highlight0     Ports:       53/UDP, 53/TCP\cf1\highlight2 
\par \cf0\highlight0     Host Ports:  0/UDP, 0/TCP\cf1\highlight2 
\par \cf0\highlight0     Args:\cf1\highlight2 
\par \cf0\highlight0       -v=2\cf1\highlight2 
\par \cf0\highlight0       -logtostderr\cf1\highlight2 
\par \cf0\highlight0       -configDir=/etc/k8s/dns/dnsmasq-nanny\cf1\highlight2 
\par \cf0\highlight0       -restartDnsmasq=true\cf1\highlight2 
\par \cf0\highlight0       --\cf1\highlight2 
\par \cf0\highlight0       -k\cf1\highlight2 
\par \cf0\highlight0       --cache-size=1000\cf1\highlight2 
\par \cf0\highlight0       --no-negcache\cf1\highlight2 
\par \cf0\highlight0       --dns-loop-detect\cf1\highlight2 
\par \cf0\highlight0       --log-facility=-\cf1\highlight2 
\par \cf0\highlight0       --server=/cluster.local/127.0.0.1#10053\cf1\highlight2 
\par \cf0\highlight0       --server=/in-addr.arpa/127.0.0.1#10053\cf1\highlight2 
\par \cf0\highlight0       --server=/ip6.arpa/127.0.0.1#10053\cf1\highlight2 
\par \cf0\highlight0     Requests:\cf1\highlight2 
\par \cf0\highlight0       cpu:        150m\cf1\highlight2 
\par \cf0\highlight0       memory:     20Mi\cf1\highlight2 
\par \cf0\highlight0     Liveness:     http-get http://:10054/healthcheck/dnsmasq delay=60s timeout=5s period=10s #success=1 #failure=5\cf1\highlight2 
\par \cf0\highlight0     Environment:  <none>\cf1\highlight2 
\par \cf0\highlight0     Mounts:\cf1\highlight2 
\par \cf0\highlight0       /etc/k8s/dns/dnsmasq-nanny from kube-dns-config (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/run/secrets/kubernetes.io/serviceaccount from kube-dns-token-6s4bc (ro)\cf1\highlight2 
\par \cf0\highlight0   sidecar:\cf1\highlight2 
\par \cf0\highlight0     Image:      k8s.gcr.io/k8s-dns-sidecar:1.14.13\cf1\highlight2 
\par \cf0\highlight0     Port:       10054/TCP\cf1\highlight2 
\par \cf0\highlight0     Host Port:  0/TCP\cf1\highlight2 
\par \cf0\highlight0     Args:\cf1\highlight2 
\par \cf0\highlight0       --v=2\cf1\highlight2 
\par \cf0\highlight0       --logtostderr\cf1\highlight2 
\par \cf0\highlight0       --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,SRV\cf1\highlight2 
\par \cf0\highlight0       --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,SRV\cf1\highlight2 
\par \cf0\highlight0     Requests:\cf1\highlight2 
\par \cf0\highlight0       cpu:        10m\cf1\highlight2 
\par \cf0\highlight0       memory:     20Mi\cf1\highlight2 
\par \cf0\highlight0     Liveness:     http-get http://:10054/metrics delay=60s timeout=5s period=10s #success=1 #failure=5\cf1\highlight2 
\par \cf0\highlight0     Environment:  <none>\cf1\highlight2 
\par \cf0\highlight0     Mounts:\cf1\highlight2 
\par \cf0\highlight0       /var/run/secrets/kubernetes.io/serviceaccount from kube-dns-token-6s4bc (ro)\cf1\highlight2 
\par \cf0\highlight0 Conditions:\cf1\highlight2 
\par \cf0\highlight0   Type           Status\cf1\highlight2 
\par \cf0\highlight0   PodScheduled   False\cf1\highlight2 
\par \cf0\highlight0 Volumes:\cf1\highlight2 
\par \cf0\highlight0   kube-dns-config:\cf1\highlight2 
\par \cf0\highlight0     Type:      ConfigMap (a volume populated by a ConfigMap)\cf1\highlight2 
\par \cf0\highlight0     Name:      kube-dns\cf1\highlight2 
\par \cf0\highlight0     Optional:  true\cf1\highlight2 
\par \cf0\highlight0   kube-dns-token-6s4bc:\cf1\highlight2 
\par \cf0\highlight0     Type:        Secret (a volume populated by a Secret)\cf1\highlight2 
\par \cf0\highlight0     SecretName:  kube-dns-token-6s4bc\cf1\highlight2 
\par \cf0\highlight0     Optional:    false\cf1\highlight2 
\par \cf0\highlight0 QoS Class:       Burstable\cf1\highlight2 
\par \cf0\highlight0 Node-Selectors:  <none>\cf1\highlight2 
\par \cf0\highlight0 Tolerations:     CriticalAddonsOnly\cf1\highlight2 
\par \cf0\highlight0                  node.kubernetes.io/not-ready:NoExecute for 300s\cf1\highlight2 
\par \cf0\highlight0                  node.kubernetes.io/unreachable:NoExecute for 300s\cf1\highlight2 
\par \cf0\highlight0 Events:          <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-9zm46                5447211320762283802   1/1     Running   0          56m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-p979h   2737426404035080618   1/1     Running   0          56m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-2m9zz         7431288247843844650   0/3     Pending   0          56m   <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get vpc -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE   NAME   IP         PREFIX   VNI   DIVIDERS   STATUS   CREATETIME                   PROVISIONDELAY\cf1\highlight2 
\par \cf0\highlight0 default     vpc0   20.0.0.0   8        1     1          Init     2021-12-07T05:49:08.897243\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get subnets -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE   NAME   IP         PREFIX   VNI   VPC    STATUS   BOUNCERS   CREATETIME                   PROVISIONDELAY\cf1\highlight2 
\par \cf0\highlight0 default     net0   20.0.0.0   8        1     vpc0   Init     1          2021-12-07T05:49:08.965783\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get net -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAME      TYPE    VPC                      PHASE   DNS\cf1\highlight2 
\par \cf0\highlight0 default   mizar   system-default-network\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get dividers -Ao wide\cf1\highlight2 
\par \cf0\highlight0 No resources found.\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get divider -Ao wide\cf1\highlight2 
\par \cf0\highlight0 No resources found.\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get dividers -Ao wide\cf1\highlight2 
\par \cf0\highlight0 No resources found.\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get bouncers -Ao wide\cf1\highlight2 
\par \cf0\highlight0 No resources found.\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-9zm46                5447211320762283802   1/1     Running   0          66m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-p979h   2737426404035080618   1/1     Running   0          66m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-2m9zz         7431288247843844650   0/3     Pending   0          66m   <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get net -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAME      TYPE    VPC                      PHASE   DNS\cf1\highlight2 
\par \cf0\highlight0 default   mizar   system-default-network\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get net -A wide\cf1\highlight2 
\par \cf0\highlight0 Error from server (NotFound): networks.arktos.futurewei.com "wide" not found\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get net -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAME      TYPE    VPC                      PHASE   DNS\cf1\highlight2 
\par \cf0\highlight0 default   mizar   system-default-network\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-9zm46                5447211320762283802   1/1     Running   0          86m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-p979h   2737426404035080618   1/1     Running   0          86m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-2m9zz         7431288247843844650   0/3     Pending   0          86m   <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ ./cluster/kubectl.sh apply -f https://raw.githubusercontent.com/Click2CloudCentaurus/Documentation/main/test-yamls/test_pods.yaml\cf1\highlight2 
\par \cf0\highlight0 ./cluster/../cluster/../cluster/gce/../../cluster/gce/../../cluster/common.sh: line 30: /root/go/src/k8s.io/arktos/hack/lib/util.sh: Permission denied\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh apply -f https://raw.githubusercontent.com/Click2CloudCentaurus/Documentation/main/test-yamls/test_pods.yaml\cf1\highlight2 
\par \cf0\highlight0 error: unable to read URL "https://raw.githubusercontent.com/Click2CloudCentaurus/Documentation/main/test-yamls/test_pods.yaml", server reported 404 Not Found, status code=404\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh apply -f https://raw.githubusercontent.com/Click2Cloud-Centaurus/Documentation/main/test-yamls/test_pods.yaml\cf1\highlight2 
\par \cf0\highlight0 pod/netpod1 created\cf1\highlight2 
\par \cf0\highlight0 pod/netpod2 created\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -Ao wide                                                    NAMESPACE     NAME                              HASHKEY               READY   STATUS    RESTARTS   AGE    IP             NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-9zm46                5447211320762283802   1/1     Running   0          108m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-p979h   2737426404035080618   1/1     Running   0          108m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod1                           1413623026836083860   0/1     Pending   0          7s     <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod2                           8265626918342953523   0/1     Pending   0          7s     <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-2m9zz         7431288247843844650   0/3     Pending   0          108m   <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS    RESTARTS   AGE    IP             NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-9zm46                5447211320762283802   1/1     Running   0          108m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-p979h   2737426404035080618   1/1     Running   0          108m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod1                           1413623026836083860   0/1     Pending   0          15s    <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod2                           8265626918342953523   0/1     Pending   0          15s    <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-2m9zz         7431288247843844650   0/3     Pending   0          108m   <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS    RESTARTS   AGE    IP             NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-9zm46                5447211320762283802   1/1     Running   0          108m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-p979h   2737426404035080618   1/1     Running   0          108m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod1                           1413623026836083860   0/1     Pending   0          17s    <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod2                           8265626918342953523   0/1     Pending   0          17s    <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-2m9zz         7431288247843844650   0/3     Pending   0          108m   <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS    RESTARTS   AGE    IP             NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-9zm46                5447211320762283802   1/1     Running   0          108m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-p979h   2737426404035080618   1/1     Running   0          108m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod1                           1413623026836083860   0/1     Pending   0          18s    <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod2                           8265626918342953523   0/1     Pending   0          18s    <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-2m9zz         7431288247843844650   0/3     Pending   0          108m   <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS    RESTARTS   AGE    IP             NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-9zm46                5447211320762283802   1/1     Running   0          108m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-p979h   2737426404035080618   1/1     Running   0          108m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod1                           1413623026836083860   0/1     Pending   0          25s    <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod2                           8265626918342953523   0/1     Pending   0          25s    <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-2m9zz         7431288247843844650   0/3     Pending   0          108m   <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS    RESTARTS   AGE    IP             NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-9zm46                5447211320762283802   1/1     Running   0          108m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-p979h   2737426404035080618   1/1     Running   0          108m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod1                           1413623026836083860   0/1     Pending   0          27s    <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod2                           8265626918342953523   0/1     Pending   0          27s    <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-2m9zz         7431288247843844650   0/3     Pending   0          108m   <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh describe pod netpod1 -n default\cf1\highlight2 
\par \cf0\highlight0 Name:         netpod1\cf1\highlight2 
\par \cf0\highlight0 Namespace:    default\cf1\highlight2 
\par \cf0\highlight0 Tenant:       system\cf1\highlight2 
\par \cf0\highlight0 Priority:     0\cf1\highlight2 
\par \cf0\highlight0 Node:         <none>\cf1\highlight2 
\par \cf0\highlight0 Labels:       podkey=netpodkey1\cf1\highlight2 
\par \cf0\highlight0 Annotations:  kopf.zalando.org/builtins_on_pod:\cf1\highlight2 
\par \cf0\highlight0                 \{"started":"2021-12-07T07:36:40.236881","delayed":"2021-12-07T07:38:00.695554","purpose":"create","retries":4,"success":false,"failure":fa...\cf1\highlight2 
\par \cf0\highlight0               kubectl.kubernetes.io/last-applied-configuration:\cf1\highlight2 
\par \cf0\highlight0                 \{"apiVersion":"v1","kind":"Pod","metadata":\{"annotations":\{\},"labels":\{"podkey":"netpodkey1"\},"name":"netpod1","namespace":"default","tena...\cf1\highlight2 
\par \cf0\highlight0 Status:       Pending\cf1\highlight2 
\par \cf0\highlight0 IP:\cf1\highlight2 
\par \cf0\highlight0 Containers:\cf1\highlight2 
\par \cf0\highlight0   netctr:\cf1\highlight2 
\par \cf0\highlight0     Image:      yuvalif/fedora-tcpdump\cf1\highlight2 
\par \cf0\highlight0     Port:       <none>\cf1\highlight2 
\par \cf0\highlight0     Host Port:  <none>\cf1\highlight2 
\par \cf0\highlight0     Command:\cf1\highlight2 
\par \cf0\highlight0       tail\cf1\highlight2 
\par \cf0\highlight0       -f\cf1\highlight2 
\par \cf0\highlight0       /dev/null\cf1\highlight2 
\par \cf0\highlight0     Environment:  <none>\cf1\highlight2 
\par \cf0\highlight0     Mounts:\cf1\highlight2 
\par \cf0\highlight0       /var/run/secrets/kubernetes.io/serviceaccount from default-token-4b84l (ro)\cf1\highlight2 
\par \cf0\highlight0 Conditions:\cf1\highlight2 
\par \cf0\highlight0   Type           Status\cf1\highlight2 
\par \cf0\highlight0   PodScheduled   False\cf1\highlight2 
\par \cf0\highlight0 Volumes:\cf1\highlight2 
\par \cf0\highlight0   default-token-4b84l:\cf1\highlight2 
\par \cf0\highlight0     Type:        Secret (a volume populated by a Secret)\cf1\highlight2 
\par \cf0\highlight0     SecretName:  default-token-4b84l\cf1\highlight2 
\par \cf0\highlight0     Optional:    false\cf1\highlight2 
\par \cf0\highlight0 QoS Class:       BestEffort\cf1\highlight2 
\par \cf0\highlight0 Node-Selectors:  <none>\cf1\highlight2 
\par \cf0\highlight0 Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\cf1\highlight2 
\par \cf0\highlight0                  node.kubernetes.io/unreachable:NoExecute for 300s\cf1\highlight2 
\par \cf0\highlight0 Events:          <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods\cf1\highlight2 
\par \cf0\highlight0 NAME                              HASHKEY               READY   STATUS    RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 mizar-daemon-9zm46                5447211320762283802   1/1     Running   0          109m\cf1\highlight2 
\par \cf0\highlight0 mizar-operator-6b78d7ffc4-p979h   2737426404035080618   1/1     Running   0          109m\cf1\highlight2 
\par \cf0\highlight0 netpod1                           1413623026836083860   0/1     Pending   0          97s\cf1\highlight2 
\par \cf0\highlight0 netpod2                           8265626918342953523   0/1     Pending   0          97s\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS    RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-9zm46                5447211320762283802   1/1     Running   0          109m\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-p979h   2737426404035080618   1/1     Running   0          109m\cf1\highlight2 
\par \cf0\highlight0 default       netpod1                           1413623026836083860   0/1     Pending   0          102s\cf1\highlight2 
\par \cf0\highlight0 default       netpod2                           8265626918342953523   0/1     Pending   0          102s\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-2m9zz         7431288247843844650   0/3     Pending   0          109m\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get nodes\cf1\highlight2 
\par \cf0\highlight0 NAME     STATUS   ROLES    AGE    VERSION\cf1\highlight2 
\par \cf0\highlight0 arktos   Ready    <none>   110m   v0.9.0\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pod -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS    RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-9zm46                5447211320762283802   1/1     Running   0          110m\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-p979h   2737426404035080618   1/1     Running   0          110m\cf1\highlight2 
\par \cf0\highlight0 default       netpod1                           1413623026836083860   0/1     Pending   0          2m5s\cf1\highlight2 
\par \cf0\highlight0 default       netpod2                           8265626918342953523   0/1     Pending   0          2m5s\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-2m9zz         7431288247843844650   0/3     Pending   0          110m\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pod -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS    RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-9zm46                5447211320762283802   1/1     Running   0          110m\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-p979h   2737426404035080618   1/1     Running   0          110m\cf1\highlight2 
\par \cf0\highlight0 default       netpod1                           1413623026836083860   0/1     Pending   0          2m41s\cf1\highlight2 
\par \cf0\highlight0 default       netpod2                           8265626918342953523   0/1     Pending   0          2m41s\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-2m9zz         7431288247843844650   0/3     Pending   0          110m\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh describe pod netpod1 -n default\cf1\highlight2 
\par \cf0\highlight0 Name:         netpod1\cf1\highlight2 
\par \cf0\highlight0 Namespace:    default\cf1\highlight2 
\par \cf0\highlight0 Tenant:       system\cf1\highlight2 
\par \cf0\highlight0 Priority:     0\cf1\highlight2 
\par \cf0\highlight0 Node:         <none>\cf1\highlight2 
\par \cf0\highlight0 Labels:       podkey=netpodkey1\cf1\highlight2 
\par \cf0\highlight0 Annotations:  kopf.zalando.org/builtins_on_pod:\cf1\highlight2 
\par \cf0\highlight0                 \{"started":"2021-12-07T07:36:40.236881","delayed":"2021-12-07T07:39:41.414373","purpose":"create","retries":9,"success":false,"failure":fa...\cf1\highlight2 
\par \cf0\highlight0               kubectl.kubernetes.io/last-applied-configuration:\cf1\highlight2 
\par \cf0\highlight0                 \{"apiVersion":"v1","kind":"Pod","metadata":\{"annotations":\{\},"labels":\{"podkey":"netpodkey1"\},"name":"netpod1","namespace":"default","tena...\cf1\highlight2 
\par \cf0\highlight0 Status:       Pending\cf1\highlight2 
\par \cf0\highlight0 IP:\cf1\highlight2 
\par \cf0\highlight0 Containers:\cf1\highlight2 
\par \cf0\highlight0   netctr:\cf1\highlight2 
\par \cf0\highlight0     Image:      yuvalif/fedora-tcpdump\cf1\highlight2 
\par \cf0\highlight0     Port:       <none>\cf1\highlight2 
\par \cf0\highlight0     Host Port:  <none>\cf1\highlight2 
\par \cf0\highlight0     Command:\cf1\highlight2 
\par \cf0\highlight0       tail\cf1\highlight2 
\par \cf0\highlight0       -f\cf1\highlight2 
\par \cf0\highlight0       /dev/null\cf1\highlight2 
\par \cf0\highlight0     Environment:  <none>\cf1\highlight2 
\par \cf0\highlight0     Mounts:\cf1\highlight2 
\par \cf0\highlight0       /var/run/secrets/kubernetes.io/serviceaccount from default-token-4b84l (ro)\cf1\highlight2 
\par \cf0\highlight0 Conditions:\cf1\highlight2 
\par \cf0\highlight0   Type           Status\cf1\highlight2 
\par \cf0\highlight0   PodScheduled   False\cf1\highlight2 
\par \cf0\highlight0 Volumes:\cf1\highlight2 
\par \cf0\highlight0   default-token-4b84l:\cf1\highlight2 
\par \cf0\highlight0     Type:        Secret (a volume populated by a Secret)\cf1\highlight2 
\par \cf0\highlight0     SecretName:  default-token-4b84l\cf1\highlight2 
\par \cf0\highlight0     Optional:    false\cf1\highlight2 
\par \cf0\highlight0 QoS Class:       BestEffort\cf1\highlight2 
\par \cf0\highlight0 Node-Selectors:  <none>\cf1\highlight2 
\par \cf0\highlight0 Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\cf1\highlight2 
\par \cf0\highlight0                  node.kubernetes.io/unreachable:NoExecute for 300s\cf1\highlight2 
\par \cf0\highlight0 Events:          <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh pods -Ao\cf1\highlight2 
\par \cf0\highlight0 Error: unknown command "pods" for "kubectl"\cf1\highlight2 
\par 
\par \cf0\highlight0 Did you mean this?\cf1\highlight2 
\par \cf0\highlight0         logs\cf1\highlight2 
\par 
\par \cf0\highlight0 Run 'kubectl --help' for usage.\cf1\highlight2 
\par \cf0\highlight0 unknown command "pods" for "kubectl"\cf1\highlight2 
\par 
\par \cf0\highlight0 Did you mean this?\cf1\highlight2 
\par \cf0\highlight0         logs\cf1\highlight2 
\par 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ history\cf1\highlight2 
\par \cf0\highlight0     1  vi /etc/hsotename\cf1\highlight2 
\par \cf0\highlight0     2  vi /etc/hostname\cf1\highlight2 
\par \cf0\highlight0     3  chmod 777 /etc/hostname\cf1\highlight2 
\par \cf0\highlight0     4  sudo -i\cf1\highlight2 
\par \cf0\highlight0     5  sudo ./cluster/kubectl.sh get nodes -Ao wide\cf1\highlight2 
\par \cf0\highlight0     6  sudo /cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0     7  sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0     8  sudo ./cluster/kubectl.sh describe pod kube-dns-554c5866fc-2m9zz\cf1\highlight2 
\par \cf0\highlight0     9  sudo ./cluster/kubectl.sh describe pod kube-dns-554c5866fc-2m9zz -n kube-system\cf1\highlight2 
\par \cf0\highlight0    10  sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0    11  sudo ./cluster/kubectl.sh get vpc -Ao wide\cf1\highlight2 
\par \cf0\highlight0    12  sudo ./cluster/kubectl.sh get subnets -Ao wide\cf1\highlight2 
\par \cf0\highlight0    13  sudo ./cluster/kubectl.sh get net -Ao wide\cf1\highlight2 
\par \cf0\highlight0    14  sudo ./cluster/kubectl.sh get dividers -Ao wide\cf1\highlight2 
\par \cf0\highlight0    15  sudo ./cluster/kubectl.sh get divider -Ao wide\cf1\highlight2 
\par \cf0\highlight0    16  sudo ./cluster/kubectl.sh get dividers -Ao wide\cf1\highlight2 
\par \cf0\highlight0    17  sudo ./cluster/kubectl.sh get bouncers -Ao wide\cf1\highlight2 
\par \cf0\highlight0    18  sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0    19  sudo ./cluster/kubectl.sh get net -Ao wide\cf1\highlight2 
\par \cf0\highlight0    20  sudo ./cluster/kubectl.sh get net -A wide\cf1\highlight2 
\par \cf0\highlight0    21  sudo ./cluster/kubectl.sh get net -Ao wide\cf1\highlight2 
\par \cf0\highlight0    22  sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0    23  ./cluster/kubectl.sh apply -f https://raw.githubusercontent.com/Click2CloudCentaurus/Documentation/main/test-yamls/test_pods.yaml\cf1\highlight2 
\par \cf0\highlight0    24  sudo ./cluster/kubectl.sh apply -f https://raw.githubusercontent.com/Click2CloudCentaurus/Documentation/main/test-yamls/test_pods.yaml\cf1\highlight2 
\par \cf0\highlight0    25  sudo ./cluster/kubectl.sh apply -f https://raw.githubusercontent.com/Click2Cloud-Centaurus/Documentation/main/test-yamls/test_pods.yaml\cf1\highlight2 
\par \cf0\highlight0    26  sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0    27  sudo ./cluster/kubectl.sh describe pod netpod1 -n default\cf1\highlight2 
\par \cf0\highlight0    28  sudo ./cluster/kubectl.sh get pods\cf1\highlight2 
\par \cf0\highlight0    29  sudo ./cluster/kubectl.sh get pods -A\cf1\highlight2 
\par \cf0\highlight0    30  sudo ./cluster/kubectl.sh get nodes\cf1\highlight2 
\par \cf0\highlight0    31  sudo ./cluster/kubectl.sh get pod -A\cf1\highlight2 
\par \cf0\highlight0    32  sudo ./cluster/kubectl.sh describe pod netpod1 -n default\cf1\highlight2 
\par \cf0\highlight0    33  sudo ./cluster/kubectl.sh pods -Ao\cf1\highlight2 
\par \cf0\highlight0    34  history\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh apply -f https://raw.githubusercontent.com/Click2Cloud-Centaurus/Documentation/main/test-yamls/test_pods.yaml\cf1\highlight2 
\par \cf0\highlight0 pod/netpod1 unchanged\cf1\highlight2 
\par \cf0\highlight0 pod/netpod2 unchanged\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh delete -f https://raw.githubusercontent.com/Click2Cloud-Centaurus/Documentation/main/test-yamls/test_pods.yaml\cf1\highlight2 
\par \cf0\highlight0 pod "netpod1" deleted\cf1\highlight2 
\par \cf0\highlight0 pod "netpod2" deleted\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ vi pod.yaml\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo vi pod.yaml\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh apply -f pod.yaml\cf1\highlight2 
\par \cf0\highlight0 pod/netpod1 created\cf1\highlight2 
\par \cf0\highlight0 pod/netpod2 created\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh pods -A\cf1\highlight2 
\par \cf0\highlight0 Error: unknown command "pods" for "kubectl"\cf1\highlight2 
\par 
\par \cf0\highlight0 Did you mean this?\cf1\highlight2 
\par \cf0\highlight0         logs\cf1\highlight2 
\par 
\par \cf0\highlight0 Run 'kubectl --help' for usage.\cf1\highlight2 
\par \cf0\highlight0 unknown command "pods" for "kubectl"\cf1\highlight2 
\par 
\par \cf0\highlight0 Did you mean this?\cf1\highlight2 
\par \cf0\highlight0         logs\cf1\highlight2 
\par 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 Error: unknown command "pods" for "kubectl"\cf1\highlight2 
\par 
\par \cf0\highlight0 Did you mean this?\cf1\highlight2 
\par \cf0\highlight0         logs\cf1\highlight2 
\par 
\par \cf0\highlight0 Run 'kubectl --help' for usage.\cf1\highlight2 
\par \cf0\highlight0 unknown command "pods" for "kubectl"\cf1\highlight2 
\par 
\par \cf0\highlight0 Did you mean this?\cf1\highlight2 
\par \cf0\highlight0         logs\cf1\highlight2 
\par 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS    RESTARTS   AGE    IP             NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-9zm46                5447211320762283802   1/1     Running   0          114m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-p979h   2737426404035080618   1/1     Running   0          114m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod1                           1089016030865069193   0/1     Pending   0          23s    <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod2                           8395493550890446567   0/1     Pending   0          23s    <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-2m9zz         7431288247843844650   0/3     Pending   0          114m   <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS    RESTARTS   AGE    IP             NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-9zm46                5447211320762283802   1/1     Running   0          114m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-p979h   2737426404035080618   1/1     Running   0          114m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod1                           1089016030865069193   0/1     Pending   0          33s    <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod2                           8395493550890446567   0/1     Pending   0          33s    <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-2m9zz         7431288247843844650   0/3     Pending   0          114m   <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS    RESTARTS   AGE    IP             NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-9zm46                5447211320762283802   1/1     Running   0          114m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-p979h   2737426404035080618   1/1     Running   0          114m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod1                           1089016030865069193   0/1     Pending   0          35s    <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod2                           8395493550890446567   0/1     Pending   0          35s    <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-2m9zz         7431288247843844650   0/3     Pending   0          114m   <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS    RESTARTS   AGE    IP             NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-9zm46                5447211320762283802   1/1     Running   0          114m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-p979h   2737426404035080618   1/1     Running   0          114m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod1                           1089016030865069193   0/1     Pending   0          44s    <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod2                           8395493550890446567   0/1     Pending   0          44s    <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-2m9zz         7431288247843844650   0/3     Pending   0          114m   <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS    RESTARTS   AGE    IP             NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-9zm46                5447211320762283802   1/1     Running   0          115m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-p979h   2737426404035080618   1/1     Running   0          115m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod1                           1089016030865069193   0/1     Pending   0          48s    <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod2                           8395493550890446567   0/1     Pending   0          48s    <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-2m9zz         7431288247843844650   0/3     Pending   0          115m   <none>         <none>   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh describe pod netpod1 -n default\cf1\highlight2 
\par \cf0\highlight0 Name:         netpod1\cf1\highlight2 
\par \cf0\highlight0 Namespace:    default\cf1\highlight2 
\par \cf0\highlight0 Tenant:       system\cf1\highlight2 
\par \cf0\highlight0 Priority:     0\cf1\highlight2 
\par \cf0\highlight0 Node:         <none>\cf1\highlight2 
\par \cf0\highlight0 Labels:       podkey=netpodkey1\cf1\highlight2 
\par \cf0\highlight0 Annotations:  kopf.zalando.org/builtins_on_pod:\cf1\highlight2 
\par \cf0\highlight0                 \{"started":"2021-12-07T07:42:54.221110","delayed":"2021-12-07T07:44:14.690220","purpose":"create","retries":4,"success":false,"failure":fa...\cf1\highlight2 
\par \cf0\highlight0               kubectl.kubernetes.io/last-applied-configuration:\cf1\highlight2 
\par \cf0\highlight0                 \{"apiVersion":"v1","kind":"Pod","metadata":\{"annotations":\{\},"labels":\{"podkey":"netpodkey1"\},"name":"netpod1","namespace":"default","tena...\cf1\highlight2 
\par \cf0\highlight0 Status:       Pending\cf1\highlight2 
\par \cf0\highlight0 IP:\cf1\highlight2 
\par \cf0\highlight0 Containers:\cf1\highlight2 
\par \cf0\highlight0   netctr:\cf1\highlight2 
\par \cf0\highlight0     Image:      yuvalif/fedora-tcpdump\cf1\highlight2 
\par \cf0\highlight0     Port:       <none>\cf1\highlight2 
\par \cf0\highlight0     Host Port:  <none>\cf1\highlight2 
\par \cf0\highlight0     Command:\cf1\highlight2 
\par \cf0\highlight0       tail\cf1\highlight2 
\par \cf0\highlight0       -f\cf1\highlight2 
\par \cf0\highlight0       /dev/null\cf1\highlight2 
\par \cf0\highlight0     Environment:  <none>\cf1\highlight2 
\par \cf0\highlight0     Mounts:\cf1\highlight2 
\par \cf0\highlight0       /var/run/secrets/kubernetes.io/serviceaccount from default-token-4b84l (ro)\cf1\highlight2 
\par \cf0\highlight0 Conditions:\cf1\highlight2 
\par \cf0\highlight0   Type           Status\cf1\highlight2 
\par \cf0\highlight0   PodScheduled   False\cf1\highlight2 
\par \cf0\highlight0 Volumes:\cf1\highlight2 
\par \cf0\highlight0   default-token-4b84l:\cf1\highlight2 
\par \cf0\highlight0     Type:        Secret (a volume populated by a Secret)\cf1\highlight2 
\par \cf0\highlight0     SecretName:  default-token-4b84l\cf1\highlight2 
\par \cf0\highlight0     Optional:    false\cf1\highlight2 
\par \cf0\highlight0 QoS Class:       BestEffort\cf1\highlight2 
\par \cf0\highlight0 Node-Selectors:  <none>\cf1\highlight2 
\par \cf0\highlight0 Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\cf1\highlight2 
\par \cf0\highlight0                  node.kubernetes.io/unreachable:NoExecute for 300s\cf1\highlight2 
\par \cf0\highlight0 Events:          <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh describe pod netpod1 -n default\cf1\highlight2 
\par \cf0\highlight0 Name:         netpod1\cf1\highlight2 
\par \cf0\highlight0 Namespace:    default\cf1\highlight2 
\par \cf0\highlight0 Tenant:       system\cf1\highlight2 
\par \cf0\highlight0 Priority:     0\cf1\highlight2 
\par \cf0\highlight0 Node:         <none>\cf1\highlight2 
\par \cf0\highlight0 Labels:       podkey=netpodkey1\cf1\highlight2 
\par \cf0\highlight0 Annotations:  kopf.zalando.org/last-handled-configuration:\cf1\highlight2 
\par \cf0\highlight0                 \{"spec":\{"volumes":[\{"name":"default-token-4b84l","secret":\{"secretName":"default-token-4b84l","defaultMode":420\}\}],"containers":[\{"name":...\cf1\highlight2 
\par \cf0\highlight0               kubectl.kubernetes.io/last-applied-configuration:\cf1\highlight2 
\par \cf0\highlight0                 \{"apiVersion":"v1","kind":"Pod","metadata":\{"annotations":\{\},"labels":\{"podkey":"netpodkey1"\},"name":"netpod1","namespace":"default","tena...\cf1\highlight2 
\par \cf0\highlight0 Status:       Pending\cf1\highlight2 
\par \cf0\highlight0 IP:\cf1\highlight2 
\par \cf0\highlight0 Containers:\cf1\highlight2 
\par \cf0\highlight0   netctr:\cf1\highlight2 
\par \cf0\highlight0     Image:      yuvalif/fedora-tcpdump\cf1\highlight2 
\par \cf0\highlight0     Port:       <none>\cf1\highlight2 
\par \cf0\highlight0     Host Port:  <none>\cf1\highlight2 
\par \cf0\highlight0     Command:\cf1\highlight2 
\par \cf0\highlight0       tail\cf1\highlight2 
\par \cf0\highlight0       -f\cf1\highlight2 
\par \cf0\highlight0       /dev/null\cf1\highlight2 
\par \cf0\highlight0     Environment:  <none>\cf1\highlight2 
\par \cf0\highlight0     Mounts:\cf1\highlight2 
\par \cf0\highlight0       /var/run/secrets/kubernetes.io/serviceaccount from default-token-4b84l (ro)\cf1\highlight2 
\par \cf0\highlight0 Conditions:\cf1\highlight2 
\par \cf0\highlight0   Type           Status\cf1\highlight2 
\par \cf0\highlight0   PodScheduled   False\cf1\highlight2 
\par \cf0\highlight0 Volumes:\cf1\highlight2 
\par \cf0\highlight0   default-token-4b84l:\cf1\highlight2 
\par \cf0\highlight0     Type:        Secret (a volume populated by a Secret)\cf1\highlight2 
\par \cf0\highlight0     SecretName:  default-token-4b84l\cf1\highlight2 
\par \cf0\highlight0     Optional:    false\cf1\highlight2 
\par \cf0\highlight0 QoS Class:       BestEffort\cf1\highlight2 
\par \cf0\highlight0 Node-Selectors:  <none>\cf1\highlight2 
\par \cf0\highlight0 Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\cf1\highlight2 
\par \cf0\highlight0                  node.kubernetes.io/unreachable:NoExecute for 300s\cf1\highlight2 
\par \cf0\highlight0 Events:          <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -A wide\cf1\highlight2 
\par \cf0\highlight0 error: a resource cannot be retrieved by name across all namespaces\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS    RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-9zm46                5447211320762283802   1/1     Running   0          159m\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-p979h   2737426404035080618   1/1     Running   0          159m\cf1\highlight2 
\par \cf0\highlight0 default       netpod1                           1089016030865069193   0/1     Pending   0          45m\cf1\highlight2 
\par \cf0\highlight0 default       netpod2                           8395493550890446567   0/1     Pending   0          45m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-2m9zz         7431288247843844650   0/3     Pending   0          159m\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh describe pod kube-dns-554c5866fc-2m9zz -n kube-system\cf1\highlight2 
\par \cf0\highlight0 Name:                 kube-dns-554c5866fc-2m9zz\cf1\highlight2 
\par \cf0\highlight0 Namespace:            kube-system\cf1\highlight2 
\par \cf0\highlight0 Tenant:               system\cf1\highlight2 
\par \cf0\highlight0 Priority:             2000000000\cf1\highlight2 
\par \cf0\highlight0 Priority Class Name:  system-cluster-critical\cf1\highlight2 
\par \cf0\highlight0 Node:                 <none>\cf1\highlight2 
\par \cf0\highlight0 Labels:               k8s-app=kube-dns\cf1\highlight2 
\par \cf0\highlight0                       pod-template-hash=554c5866fc\cf1\highlight2 
\par \cf0\highlight0 Annotations:          kopf.zalando.org/last-handled-configuration:\cf1\highlight2 
\par \cf0\highlight0                         \{"spec":\{"volumes":[\{"name":"kube-dns-config","configMap":\{"name":"kube-dns","defaultMode":420,"optional":true\}\},\{"name":"kube-dns-token-6...\cf1\highlight2 
\par \cf0\highlight0                       prometheus.io/port: 10054\cf1\highlight2 
\par \cf0\highlight0                       prometheus.io/scrape: true\cf1\highlight2 
\par \cf0\highlight0                       scheduler.alpha.kubernetes.io/critical-pod:\cf1\highlight2 
\par \cf0\highlight0                       seccomp.security.alpha.kubernetes.io/pod: docker/default\cf1\highlight2 
\par \cf0\highlight0 Status:               Pending\cf1\highlight2 
\par \cf0\highlight0 IP:\cf1\highlight2 
\par \cf0\highlight0 Controlled By:        ReplicaSet/kube-dns-554c5866fc\cf1\highlight2 
\par \cf0\highlight0 Containers:\cf1\highlight2 
\par \cf0\highlight0   kubedns:\cf1\highlight2 
\par \cf0\highlight0     Image:       k8s.gcr.io/k8s-dns-kube-dns:1.14.13\cf1\highlight2 
\par \cf0\highlight0     Ports:       10053/UDP, 10053/TCP, 10055/TCP\cf1\highlight2 
\par \cf0\highlight0     Host Ports:  0/UDP, 0/TCP, 0/TCP\cf1\highlight2 
\par \cf0\highlight0     Args:\cf1\highlight2 
\par \cf0\highlight0       --domain=cluster.local.\cf1\highlight2 
\par \cf0\highlight0       --dns-port=10053\cf1\highlight2 
\par \cf0\highlight0       --config-dir=/kube-dns-config\cf1\highlight2 
\par \cf0\highlight0       --v=2\cf1\highlight2 
\par \cf0\highlight0     Limits:\cf1\highlight2 
\par \cf0\highlight0       memory:  170Mi\cf1\highlight2 
\par \cf0\highlight0     Requests:\cf1\highlight2 
\par \cf0\highlight0       cpu:      100m\cf1\highlight2 
\par \cf0\highlight0       memory:   70Mi\cf1\highlight2 
\par \cf0\highlight0     Liveness:   http-get http://:10054/healthcheck/kubedns delay=60s timeout=5s period=10s #success=1 #failure=5\cf1\highlight2 
\par \cf0\highlight0     Readiness:  http-get http://:8081/readiness delay=3s timeout=5s period=10s #success=1 #failure=3\cf1\highlight2 
\par \cf0\highlight0     Environment:\cf1\highlight2 
\par \cf0\highlight0       PROMETHEUS_PORT:  10055\cf1\highlight2 
\par \cf0\highlight0     Mounts:\cf1\highlight2 
\par \cf0\highlight0       /kube-dns-config from kube-dns-config (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/run/secrets/kubernetes.io/serviceaccount from kube-dns-token-6s4bc (ro)\cf1\highlight2 
\par \cf0\highlight0   dnsmasq:\cf1\highlight2 
\par \cf0\highlight0     Image:       k8s.gcr.io/k8s-dns-dnsmasq-nanny:1.14.13\cf1\highlight2 
\par \cf0\highlight0     Ports:       53/UDP, 53/TCP\cf1\highlight2 
\par \cf0\highlight0     Host Ports:  0/UDP, 0/TCP\cf1\highlight2 
\par \cf0\highlight0     Args:\cf1\highlight2 
\par \cf0\highlight0       -v=2\cf1\highlight2 
\par \cf0\highlight0       -logtostderr\cf1\highlight2 
\par \cf0\highlight0       -configDir=/etc/k8s/dns/dnsmasq-nanny\cf1\highlight2 
\par \cf0\highlight0       -restartDnsmasq=true\cf1\highlight2 
\par \cf0\highlight0       --\cf1\highlight2 
\par \cf0\highlight0       -k\cf1\highlight2 
\par \cf0\highlight0       --cache-size=1000\cf1\highlight2 
\par \cf0\highlight0       --no-negcache\cf1\highlight2 
\par \cf0\highlight0       --dns-loop-detect\cf1\highlight2 
\par \cf0\highlight0       --log-facility=-\cf1\highlight2 
\par \cf0\highlight0       --server=/cluster.local/127.0.0.1#10053\cf1\highlight2 
\par \cf0\highlight0       --server=/in-addr.arpa/127.0.0.1#10053\cf1\highlight2 
\par \cf0\highlight0       --server=/ip6.arpa/127.0.0.1#10053\cf1\highlight2 
\par \cf0\highlight0     Requests:\cf1\highlight2 
\par \cf0\highlight0       cpu:        150m\cf1\highlight2 
\par \cf0\highlight0       memory:     20Mi\cf1\highlight2 
\par \cf0\highlight0     Liveness:     http-get http://:10054/healthcheck/dnsmasq delay=60s timeout=5s period=10s #success=1 #failure=5\cf1\highlight2 
\par \cf0\highlight0     Environment:  <none>\cf1\highlight2 
\par \cf0\highlight0     Mounts:\cf1\highlight2 
\par \cf0\highlight0       /etc/k8s/dns/dnsmasq-nanny from kube-dns-config (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/run/secrets/kubernetes.io/serviceaccount from kube-dns-token-6s4bc (ro)\cf1\highlight2 
\par \cf0\highlight0   sidecar:\cf1\highlight2 
\par \cf0\highlight0     Image:      k8s.gcr.io/k8s-dns-sidecar:1.14.13\cf1\highlight2 
\par \cf0\highlight0     Port:       10054/TCP\cf1\highlight2 
\par \cf0\highlight0     Host Port:  0/TCP\cf1\highlight2 
\par \cf0\highlight0     Args:\cf1\highlight2 
\par \cf0\highlight0       --v=2\cf1\highlight2 
\par \cf0\highlight0       --logtostderr\cf1\highlight2 
\par \cf0\highlight0       --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,SRV\cf1\highlight2 
\par \cf0\highlight0       --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,SRV\cf1\highlight2 
\par \cf0\highlight0     Requests:\cf1\highlight2 
\par \cf0\highlight0       cpu:        10m\cf1\highlight2 
\par \cf0\highlight0       memory:     20Mi\cf1\highlight2 
\par \cf0\highlight0     Liveness:     http-get http://:10054/metrics delay=60s timeout=5s period=10s #success=1 #failure=5\cf1\highlight2 
\par \cf0\highlight0     Environment:  <none>\cf1\highlight2 
\par \cf0\highlight0     Mounts:\cf1\highlight2 
\par \cf0\highlight0       /var/run/secrets/kubernetes.io/serviceaccount from kube-dns-token-6s4bc (ro)\cf1\highlight2 
\par \cf0\highlight0 Conditions:\cf1\highlight2 
\par \cf0\highlight0   Type           Status\cf1\highlight2 
\par \cf0\highlight0   PodScheduled   False\cf1\highlight2 
\par \cf0\highlight0 Volumes:\cf1\highlight2 
\par \cf0\highlight0   kube-dns-config:\cf1\highlight2 
\par \cf0\highlight0     Type:      ConfigMap (a volume populated by a ConfigMap)\cf1\highlight2 
\par \cf0\highlight0     Name:      kube-dns\cf1\highlight2 
\par \cf0\highlight0     Optional:  true\cf1\highlight2 
\par \cf0\highlight0   kube-dns-token-6s4bc:\cf1\highlight2 
\par \cf0\highlight0     Type:        Secret (a volume populated by a Secret)\cf1\highlight2 
\par \cf0\highlight0     SecretName:  kube-dns-token-6s4bc\cf1\highlight2 
\par \cf0\highlight0     Optional:    false\cf1\highlight2 
\par \cf0\highlight0 QoS Class:       Burstable\cf1\highlight2 
\par \cf0\highlight0 Node-Selectors:  <none>\cf1\highlight2 
\par \cf0\highlight0 Tolerations:     CriticalAddonsOnly\cf1\highlight2 
\par \cf0\highlight0                  node.kubernetes.io/not-ready:NoExecute for 300s\cf1\highlight2 
\par \cf0\highlight0                  node.kubernetes.io/unreachable:NoExecute for 300s\cf1\highlight2 
\par \cf0\highlight0 Events:          <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS    RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-9zm46                5447211320762283802   1/1     Running   0          160m\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-p979h   2737426404035080618   1/1     Running   0          160m\cf1\highlight2 
\par \cf0\highlight0 default       netpod1                           1089016030865069193   0/1     Pending   0          46m\cf1\highlight2 
\par \cf0\highlight0 default       netpod2                           8395493550890446567   0/1     Pending   0          46m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-2m9zz         7431288247843844650   0/3     Pending   0          160m\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh describe pod netpod1 -n deafualt\cf1\highlight2 
\par \cf0\highlight0 Error from server (NotFound): pods "netpod1" not found\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh describe pod netpod1 -n default\cf1\highlight2 
\par \cf0\highlight0 Name:         netpod1\cf1\highlight2 
\par \cf0\highlight0 Namespace:    default\cf1\highlight2 
\par \cf0\highlight0 Tenant:       system\cf1\highlight2 
\par \cf0\highlight0 Priority:     0\cf1\highlight2 
\par \cf0\highlight0 Node:         <none>\cf1\highlight2 
\par \cf0\highlight0 Labels:       podkey=netpodkey1\cf1\highlight2 
\par \cf0\highlight0 Annotations:  kopf.zalando.org/last-handled-configuration:\cf1\highlight2 
\par \cf0\highlight0                 \{"spec":\{"volumes":[\{"name":"default-token-4b84l","secret":\{"secretName":"default-token-4b84l","defaultMode":420\}\}],"containers":[\{"name":...\cf1\highlight2 
\par \cf0\highlight0               kubectl.kubernetes.io/last-applied-configuration:\cf1\highlight2 
\par \cf0\highlight0                 \{"apiVersion":"v1","kind":"Pod","metadata":\{"annotations":\{\},"labels":\{"podkey":"netpodkey1"\},"name":"netpod1","namespace":"default","tena...\cf1\highlight2 
\par \cf0\highlight0 Status:       Pending\cf1\highlight2 
\par \cf0\highlight0 IP:\cf1\highlight2 
\par \cf0\highlight0 Containers:\cf1\highlight2 
\par \cf0\highlight0   netctr:\cf1\highlight2 
\par \cf0\highlight0     Image:      yuvalif/fedora-tcpdump\cf1\highlight2 
\par \cf0\highlight0     Port:       <none>\cf1\highlight2 
\par \cf0\highlight0     Host Port:  <none>\cf1\highlight2 
\par \cf0\highlight0     Command:\cf1\highlight2 
\par \cf0\highlight0       tail\cf1\highlight2 
\par \cf0\highlight0       -f\cf1\highlight2 
\par \cf0\highlight0       /dev/null\cf1\highlight2 
\par \cf0\highlight0     Environment:  <none>\cf1\highlight2 
\par \cf0\highlight0     Mounts:\cf1\highlight2 
\par \cf0\highlight0       /var/run/secrets/kubernetes.io/serviceaccount from default-token-4b84l (ro)\cf1\highlight2 
\par \cf0\highlight0 Conditions:\cf1\highlight2 
\par \cf0\highlight0   Type           Status\cf1\highlight2 
\par \cf0\highlight0   PodScheduled   False\cf1\highlight2 
\par \cf0\highlight0 Volumes:\cf1\highlight2 
\par \cf0\highlight0   default-token-4b84l:\cf1\highlight2 
\par \cf0\highlight0     Type:        Secret (a volume populated by a Secret)\cf1\highlight2 
\par \cf0\highlight0     SecretName:  default-token-4b84l\cf1\highlight2 
\par \cf0\highlight0     Optional:    false\cf1\highlight2 
\par \cf0\highlight0 QoS Class:       BestEffort\cf1\highlight2 
\par \cf0\highlight0 Node-Selectors:  <none>\cf1\highlight2 
\par \cf0\highlight0 Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\cf1\highlight2 
\par \cf0\highlight0                  node.kubernetes.io/unreachable:NoExecute for 300s\cf1\highlight2 
\par \cf0\highlight0 Events:          <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ ./cluster/kubectl.sh get pods -A\cf1\highlight2 
\par \cf0\highlight0 ./cluster/../cluster/../cluster/gce/../../cluster/gce/../../cluster/common.sh: line 30: /root/go/src/k8s.io/arktos/hack/lib/util.sh: Permission denied\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS    RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-9zm46                5447211320762283802   1/1     Running   0          176m\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-p979h   2737426404035080618   1/1     Running   0          176m\cf1\highlight2 
\par \cf0\highlight0 default       netpod1                           1089016030865069193   0/1     Pending   0          62m\cf1\highlight2 
\par \cf0\highlight0 default       netpod2                           8395493550890446567   0/1     Pending   0          62m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-2m9zz         7431288247843844650   0/3     Pending   0          176m\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get node -A\cf1\highlight2 
\par \cf0\highlight0 NAME     STATUS   ROLES    AGE    VERSION\cf1\highlight2 
\par \cf0\highlight0 arktos   Ready    <none>   177m   v0.9.0\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get events -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     LAST SEEN   TYPE      REASON             OBJECT                          MESSAGE\cf1\highlight2 
\par \cf0\highlight0 default       2m14s       Warning   ImageGCFailed      node/arktos                     rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/virtlet.sock: connect: no such file or directory"\cf1\highlight2 
\par \cf0\highlight0 default       59m         Error     Logging            pod/netpod1                     Handler 'builtins_on_pod' failed temporarily: Temporary Error: Pod spec not ready.\cf1\highlight2 
\par \cf0\highlight0 default       58m         Error     Logging            pod/netpod1                     Handler 'builtins_on_pod' failed temporarily: Temporary Error: Pod spec not ready.\cf1\highlight2 
\par \cf0\highlight0 default       59m         Error     Logging            pod/netpod2                     Handler 'builtins_on_pod' failed temporarily: Temporary Error: Pod spec not ready.\cf1\highlight2 
\par \cf0\highlight0 default       60m         Error     Logging            pod/netpod2                     Handler 'builtins_on_pod' failed temporarily: Temporary Error: Pod spec not ready.\cf1\highlight2 
\par \cf0\highlight0 default       57m         Error     Logging            pod/netpod1                     Handler 'builtins_on_pod' failed permanently: Handler 'builtins_on_pod' has exceeded 15 retries.\cf1\highlight2 
\par \cf0\highlight0 default       58m         Error     Logging            pod/netpod2                     Handler 'builtins_on_pod' failed temporarily: Temporary Error: Pod spec not ready.\cf1\highlight2 
\par \cf0\highlight0 default       60m         Error     Logging            pod/netpod2                     Handler 'builtins_on_pod' failed temporarily: Temporary Error: Pod spec not ready.\cf1\highlight2 
\par \cf0\highlight0 default       59m         Error     Logging            pod/netpod1                     Handler 'builtins_on_pod' failed temporarily: Temporary Error: Pod spec not ready.\cf1\highlight2 
\par \cf0\highlight0 default       58m         Error     Logging            pod/netpod1                     Handler 'builtins_on_pod' failed temporarily: Temporary Error: Pod spec not ready.\cf1\highlight2 
\par \cf0\highlight0 default       58m         Error     Logging            pod/netpod2                     Handler 'builtins_on_pod' failed temporarily: Temporary Error: Pod spec not ready.\cf1\highlight2 
\par \cf0\highlight0 default       58m         Error     Logging            pod/netpod1                     Handler 'builtins_on_pod' failed temporarily: Temporary Error: Pod spec not ready.\cf1\highlight2 
\par \cf0\highlight0 default       59m         Error     Logging            pod/netpod2                     Handler 'builtins_on_pod' failed temporarily: Temporary Error: Pod spec not ready.\cf1\highlight2 
\par \cf0\highlight0 default       57m         Normal    Logging            pod/netpod1                     Creation is processed: 0 succeeded; 1 failed.\cf1\highlight2 
\par \cf0\highlight0 default       57m         Error     Logging            pod/netpod2                     Handler 'builtins_on_pod' failed permanently: Handler 'builtins_on_pod' has exceeded 15 retries.\cf1\highlight2 
\par \cf0\highlight0 default       60m         Error     Logging            pod/netpod1                     Handler 'builtins_on_pod' failed temporarily: Temporary Error: Pod spec not ready.\cf1\highlight2 
\par \cf0\highlight0 default       59m         Error     Logging            pod/netpod2                     Handler 'builtins_on_pod' failed temporarily: Temporary Error: Pod spec not ready.\cf1\highlight2 
\par \cf0\highlight0 default       59m         Error     Logging            pod/netpod1                     Handler 'builtins_on_pod' failed temporarily: Temporary Error: Pod spec not ready.\cf1\highlight2 
\par \cf0\highlight0 default       58m         Error     Logging            pod/netpod2                     Handler 'builtins_on_pod' failed temporarily: Temporary Error: Pod spec not ready.\cf1\highlight2 
\par \cf0\highlight0 default       60m         Error     Logging            pod/netpod2                     Handler 'builtins_on_pod' failed temporarily: Temporary Error: Pod spec not ready.\cf1\highlight2 
\par \cf0\highlight0 default       60m         Error     Logging            pod/netpod1                     Handler 'builtins_on_pod' failed temporarily: Temporary Error: Pod spec not ready.\cf1\highlight2 
\par \cf0\highlight0 default       57m         Normal    Logging            pod/netpod2                     Creation is processed: 0 succeeded; 1 failed.\cf1\highlight2 
\par \cf0\highlight0 default       60m         Error     Logging            pod/netpod1                     Handler 'builtins_on_pod' failed temporarily: Temporary Error: Pod spec not ready.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod1                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod1                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod1                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod1                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod1                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod1                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod1                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod1                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod1                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod1                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod1                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod1                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod1                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod1                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod1                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod1                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod1                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod1                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod1                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod1                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod1                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod2                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod2                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod2                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod2                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod2                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod2                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod2                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod2                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod2                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod2                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod2                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod2                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod2                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod2                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod2                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod2                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod2                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod2                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod2                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod2                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 default       <unknown>   Warning   FailedScheduling   pod/netpod2                     0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 kube-system   <unknown>   Warning   FailedScheduling   pod/kube-dns-554c5866fc-2m9zz   0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 kube-system   <unknown>   Warning   FailedScheduling   pod/kube-dns-554c5866fc-2m9zz   0/1 nodes are available: 1 node(s) had taint \{node.kubernetes.io/not-ready: \}, that the pod didn't tolerate.\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get vpcs\cf1\highlight2 
\par \cf0\highlight0 NAME   IP         PREFIX   VNI   DIVIDERS   STATUS   CREATETIME                   PROVISIONDELAY\cf1\highlight2 
\par \cf0\highlight0 vpc0   20.0.0.0   8        1     1          Init     2021-12-07T05:49:08.897243\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -A\cf1\highlight2 
\par \cf0\highlight0 No resources found.\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS              RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-c5jrs                5433553103926506877   0/1     Init:0/1            0          26s\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-tkx5m   4252175553422586693   1/1     Running             0          26s\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-dgwpx         1606520876123553529   0/3     ContainerCreating   0          26s\cf1\highlight2 
\par \cf0\highlight0 kube-system   virtlet-g97z2                     2296479715811486546   0/3     Init:0/1            0          26s\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS     RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-c5jrs                5433553103926506877   1/1     Running    0          2m\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-tkx5m   4252175553422586693   1/1     Running    0          2m\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-dgwpx         1606520876123553529   3/3     Running    0          2m\cf1\highlight2 
\par \cf0\highlight0 kube-system   virtlet-g97z2                     2296479715811486546   0/3     Init:0/1   0          2m\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS     RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-c5jrs                5433553103926506877   1/1     Running    0          2m8s\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-tkx5m   4252175553422586693   1/1     Running    0          2m8s\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-dgwpx         1606520876123553529   3/3     Running    0          2m8s\cf1\highlight2 
\par \cf0\highlight0 kube-system   virtlet-g97z2                     2296479715811486546   0/3     Init:0/1   0          2m8s\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS     RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-c5jrs                5433553103926506877   1/1     Running    0          2m10s\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-tkx5m   4252175553422586693   1/1     Running    0          2m10s\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-dgwpx         1606520876123553529   3/3     Running    0          2m10s\cf1\highlight2 
\par \cf0\highlight0 kube-system   virtlet-g97z2                     2296479715811486546   0/3     Init:0/1   0          2m10s\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS     RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-c5jrs                5433553103926506877   1/1     Running    0          2m11s\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-tkx5m   4252175553422586693   1/1     Running    0          2m11s\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-dgwpx         1606520876123553529   3/3     Running    0          2m11s\cf1\highlight2 
\par \cf0\highlight0 kube-system   virtlet-g97z2                     2296479715811486546   0/3     Init:0/1   0          2m11s\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS     RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-c5jrs                5433553103926506877   1/1     Running    0          2m12s\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-tkx5m   4252175553422586693   1/1     Running    0          2m12s\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-dgwpx         1606520876123553529   3/3     Running    0          2m12s\cf1\highlight2 
\par \cf0\highlight0 kube-system   virtlet-g97z2                     2296479715811486546   0/3     Init:0/1   0          2m12s\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get vpcs\cf1\highlight2 
\par \cf0\highlight0 NAME   IP         PREFIX   VNI   DIVIDERS   STATUS        CREATETIME                   PROVISIONDELAY\cf1\highlight2 
\par \cf0\highlight0 vpc0   20.0.0.0   8        1     1          Provisioned   2021-12-07T08:46:54.119013   41.164764\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -A\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS     RESTARTS   AGE\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-c5jrs                5433553103926506877   1/1     Running    0          2m30s\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-tkx5m   4252175553422586693   1/1     Running    0          2m30s\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-dgwpx         1606520876123553529   3/3     Running    0          2m30s\cf1\highlight2 
\par \cf0\highlight0 kube-system   virtlet-g97z2                     2296479715811486546   0/3     Init:0/1   0          2m30s\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS     RESTARTS   AGE     IP             NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-c5jrs                5433553103926506877   1/1     Running    0          4m47s   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-tkx5m   4252175553422586693   1/1     Running    0          4m47s   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-dgwpx         1606520876123553529   3/3     Running    0          4m47s   20.0.0.2       arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   virtlet-g97z2                     2296479715811486546   0/3     Init:0/1   0          4m47s   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get nodes -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAME     STATUS   ROLES    AGE     VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION   CONTAINER-RUNTIME\cf1\highlight2 
\par \cf0\highlight0 arktos   Ready    <none>   6m51s   v0.9.0    172.31.19.33   <none>        Ubuntu 18.04.6 LTS   5.6.0-rc2        containerd://1.4.0-beta.1-29-g70b0d3cf\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get nodes -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAME     STATUS   ROLES    AGE     VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION   CONTAINER-RUNTIME\cf1\highlight2 
\par \cf0\highlight0 arktos   Ready    <none>   8m24s   v0.9.0    172.31.19.33   <none>        Ubuntu 18.04.6 LTS   5.6.0-rc2        containerd://1.4.0-beta.1-29-g70b0d3cf\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get vpc -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE   NAME   IP         PREFIX   VNI   DIVIDERS   STATUS        CREATETIME                   PROVISIONDELAY\cf1\highlight2 
\par \cf0\highlight0 default     vpc0   20.0.0.0   8        1     1          Provisioned   2021-12-07T08:46:54.119013   41.164764\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get net -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAME      TYPE    VPC                      PHASE   DNS\cf1\highlight2 
\par \cf0\highlight0 default   mizar   system-default-network\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get dividers -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE   NAME                                          VPC    IP             MAC                 DROPLET   STATUS        CREATETIME                   PROVISIONDELAY\cf1\highlight2 
\par \cf0\highlight0 default     vpc0-d-c2c16f11-1758-44fe-8dd1-bac7096437ad   vpc0   172.31.19.33   06:c5:e6:0f:54:9e   arktos    Provisioned   2021-12-07T08:47:35.278403   0.240979\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get bouncers -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE   NAME                                          VPC    NET    IP             MAC                 DROPLET   STATUS        CREATETIME                   PROVISIONDELAY\cf1\highlight2 
\par \cf0\highlight0 default     net0-b-ab61a092-5b45-4f9f-82e7-69e23bc0135e   vpc0   net0   172.31.19.33   06:c5:e6:0f:54:9e   arktos    Provisioned   2021-12-07T08:47:55.141759   1.05588\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS     RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-c5jrs                5433553103926506877   1/1     Running    0          15m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-tkx5m   4252175553422586693   1/1     Running    0          15m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-dgwpx         1606520876123553529   3/3     Running    0          15m   20.0.0.2       arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   virtlet-g97z2                     2296479715811486546   0/3     Init:0/1   0          15m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh describe pod virtlet-g97z2 -n kube-system\cf1\highlight2 
\par \cf0\highlight0 Name:           virtlet-g97z2\cf1\highlight2 
\par \cf0\highlight0 Namespace:      kube-system\cf1\highlight2 
\par \cf0\highlight0 Tenant:         system\cf1\highlight2 
\par \cf0\highlight0 Priority:       0\cf1\highlight2 
\par \cf0\highlight0 Node:           arktos/172.31.19.33\cf1\highlight2 
\par \cf0\highlight0 Start Time:     Tue, 07 Dec 2021 08:46:51 +0000\cf1\highlight2 
\par \cf0\highlight0 Labels:         controller-revision-hash=5899cf49f7\cf1\highlight2 
\par \cf0\highlight0                 pod-template-generation=1\cf1\highlight2 
\par \cf0\highlight0                 runtime=virtlet\cf1\highlight2 
\par \cf0\highlight0 Annotations:    apparmorlibvirtname: apparmorlibvirtvalue\cf1\highlight2 
\par \cf0\highlight0                 apparmorvirtletname: apparmorvirtletvalue\cf1\highlight2 
\par \cf0\highlight0                 apparmorvmsname: apparmorvmsvalue\cf1\highlight2 
\par \cf0\highlight0                 kopf.zalando.org/last-handled-configuration:\cf1\highlight2 
\par \cf0\highlight0                   \{"spec":\{"volumes":[\{"name":"dev","hostPath":\{"path":"/dev","type":""\}\},\{"name":"cgroup","hostPath":\{"path":"/sys/fs/cgroup","type":""\}\},\{...\cf1\highlight2 
\par \cf0\highlight0 Status:         Pending\cf1\highlight2 
\par \cf0\highlight0 IP:             172.31.19.33\cf1\highlight2 
\par \cf0\highlight0 Controlled By:  DaemonSet/virtlet\cf1\highlight2 
\par \cf0\highlight0 Init Containers:\cf1\highlight2 
\par \cf0\highlight0   prepare-node:\cf1\highlight2 
\par \cf0\highlight0     Container ID:\cf1\highlight2 
\par \cf0\highlight0     Image:         arktosstaging/vmruntime:0.5.4\cf1\highlight2 
\par \cf0\highlight0     Image ID:\cf1\highlight2 
\par \cf0\highlight0     Port:          <none>\cf1\highlight2 
\par \cf0\highlight0     Host Port:     <none>\cf1\highlight2 
\par \cf0\highlight0     Command:\cf1\highlight2 
\par \cf0\highlight0       /prepare-node.sh\cf1\highlight2 
\par \cf0\highlight0     State:          Waiting\cf1\highlight2 
\par \cf0\highlight0       Reason:       PodInitializing\cf1\highlight2 
\par \cf0\highlight0     Ready:          False\cf1\highlight2 
\par \cf0\highlight0     Restart Count:  0\cf1\highlight2 
\par \cf0\highlight0     Environment:\cf1\highlight2 
\par \cf0\highlight0       KUBE_NODE_NAME:                   (v1:spec.nodeName)\cf1\highlight2 
\par \cf0\highlight0       VIRTLET_DISABLE_KVM:             <set to the key 'disable_kvm' of config map 'virtlet-config'>               Optional: true\cf1\highlight2 
\par \cf0\highlight0       VIRTLET_SRIOV_SUPPORT:           <set to the key 'sriov_support' of config map 'virtlet-config'>             Optional: true\cf1\highlight2 
\par \cf0\highlight0       VIRTLET_DOWNLOAD_PROTOCOL:       <set to the key 'download_protocol' of config map 'virtlet-config'>         Optional: true\cf1\highlight2 
\par \cf0\highlight0       VIRTLET_LOGLEVEL:                <set to the key 'loglevel' of config map 'virtlet-config'>                  Optional: true\cf1\highlight2 
\par \cf0\highlight0       VIRTLET_CALICO_SUBNET:           <set to the key 'calico-subnet' of config map 'virtlet-config'>             Optional: true\cf1\highlight2 
\par \cf0\highlight0       IMAGE_REGEXP_TRANSLATION:        <set to the key 'image_regexp_translation' of config map 'virtlet-config'>  Optional: true\cf1\highlight2 
\par \cf0\highlight0       VIRTLET_RAW_DEVICES:             <set to the key 'raw_devices' of config map 'virtlet-config'>               Optional: true\cf1\highlight2 
\par \cf0\highlight0       VIRTLET_DISABLE_LOGGING:         <set to the key 'disable_logging' of config map 'virtlet-config'>           Optional: true\cf1\highlight2 
\par \cf0\highlight0       VIRTLET_CPU_MODEL:               <set to the key 'cpu-model' of config map 'virtlet-config'>                 Optional: true\cf1\highlight2 
\par \cf0\highlight0       KUBELET_ROOT_DIR:                <set to the key 'kubelet_root_dir' of config map 'virtlet-config'>          Optional: true\cf1\highlight2 
\par \cf0\highlight0       VIRTLET_IMAGE_TRANSLATIONS_DIR:  /etc/virtlet/images\cf1\highlight2 
\par \cf0\highlight0     Mounts:\cf1\highlight2 
\par \cf0\highlight0       /dev from dev (rw)\cf1\highlight2 
\par \cf0\highlight0       /host-var-lib from var-lib (rw)\cf1\highlight2 
\par \cf0\highlight0       /hostlog from log (rw)\cf1\highlight2 
\par \cf0\highlight0       /kubelet-volume-plugins from k8s-flexvolume-plugins-dir (rw)\cf1\highlight2 
\par \cf0\highlight0       /run from run (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/lib/virtlet from virtlet (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/run/docker.sock from dockersock (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/run/secrets/kubernetes.io/serviceaccount from virtlet-token-8nrdd (ro)\cf1\highlight2 
\par \cf0\highlight0 Containers:\cf1\highlight2 
\par \cf0\highlight0   libvirt:\cf1\highlight2 
\par \cf0\highlight0     Container ID:\cf1\highlight2 
\par \cf0\highlight0     Image:         arktosstaging/vmruntime:0.5.4\cf1\highlight2 
\par \cf0\highlight0     Image ID:\cf1\highlight2 
\par \cf0\highlight0     Port:          <none>\cf1\highlight2 
\par \cf0\highlight0     Host Port:     <none>\cf1\highlight2 
\par \cf0\highlight0     Command:\cf1\highlight2 
\par \cf0\highlight0       /libvirt.sh\cf1\highlight2 
\par \cf0\highlight0     State:          Waiting\cf1\highlight2 
\par \cf0\highlight0       Reason:       PodInitializing\cf1\highlight2 
\par \cf0\highlight0     Ready:          False\cf1\highlight2 
\par \cf0\highlight0     Restart Count:  0\cf1\highlight2 
\par \cf0\highlight0     Readiness:      exec [/bin/sh -c socat - UNIX:/var/run/libvirt/libvirt-sock-ro </dev/null] delay=0s timeout=1s period=10s #success=1 #failure=3\cf1\highlight2 
\par \cf0\highlight0     Environment:    <none>\cf1\highlight2 
\par \cf0\highlight0     Mounts:\cf1\highlight2 
\par \cf0\highlight0       /boot from boot (ro)\cf1\highlight2 
\par \cf0\highlight0       /dev from dev (rw)\cf1\highlight2 
\par \cf0\highlight0       /etc/libvirt/qemu from qemu (rw)\cf1\highlight2 
\par \cf0\highlight0       /lib/modules from modules (ro)\cf1\highlight2 
\par \cf0\highlight0       /run from run (rw)\cf1\highlight2 
\par \cf0\highlight0       /sys/fs/cgroup from cgroup (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/lib/libvirt from libvirt (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/lib/virtlet from virtlet (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/log/libvirt from libvirt-log (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/log/vms from vms-log (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/run/libvirt from libvirt-sockets (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/run/secrets/kubernetes.io/serviceaccount from virtlet-token-8nrdd (ro)\cf1\highlight2 
\par \cf0\highlight0   virtlet:\cf1\highlight2 
\par \cf0\highlight0     Container ID:\cf1\highlight2 
\par \cf0\highlight0     Image:          arktosstaging/vmruntime:0.5.4\cf1\highlight2 
\par \cf0\highlight0     Image ID:\cf1\highlight2 
\par \cf0\highlight0     Port:           <none>\cf1\highlight2 
\par \cf0\highlight0     Host Port:      <none>\cf1\highlight2 
\par \cf0\highlight0     State:          Waiting\cf1\highlight2 
\par \cf0\highlight0       Reason:       PodInitializing\cf1\highlight2 
\par \cf0\highlight0     Ready:          False\cf1\highlight2 
\par \cf0\highlight0     Restart Count:  0\cf1\highlight2 
\par \cf0\highlight0     Readiness:      exec [/bin/sh -c socat - UNIX:/run/virtlet.sock </dev/null] delay=0s timeout=1s period=10s #success=1 #failure=3\cf1\highlight2 
\par \cf0\highlight0     Environment:    <none>\cf1\highlight2 
\par \cf0\highlight0     Mounts:\cf1\highlight2 
\par \cf0\highlight0       /boot from boot (ro)\cf1\highlight2 
\par \cf0\highlight0       /dev from dev (rw)\cf1\highlight2 
\par \cf0\highlight0       /etc/libvirt/qemu from qemu (rw)\cf1\highlight2 
\par \cf0\highlight0       /lib/modules from modules (ro)\cf1\highlight2 
\par \cf0\highlight0       /run from run (rw)\cf1\highlight2 
\par \cf0\highlight0       /sys/fs/cgroup from cgroup (rw)\cf1\highlight2 
\par \cf0\highlight0       /usr/libexec/kubernetes/kubelet-plugins/volume/exec from k8s-flexvolume-plugins-dir (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/lib/kubelet/pods from k8s-pods-dir (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/lib/libvirt from libvirt (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/lib/virtlet from virtlet (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/log/libvirt from libvirt-log (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/log/pods from pods-log (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/log/vms from vms-log (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/run/libvirt from libvirt-sockets (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/run/netns from netns-dir (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/run/secrets/kubernetes.io/serviceaccount from virtlet-token-8nrdd (ro)\cf1\highlight2 
\par \cf0\highlight0   vms:\cf1\highlight2 
\par \cf0\highlight0     Container ID:\cf1\highlight2 
\par \cf0\highlight0     Image:         arktosstaging/vmruntime:0.5.4\cf1\highlight2 
\par \cf0\highlight0     Image ID:\cf1\highlight2 
\par \cf0\highlight0     Port:          <none>\cf1\highlight2 
\par \cf0\highlight0     Host Port:     <none>\cf1\highlight2 
\par \cf0\highlight0     Command:\cf1\highlight2 
\par \cf0\highlight0       /vms.sh\cf1\highlight2 
\par \cf0\highlight0     State:          Waiting\cf1\highlight2 
\par \cf0\highlight0       Reason:       PodInitializing\cf1\highlight2 
\par \cf0\highlight0     Ready:          False\cf1\highlight2 
\par \cf0\highlight0     Restart Count:  0\cf1\highlight2 
\par \cf0\highlight0     Environment:    <none>\cf1\highlight2 
\par \cf0\highlight0     Mounts:\cf1\highlight2 
\par \cf0\highlight0       /dev from dev (rw)\cf1\highlight2 
\par \cf0\highlight0       /lib/modules from modules (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/lib/kubelet/pods from k8s-pods-dir (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/lib/libvirt from libvirt (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/lib/virtlet from virtlet (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/log/vms from vms-log (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/run/secrets/kubernetes.io/serviceaccount from virtlet-token-8nrdd (ro)\cf1\highlight2 
\par \cf0\highlight0 Conditions:\cf1\highlight2 
\par \cf0\highlight0   Type              Status\cf1\highlight2 
\par \cf0\highlight0   Initialized       False\cf1\highlight2 
\par \cf0\highlight0   ContainersReady   False\cf1\highlight2 
\par \cf0\highlight0   Ready             False\cf1\highlight2 
\par \cf0\highlight0   PodScheduled      True\cf1\highlight2 
\par \cf0\highlight0 Volumes:\cf1\highlight2 
\par \cf0\highlight0   dev:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /dev\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   cgroup:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /sys/fs/cgroup\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   modules:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /lib/modules\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   boot:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /boot\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   run:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /run\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   dockersock:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /var/run/docker.sock\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   virtlet:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /var/lib/virtlet\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   libvirt:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /var/lib/libvirt\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   log:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /var/log\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   k8s-flexvolume-plugins-dir:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   k8s-pods-dir:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /var/lib/kubelet/pods\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   var-lib:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /var/lib\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   vms-log:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /var/log/virtlet/vms\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   libvirt-log:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /var/log/libvirt\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   libvirt-sockets:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /var/run/libvirt\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   pods-log:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /var/log/pods\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   netns-dir:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /var/run/netns\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   qemu:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /etc/libvirt/qemu\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   virtlet-token-8nrdd:\cf1\highlight2 
\par \cf0\highlight0     Type:        Secret (a volume populated by a Secret)\cf1\highlight2 
\par \cf0\highlight0     SecretName:  virtlet-token-8nrdd\cf1\highlight2 
\par \cf0\highlight0     Optional:    false\cf1\highlight2 
\par \cf0\highlight0 QoS Class:       BestEffort\cf1\highlight2 
\par \cf0\highlight0 Node-Selectors:  <none>\cf1\highlight2 
\par \cf0\highlight0 Tolerations:     node.kubernetes.io/disk-pressure:NoSchedule\cf1\highlight2 
\par \cf0\highlight0                  node.kubernetes.io/memory-pressure:NoSchedule\cf1\highlight2 
\par \cf0\highlight0                  node.kubernetes.io/network-unavailable:NoSchedule\cf1\highlight2 
\par \cf0\highlight0                  node.kubernetes.io/not-ready:NoExecute\cf1\highlight2 
\par \cf0\highlight0                  node.kubernetes.io/pid-pressure:NoSchedule\cf1\highlight2 
\par \cf0\highlight0                  node.kubernetes.io/unreachable:NoExecute\cf1\highlight2 
\par \cf0\highlight0                  node.kubernetes.io/unschedulable:NoSchedule\cf1\highlight2 
\par \cf0\highlight0 Events:          <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS     RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-c5jrs                5433553103926506877   1/1     Running    0          18m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-tkx5m   4252175553422586693   1/1     Running    0          18m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-dgwpx         1606520876123553529   3/3     Running    0          18m   20.0.0.2       arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   virtlet-g97z2                     2296479715811486546   0/3     Init:0/1   0          18m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh apply -f https://raw.githubusercontent.com/Click2Cloud-Centaurus/Documentation/main/test-yamls/test_pods.yaml\cf1\highlight2 
\par \cf0\highlight0 pod/netpod1 created\cf1\highlight2 
\par \cf0\highlight0 pod/netpod2 created\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS              RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-c5jrs                5433553103926506877   1/1     Running             0          22m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-tkx5m   4252175553422586693   1/1     Running             0          22m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod1                           2754427458370010145   0/1     ContainerCreating   0          5s    <none>         arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod2                           7586843493290022794   0/1     ContainerCreating   0          5s    <none>         arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-dgwpx         1606520876123553529   3/3     Running             0          22m   20.0.0.2       arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   virtlet-g97z2                     2296479715811486546   0/3     Init:0/1            0          22m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS              RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-c5jrs                5433553103926506877   1/1     Running             0          22m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-tkx5m   4252175553422586693   1/1     Running             0          22m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod1                           2754427458370010145   0/1     ContainerCreating   0          9s    <none>         arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod2                           7586843493290022794   0/1     ContainerCreating   0          9s    <none>         arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-dgwpx         1606520876123553529   3/3     Running             0          22m   20.0.0.2       arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   virtlet-g97z2                     2296479715811486546   0/3     Init:0/1            0          22m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS              RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-c5jrs                5433553103926506877   1/1     Running             0          22m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-tkx5m   4252175553422586693   1/1     Running             0          22m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod1                           2754427458370010145   0/1     ContainerCreating   0          11s   <none>         arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod2                           7586843493290022794   0/1     ContainerCreating   0          11s   <none>         arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-dgwpx         1606520876123553529   3/3     Running             0          22m   20.0.0.2       arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   virtlet-g97z2                     2296479715811486546   0/3     Init:0/1            0          22m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS              RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-c5jrs                5433553103926506877   1/1     Running             0          22m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-tkx5m   4252175553422586693   1/1     Running             0          22m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod1                           2754427458370010145   0/1     ContainerCreating   0          13s   <none>         arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod2                           7586843493290022794   0/1     ContainerCreating   0          13s   <none>         arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-dgwpx         1606520876123553529   3/3     Running             0          22m   20.0.0.2       arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   virtlet-g97z2                     2296479715811486546   0/3     Init:0/1            0          22m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS              RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-c5jrs                5433553103926506877   1/1     Running             0          22m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-tkx5m   4252175553422586693   1/1     Running             0          22m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod1                           2754427458370010145   0/1     ContainerCreating   0          16s   <none>         arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod2                           7586843493290022794   0/1     ContainerCreating   0          16s   <none>         arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-dgwpx         1606520876123553529   3/3     Running             0          22m   20.0.0.2       arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   virtlet-g97z2                     2296479715811486546   0/3     Init:0/1            0          22m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ ./cluster/kubectl.sh version\cf1\highlight2 
\par \cf0\highlight0 ./cluster/../cluster/../cluster/gce/../../cluster/gce/../../cluster/common.sh: line 30: /root/go/src/k8s.io/arktos/hack/lib/util.sh: Permission denied\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh --version\cf1\highlight2 
\par \cf0\highlight0 Error: unknown flag: --version\cf1\highlight2 
\par 
\par 
\par \cf0\highlight0 Basic Commands (Beginner):\cf1\highlight2 
\par \cf0\highlight0   create         Create a resource from a file or from stdin.\cf1\highlight2 
\par \cf0\highlight0   expose         Take a replication controller, service, deployment or pod and expose it as a new Kubernetes Service\cf1\highlight2 
\par \cf0\highlight0   run            Run a particular image on the cluster\cf1\highlight2 
\par \cf0\highlight0   set            Set specific features on objects\cf1\highlight2 
\par 
\par \cf0\highlight0 Basic Commands (Intermediate):\cf1\highlight2 
\par \cf0\highlight0   explain        Documentation of resources\cf1\highlight2 
\par \cf0\highlight0   get            Display one or many resources\cf1\highlight2 
\par \cf0\highlight0   edit           Edit a resource on the server\cf1\highlight2 
\par \cf0\highlight0   delete         Delete resources by filenames, stdin, resources and names, or by resources and label selector\cf1\highlight2 
\par 
\par \cf0\highlight0 Deploy Commands:\cf1\highlight2 
\par \cf0\highlight0   rollout        Manage the rollout of a resource\cf1\highlight2 
\par \cf0\highlight0   scale          Set a new size for a Deployment, ReplicaSet, Replication Controller, or Job\cf1\highlight2 
\par \cf0\highlight0   autoscale      Auto-scale a Deployment, ReplicaSet, or ReplicationController\cf1\highlight2 
\par 
\par \cf0\highlight0 Cluster Management Commands:\cf1\highlight2 
\par \cf0\highlight0   certificate    Modify certificate resources.\cf1\highlight2 
\par \cf0\highlight0   cluster-info   Display cluster info\cf1\highlight2 
\par \cf0\highlight0   top            Display Resource (CPU/Memory/Storage) usage.\cf1\highlight2 
\par \cf0\highlight0   cordon         Mark node as unschedulable\cf1\highlight2 
\par \cf0\highlight0   uncordon       Mark node as schedulable\cf1\highlight2 
\par \cf0\highlight0   drain          Drain node in preparation for maintenance\cf1\highlight2 
\par \cf0\highlight0   taint          Update the taints on one or more nodes\cf1\highlight2 
\par 
\par \cf0\highlight0 Troubleshooting and Debugging Commands:\cf1\highlight2 
\par \cf0\highlight0   describe       Show details of a specific resource or group of resources\cf1\highlight2 
\par \cf0\highlight0   logs           Print the logs for a container in a pod\cf1\highlight2 
\par \cf0\highlight0   attach         Attach to a running container\cf1\highlight2 
\par \cf0\highlight0   exec           Execute a command in a container\cf1\highlight2 
\par \cf0\highlight0   port-forward   Forward one or more local ports to a pod\cf1\highlight2 
\par \cf0\highlight0   proxy          Run a proxy to the Kubernetes API server\cf1\highlight2 
\par \cf0\highlight0   cp             Copy files and directories to and from containers.\cf1\highlight2 
\par \cf0\highlight0   auth           Inspect authorization\cf1\highlight2 
\par 
\par \cf0\highlight0 Advanced Commands:\cf1\highlight2 
\par \cf0\highlight0   diff           Diff live version against would-be applied version\cf1\highlight2 
\par \cf0\highlight0   apply          Apply a configuration to a resource by filename or stdin\cf1\highlight2 
\par \cf0\highlight0   patch          Update field(s) of a resource using strategic merge patch\cf1\highlight2 
\par \cf0\highlight0   replace        Replace a resource by filename or stdin\cf1\highlight2 
\par \cf0\highlight0   wait           Experimental: Wait for a specific condition on one or many resources.\cf1\highlight2 
\par \cf0\highlight0   convert        Convert config files between different API versions\cf1\highlight2 
\par \cf0\highlight0   kustomize      Build a kustomization target from a directory or a remote url.\cf1\highlight2 
\par 
\par \cf0\highlight0 Settings Commands:\cf1\highlight2 
\par \cf0\highlight0   label          Update the labels on a resource\cf1\highlight2 
\par \cf0\highlight0   annotate       Update the annotations on a resource\cf1\highlight2 
\par \cf0\highlight0   completion     Output shell completion code for the specified shell (bash or zsh)\cf1\highlight2 
\par 
\par \cf0\highlight0 Other Commands:\cf1\highlight2 
\par \cf0\highlight0   api-resources  Print the supported API resources on the server\cf1\highlight2 
\par \cf0\highlight0   api-versions   Print the supported API versions on the server, in the form of "group/version"\cf1\highlight2 
\par \cf0\highlight0   config         Modify kubeconfig files\cf1\highlight2 
\par \cf0\highlight0   plugin         Provides utilities for interacting with plugins.\cf1\highlight2 
\par \cf0\highlight0   version        Print the client and server version information\cf1\highlight2 
\par 
\par \cf0\highlight0 Usage:\cf1\highlight2 
\par \cf0\highlight0   kubectl [flags] [options]\cf1\highlight2 
\par 
\par \cf0\highlight0 Use "kubectl <command> --help" for more information about a given command.\cf1\highlight2 
\par \cf0\highlight0 Use "kubectl options" for a list of global command-line options (applies to all commands).\cf1\highlight2 
\par 
\par \cf0\highlight0 unknown flag: --version\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh version\cf1\highlight2 
\par \cf0\highlight0 Client Version: version.Info\{Major:"", Minor:"", GitVersion:"v0.9.0", GitCommit:"$Format:%H$", GitTreeState:"", BuildDate:"2021-12-07T05:45:43Z", GoVersion:"go1.13.9", Compiler:"gc", Platform:"linux/amd64"\}\cf1\highlight2 
\par \cf0\highlight0 Server Version: version.Info\{Major:"", Minor:"", GitVersion:"v0.9.0", GitCommit:"$Format:%H$", GitTreeState:"", BuildDate:"2021-12-07T05:45:43Z", GoVersion:"go1.13.9", Compiler:"gc", Platform:"linux/amd64"\}\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS              RESTARTS   AGE    IP             NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-c5jrs                5433553103926506877   1/1     Running             0          24m    172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-tkx5m   4252175553422586693   1/1     Running             0          24m    172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod1                           2754427458370010145   0/1     ContainerCreating   0          112s   <none>         arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod2                           7586843493290022794   0/1     ContainerCreating   0          112s   <none>         arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-dgwpx         1606520876123553529   3/3     Running             0          24m    20.0.0.2       arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   virtlet-g97z2                     2296479715811486546   0/3     Init:0/1            0          24m    172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS              RESTARTS   AGE    IP             NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-c5jrs                5433553103926506877   1/1     Running             0          24m    172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-tkx5m   4252175553422586693   1/1     Running             0          24m    172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod1                           2754427458370010145   0/1     ContainerCreating   0          119s   <none>         arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod2                           7586843493290022794   0/1     ContainerCreating   0          119s   <none>         arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-dgwpx         1606520876123553529   3/3     Running             0          24m    20.0.0.2       arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   virtlet-g97z2                     2296479715811486546   0/3     Init:0/1            0          24m    172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh describe pod netpod1 -n default\cf1\highlight2 
\par \cf0\highlight0 Name:         netpod1\cf1\highlight2 
\par \cf0\highlight0 Namespace:    default\cf1\highlight2 
\par \cf0\highlight0 Tenant:       system\cf1\highlight2 
\par \cf0\highlight0 Priority:     0\cf1\highlight2 
\par \cf0\highlight0 Node:         arktos/172.31.19.33\cf1\highlight2 
\par \cf0\highlight0 Start Time:   Tue, 07 Dec 2021 09:09:26 +0000\cf1\highlight2 
\par \cf0\highlight0 Labels:       podkey=netpodkey1\cf1\highlight2 
\par \cf0\highlight0 Annotations:  kopf.zalando.org/last-handled-configuration:\cf1\highlight2 
\par \cf0\highlight0                 \{"spec":\{"volumes":[\{"name":"default-token-cx9sf","secret":\{"secretName":"default-token-cx9sf","defaultMode":420\}\}],"containers":[\{"name":...\cf1\highlight2 
\par \cf0\highlight0               kubectl.kubernetes.io/last-applied-configuration:\cf1\highlight2 
\par \cf0\highlight0                 \{"apiVersion":"v1","kind":"Pod","metadata":\{"annotations":\{\},"labels":\{"podkey":"netpodkey1"\},"name":"netpod1","namespace":"default","tena...\cf1\highlight2 
\par \cf0\highlight0 Status:       Pending\cf1\highlight2 
\par \cf0\highlight0 IP:\cf1\highlight2 
\par \cf0\highlight0 Containers:\cf1\highlight2 
\par \cf0\highlight0   netctr:\cf1\highlight2 
\par \cf0\highlight0     Container ID:\cf1\highlight2 
\par \cf0\highlight0     Image:         yuvalif/fedora-tcpdump\cf1\highlight2 
\par \cf0\highlight0     Image ID:\cf1\highlight2 
\par \cf0\highlight0     Port:          <none>\cf1\highlight2 
\par \cf0\highlight0     Host Port:     <none>\cf1\highlight2 
\par \cf0\highlight0     Command:\cf1\highlight2 
\par \cf0\highlight0       tail\cf1\highlight2 
\par \cf0\highlight0       -f\cf1\highlight2 
\par \cf0\highlight0       /dev/null\cf1\highlight2 
\par \cf0\highlight0     State:          Waiting\cf1\highlight2 
\par \cf0\highlight0       Reason:       ContainerCreating\cf1\highlight2 
\par \cf0\highlight0     Ready:          False\cf1\highlight2 
\par \cf0\highlight0     Restart Count:  0\cf1\highlight2 
\par \cf0\highlight0     Environment:    <none>\cf1\highlight2 
\par \cf0\highlight0     Mounts:\cf1\highlight2 
\par \cf0\highlight0       /var/run/secrets/kubernetes.io/serviceaccount from default-token-cx9sf (ro)\cf1\highlight2 
\par \cf0\highlight0 Conditions:\cf1\highlight2 
\par \cf0\highlight0   Type              Status\cf1\highlight2 
\par \cf0\highlight0   Initialized       True\cf1\highlight2 
\par \cf0\highlight0   ContainersReady   False\cf1\highlight2 
\par \cf0\highlight0   Ready             False\cf1\highlight2 
\par \cf0\highlight0   PodScheduled      True\cf1\highlight2 
\par \cf0\highlight0 Volumes:\cf1\highlight2 
\par \cf0\highlight0   default-token-cx9sf:\cf1\highlight2 
\par \cf0\highlight0     Type:        Secret (a volume populated by a Secret)\cf1\highlight2 
\par \cf0\highlight0     SecretName:  default-token-cx9sf\cf1\highlight2 
\par \cf0\highlight0     Optional:    false\cf1\highlight2 
\par \cf0\highlight0 QoS Class:       BestEffort\cf1\highlight2 
\par \cf0\highlight0 Node-Selectors:  <none>\cf1\highlight2 
\par \cf0\highlight0 Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\cf1\highlight2 
\par \cf0\highlight0                  node.kubernetes.io/unreachable:NoExecute for 300s\cf1\highlight2 
\par \cf0\highlight0 Events:          <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS              RESTARTS   AGE     IP             NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-c5jrs                5433553103926506877   1/1     Running             0          25m     172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-tkx5m   4252175553422586693   1/1     Running             0          25m     172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod1                           2754427458370010145   0/1     ContainerCreating   0          2m34s   <none>         arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod2                           7586843493290022794   0/1     ContainerCreating   0          2m34s   <none>         arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-dgwpx         1606520876123553529   3/3     Running             0          25m     20.0.0.2       arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   virtlet-g97z2                     2296479715811486546   0/3     Init:0/1            0          25m     172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS              RESTARTS   AGE     IP             NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-c5jrs                5433553103926506877   1/1     Running             0          30m     172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-tkx5m   4252175553422586693   1/1     Running             0          30m     172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod1                           2754427458370010145   0/1     ContainerCreating   0          7m47s   <none>         arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod2                           7586843493290022794   0/1     ContainerCreating   0          7m47s   <none>         arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-dgwpx         1606520876123553529   3/3     Running             0          30m     20.0.0.2       arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   virtlet-g97z2                     2296479715811486546   0/3     Init:0/1            0          30m     172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get vpc -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE   NAME   IP         PREFIX   VNI   DIVIDERS   STATUS        CREATETIME                   PROVISIONDELAY\cf1\highlight2 
\par \cf0\highlight0 default     vpc0   20.0.0.0   8        1     1          Provisioned   2021-12-07T08:46:54.119013   41.164764\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh getnet -Ao wide\cf1\highlight2 
\par \cf0\highlight0 Error: unknown command "getnet" for "kubectl"\cf1\highlight2 
\par \cf0\highlight0 Run 'kubectl --help' for usage.\cf1\highlight2 
\par \cf0\highlight0 unknown command "getnet" for "kubectl"\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get net -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAME      TYPE    VPC                      PHASE   DNS\cf1\highlight2 
\par \cf0\highlight0 default   mizar   system-default-network\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get subnets -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE   NAME   IP         PREFIX   VNI   VPC    STATUS        BOUNCERS   CREATETIME                   PROVISIONDELAY\cf1\highlight2 
\par \cf0\highlight0 default     net0   20.0.0.0   8        1     vpc0   Provisioned   1          2021-12-07T08:46:54.184949   60.962637\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ history\cf1\highlight2 
\par \cf0\highlight0     1  vi /etc/hsotename\cf1\highlight2 
\par \cf0\highlight0     2  vi /etc/hostname\cf1\highlight2 
\par \cf0\highlight0     3  chmod 777 /etc/hostname\cf1\highlight2 
\par \cf0\highlight0     4  sudo -i\cf1\highlight2 
\par \cf0\highlight0     5  sudo ./cluster/kubectl.sh get nodes -Ao wide\cf1\highlight2 
\par \cf0\highlight0     6  sudo /cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0     7  sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0     8  sudo ./cluster/kubectl.sh describe pod kube-dns-554c5866fc-2m9zz\cf1\highlight2 
\par \cf0\highlight0     9  sudo ./cluster/kubectl.sh describe pod kube-dns-554c5866fc-2m9zz -n kube-system\cf1\highlight2 
\par \cf0\highlight0    10  sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0    11  sudo ./cluster/kubectl.sh get vpc -Ao wide\cf1\highlight2 
\par \cf0\highlight0    12  sudo ./cluster/kubectl.sh get subnets -Ao wide\cf1\highlight2 
\par \cf0\highlight0    13  sudo ./cluster/kubectl.sh get net -Ao wide\cf1\highlight2 
\par \cf0\highlight0    14  sudo ./cluster/kubectl.sh get dividers -Ao wide\cf1\highlight2 
\par \cf0\highlight0    15  sudo ./cluster/kubectl.sh get divider -Ao wide\cf1\highlight2 
\par \cf0\highlight0    16  sudo ./cluster/kubectl.sh get dividers -Ao wide\cf1\highlight2 
\par \cf0\highlight0    17  sudo ./cluster/kubectl.sh get bouncers -Ao wide\cf1\highlight2 
\par \cf0\highlight0    18  sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0    19  sudo ./cluster/kubectl.sh get net -Ao wide\cf1\highlight2 
\par \cf0\highlight0    20  sudo ./cluster/kubectl.sh get net -A wide\cf1\highlight2 
\par \cf0\highlight0    21  sudo ./cluster/kubectl.sh get net -Ao wide\cf1\highlight2 
\par \cf0\highlight0    22  sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0    23  ./cluster/kubectl.sh apply -f https://raw.githubusercontent.com/Click2CloudCentaurus/Documentation/main/test-yamls/test_pods.yaml\cf1\highlight2 
\par \cf0\highlight0    24  sudo ./cluster/kubectl.sh apply -f https://raw.githubusercontent.com/Click2CloudCentaurus/Documentation/main/test-yamls/test_pods.yaml\cf1\highlight2 
\par \cf0\highlight0    25  sudo ./cluster/kubectl.sh apply -f https://raw.githubusercontent.com/Click2Cloud-Centaurus/Documentation/main/test-yamls/test_pods.yaml\cf1\highlight2 
\par \cf0\highlight0    26  sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0    27  sudo ./cluster/kubectl.sh describe pod netpod1 -n default\cf1\highlight2 
\par \cf0\highlight0    28  sudo ./cluster/kubectl.sh get pods\cf1\highlight2 
\par \cf0\highlight0    29  sudo ./cluster/kubectl.sh get pods -A\cf1\highlight2 
\par \cf0\highlight0    30  sudo ./cluster/kubectl.sh get nodes\cf1\highlight2 
\par \cf0\highlight0    31  sudo ./cluster/kubectl.sh get pod -A\cf1\highlight2 
\par \cf0\highlight0    32  sudo ./cluster/kubectl.sh describe pod netpod1 -n default\cf1\highlight2 
\par \cf0\highlight0    33  sudo ./cluster/kubectl.sh pods -Ao\cf1\highlight2 
\par \cf0\highlight0    34  history\cf1\highlight2 
\par \cf0\highlight0    35  sudo ./cluster/kubectl.sh apply -f https://raw.githubusercontent.com/Click2Cloud-Centaurus/Documentation/main/test-yamls/test_pods.yaml\cf1\highlight2 
\par \cf0\highlight0    36  sudo ./cluster/kubectl.sh delete -f https://raw.githubusercontent.com/Click2Cloud-Centaurus/Documentation/main/test-yamls/test_pods.yaml\cf1\highlight2 
\par \cf0\highlight0    37  vi pod.yaml\cf1\highlight2 
\par \cf0\highlight0    38  sudo vi pod.yaml\cf1\highlight2 
\par \cf0\highlight0    39  sudo ./cluster/kubectl.sh apply -f pod.yaml\cf1\highlight2 
\par \cf0\highlight0    40  sudo ./cluster/kubectl.sh pods -A\cf1\highlight2 
\par \cf0\highlight0    41  sudo ./cluster/kubectl.sh pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0    42  sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0    43  sudo ./cluster/kubectl.sh describe pod netpod1 -n default\cf1\highlight2 
\par \cf0\highlight0    44  sudo ./cluster/kubectl.sh get pods -A wide\cf1\highlight2 
\par \cf0\highlight0    45*\cf1\highlight2 
\par \cf0\highlight0    46  sudo ./cluster/kubectl.sh describe pod kube-dns-554c5866fc-2m9zz -n kube-system\cf1\highlight2 
\par \cf0\highlight0    47  sudo ./cluster/kubectl.sh get pods -A\cf1\highlight2 
\par \cf0\highlight0    48  sudo ./cluster/kubectl.sh describe pod netpod1 -n deafualt\cf1\highlight2 
\par \cf0\highlight0    49  sudo ./cluster/kubectl.sh describe pod netpod1 -n default\cf1\highlight2 
\par \cf0\highlight0    50  ./cluster/kubectl.sh get pods -A\cf1\highlight2 
\par \cf0\highlight0    51  sudo ./cluster/kubectl.sh get pods -A\cf1\highlight2 
\par \cf0\highlight0    52  sudo ./cluster/kubectl.sh get node -A\cf1\highlight2 
\par \cf0\highlight0    53  sudo ./cluster/kubectl.sh get events -A\cf1\highlight2 
\par \cf0\highlight0    54  sudo ./cluster/kubectl.sh get vpcs\cf1\highlight2 
\par \cf0\highlight0    55  sudo ./cluster/kubectl.sh get pods -A\cf1\highlight2 
\par \cf0\highlight0    56  sudo ./cluster/kubectl.sh get vpcs\cf1\highlight2 
\par \cf0\highlight0    57  sudo ./cluster/kubectl.sh get pods -A\cf1\highlight2 
\par \cf0\highlight0    58  sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0    59  sudo ./cluster/kubectl.sh get nodes -Ao wide\cf1\highlight2 
\par \cf0\highlight0    60  sudo ./cluster/kubectl.sh get vpc -Ao wide\cf1\highlight2 
\par \cf0\highlight0    61  sudo ./cluster/kubectl.sh get net -Ao wide\cf1\highlight2 
\par \cf0\highlight0    62  sudo ./cluster/kubectl.sh get dividers -Ao wide\cf1\highlight2 
\par \cf0\highlight0    63  sudo ./cluster/kubectl.sh get bouncers -Ao wide\cf1\highlight2 
\par \cf0\highlight0    64  sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0    65  sudo ./cluster/kubectl.sh describe pod virtlet-g97z2 -n kube-system\cf1\highlight2 
\par \cf0\highlight0    66  sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0    67  sudo ./cluster/kubectl.sh apply -f https://raw.githubusercontent.com/Click2Cloud-Centaurus/Documentation/main/test-yamls/test_pods.yaml\cf1\highlight2 
\par \cf0\highlight0    68  sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0    69  ./cluster/kubectl.sh version\cf1\highlight2 
\par \cf0\highlight0    70  sudo ./cluster/kubectl.sh --version\cf1\highlight2 
\par \cf0\highlight0    71  sudo ./cluster/kubectl.sh version\cf1\highlight2 
\par \cf0\highlight0    72  sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0    73  sudo ./cluster/kubectl.sh describe pod netpod1 -n default\cf1\highlight2 
\par \cf0\highlight0    74  sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0    75  sudo ./cluster/kubectl.sh get vpc -Ao wide\cf1\highlight2 
\par \cf0\highlight0    76  sudo ./cluster/kubectl.sh getnet -Ao wide\cf1\highlight2 
\par \cf0\highlight0    77  sudo ./cluster/kubectl.sh get net -Ao wide\cf1\highlight2 
\par \cf0\highlight0    78  sudo ./cluster/kubectl.sh get subnets -Ao wide\cf1\highlight2 
\par \cf0\highlight0    79  history\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$  sudo ./cluster/kubectl.sh describe pod netpod1 -n default\cf1\highlight2 
\par \cf0\highlight0 Name:         netpod1\cf1\highlight2 
\par \cf0\highlight0 Namespace:    default\cf1\highlight2 
\par \cf0\highlight0 Tenant:       system\cf1\highlight2 
\par \cf0\highlight0 Priority:     0\cf1\highlight2 
\par \cf0\highlight0 Node:         arktos/172.31.19.33\cf1\highlight2 
\par \cf0\highlight0 Start Time:   Tue, 07 Dec 2021 09:09:26 +0000\cf1\highlight2 
\par \cf0\highlight0 Labels:       podkey=netpodkey1\cf1\highlight2 
\par \cf0\highlight0 Annotations:  kopf.zalando.org/last-handled-configuration:\cf1\highlight2 
\par \cf0\highlight0                 \{"spec":\{"volumes":[\{"name":"default-token-cx9sf","secret":\{"secretName":"default-token-cx9sf","defaultMode":420\}\}],"containers":[\{"name":...\cf1\highlight2 
\par \cf0\highlight0               kubectl.kubernetes.io/last-applied-configuration:\cf1\highlight2 
\par \cf0\highlight0                 \{"apiVersion":"v1","kind":"Pod","metadata":\{"annotations":\{\},"labels":\{"podkey":"netpodkey1"\},"name":"netpod1","namespace":"default","tena...\cf1\highlight2 
\par \cf0\highlight0 Status:       Pending\cf1\highlight2 
\par \cf0\highlight0 IP:\cf1\highlight2 
\par \cf0\highlight0 Containers:\cf1\highlight2 
\par \cf0\highlight0   netctr:\cf1\highlight2 
\par \cf0\highlight0     Container ID:\cf1\highlight2 
\par \cf0\highlight0     Image:         yuvalif/fedora-tcpdump\cf1\highlight2 
\par \cf0\highlight0     Image ID:\cf1\highlight2 
\par \cf0\highlight0     Port:          <none>\cf1\highlight2 
\par \cf0\highlight0     Host Port:     <none>\cf1\highlight2 
\par \cf0\highlight0     Command:\cf1\highlight2 
\par \cf0\highlight0       tail\cf1\highlight2 
\par \cf0\highlight0       -f\cf1\highlight2 
\par \cf0\highlight0       /dev/null\cf1\highlight2 
\par \cf0\highlight0     State:          Waiting\cf1\highlight2 
\par \cf0\highlight0       Reason:       ContainerCreating\cf1\highlight2 
\par \cf0\highlight0     Ready:          False\cf1\highlight2 
\par \cf0\highlight0     Restart Count:  0\cf1\highlight2 
\par \cf0\highlight0     Environment:    <none>\cf1\highlight2 
\par \cf0\highlight0     Mounts:\cf1\highlight2 
\par \cf0\highlight0       /var/run/secrets/kubernetes.io/serviceaccount from default-token-cx9sf (ro)\cf1\highlight2 
\par \cf0\highlight0 Conditions:\cf1\highlight2 
\par \cf0\highlight0   Type              Status\cf1\highlight2 
\par \cf0\highlight0   Initialized       True\cf1\highlight2 
\par \cf0\highlight0   ContainersReady   False\cf1\highlight2 
\par \cf0\highlight0   Ready             False\cf1\highlight2 
\par \cf0\highlight0   PodScheduled      True\cf1\highlight2 
\par \cf0\highlight0 Volumes:\cf1\highlight2 
\par \cf0\highlight0   default-token-cx9sf:\cf1\highlight2 
\par \cf0\highlight0     Type:        Secret (a volume populated by a Secret)\cf1\highlight2 
\par \cf0\highlight0     SecretName:  default-token-cx9sf\cf1\highlight2 
\par \cf0\highlight0     Optional:    false\cf1\highlight2 
\par \cf0\highlight0 QoS Class:       BestEffort\cf1\highlight2 
\par \cf0\highlight0 Node-Selectors:  <none>\cf1\highlight2 
\par \cf0\highlight0 Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\cf1\highlight2 
\par \cf0\highlight0                  node.kubernetes.io/unreachable:NoExecute for 300s\cf1\highlight2 
\par \cf0\highlight0 Events:          <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ history\cf1\highlight2 
\par \cf0\highlight0     1  vi /etc/hsotename\cf1\highlight2 
\par \cf0\highlight0     2  vi /etc/hostname\cf1\highlight2 
\par \cf0\highlight0     3  chmod 777 /etc/hostname\cf1\highlight2 
\par \cf0\highlight0     4  sudo -i\cf1\highlight2 
\par \cf0\highlight0     5  sudo ./cluster/kubectl.sh get nodes -Ao wide\cf1\highlight2 
\par \cf0\highlight0     6  sudo /cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0     7  sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0     8  sudo ./cluster/kubectl.sh describe pod kube-dns-554c5866fc-2m9zz\cf1\highlight2 
\par \cf0\highlight0     9  sudo ./cluster/kubectl.sh describe pod kube-dns-554c5866fc-2m9zz -n kube-system\cf1\highlight2 
\par \cf0\highlight0    10  sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0    11  sudo ./cluster/kubectl.sh get vpc -Ao wide\cf1\highlight2 
\par \cf0\highlight0    12  sudo ./cluster/kubectl.sh get subnets -Ao wide\cf1\highlight2 
\par \cf0\highlight0    13  sudo ./cluster/kubectl.sh get net -Ao wide\cf1\highlight2 
\par \cf0\highlight0    14  sudo ./cluster/kubectl.sh get dividers -Ao wide\cf1\highlight2 
\par \cf0\highlight0    15  sudo ./cluster/kubectl.sh get divider -Ao wide\cf1\highlight2 
\par \cf0\highlight0    16  sudo ./cluster/kubectl.sh get dividers -Ao wide\cf1\highlight2 
\par \cf0\highlight0    17  sudo ./cluster/kubectl.sh get bouncers -Ao wide\cf1\highlight2 
\par \cf0\highlight0    18  sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0    19  sudo ./cluster/kubectl.sh get net -Ao wide\cf1\highlight2 
\par \cf0\highlight0    20  sudo ./cluster/kubectl.sh get net -A wide\cf1\highlight2 
\par \cf0\highlight0    21  sudo ./cluster/kubectl.sh get net -Ao wide\cf1\highlight2 
\par \cf0\highlight0    22  sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0    23  ./cluster/kubectl.sh apply -f https://raw.githubusercontent.com/Click2CloudCentaurus/Documentation/main/test-yamls/test_pods.yaml\cf1\highlight2 
\par \cf0\highlight0    24  sudo ./cluster/kubectl.sh apply -f https://raw.githubusercontent.com/Click2CloudCentaurus/Documentation/main/test-yamls/test_pods.yaml\cf1\highlight2 
\par \cf0\highlight0    25  sudo ./cluster/kubectl.sh apply -f https://raw.githubusercontent.com/Click2Cloud-Centaurus/Documentation/main/test-yamls/test_pods.yaml\cf1\highlight2 
\par \cf0\highlight0    26  sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0    27  sudo ./cluster/kubectl.sh describe pod netpod1 -n default\cf1\highlight2 
\par \cf0\highlight0    28  sudo ./cluster/kubectl.sh get pods\cf1\highlight2 
\par \cf0\highlight0    29  sudo ./cluster/kubectl.sh get pods -A\cf1\highlight2 
\par \cf0\highlight0    30  sudo ./cluster/kubectl.sh get nodes\cf1\highlight2 
\par \cf0\highlight0    31  sudo ./cluster/kubectl.sh get pod -A\cf1\highlight2 
\par \cf0\highlight0    32  sudo ./cluster/kubectl.sh describe pod netpod1 -n default\cf1\highlight2 
\par \cf0\highlight0    33  sudo ./cluster/kubectl.sh pods -Ao\cf1\highlight2 
\par \cf0\highlight0    34  history\cf1\highlight2 
\par \cf0\highlight0    35  sudo ./cluster/kubectl.sh apply -f https://raw.githubusercontent.com/Click2Cloud-Centaurus/Documentation/main/test-yamls/test_pods.yaml\cf1\highlight2 
\par \cf0\highlight0    36  sudo ./cluster/kubectl.sh delete -f https://raw.githubusercontent.com/Click2Cloud-Centaurus/Documentation/main/test-yamls/test_pods.yaml\cf1\highlight2 
\par \cf0\highlight0    37  vi pod.yaml\cf1\highlight2 
\par \cf0\highlight0    38  sudo vi pod.yaml\cf1\highlight2 
\par \cf0\highlight0    39  sudo ./cluster/kubectl.sh apply -f pod.yaml\cf1\highlight2 
\par \cf0\highlight0    40  sudo ./cluster/kubectl.sh pods -A\cf1\highlight2 
\par \cf0\highlight0    41  sudo ./cluster/kubectl.sh pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0    42  sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0    43  sudo ./cluster/kubectl.sh describe pod netpod1 -n default\cf1\highlight2 
\par \cf0\highlight0    44  sudo ./cluster/kubectl.sh get pods -A wide\cf1\highlight2 
\par \cf0\highlight0    45*\cf1\highlight2 
\par \cf0\highlight0    46  sudo ./cluster/kubectl.sh describe pod kube-dns-554c5866fc-2m9zz -n kube-system\cf1\highlight2 
\par \cf0\highlight0    47  sudo ./cluster/kubectl.sh get pods -A\cf1\highlight2 
\par \cf0\highlight0    48  sudo ./cluster/kubectl.sh describe pod netpod1 -n deafualt\cf1\highlight2 
\par \cf0\highlight0    49  sudo ./cluster/kubectl.sh describe pod netpod1 -n default\cf1\highlight2 
\par \cf0\highlight0    50  ./cluster/kubectl.sh get pods -A\cf1\highlight2 
\par \cf0\highlight0    51  sudo ./cluster/kubectl.sh get pods -A\cf1\highlight2 
\par \cf0\highlight0    52  sudo ./cluster/kubectl.sh get node -A\cf1\highlight2 
\par \cf0\highlight0    53  sudo ./cluster/kubectl.sh get events -A\cf1\highlight2 
\par \cf0\highlight0    54  sudo ./cluster/kubectl.sh get vpcs\cf1\highlight2 
\par \cf0\highlight0    55  sudo ./cluster/kubectl.sh get pods -A\cf1\highlight2 
\par \cf0\highlight0    56  sudo ./cluster/kubectl.sh get vpcs\cf1\highlight2 
\par \cf0\highlight0    57  sudo ./cluster/kubectl.sh get pods -A\cf1\highlight2 
\par \cf0\highlight0    58  sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0    59  sudo ./cluster/kubectl.sh get nodes -Ao wide\cf1\highlight2 
\par \cf0\highlight0    60  sudo ./cluster/kubectl.sh get vpc -Ao wide\cf1\highlight2 
\par \cf0\highlight0    61  sudo ./cluster/kubectl.sh get net -Ao wide\cf1\highlight2 
\par \cf0\highlight0    62  sudo ./cluster/kubectl.sh get dividers -Ao wide\cf1\highlight2 
\par \cf0\highlight0    63  sudo ./cluster/kubectl.sh get bouncers -Ao wide\cf1\highlight2 
\par \cf0\highlight0    64  sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0    65  sudo ./cluster/kubectl.sh describe pod virtlet-g97z2 -n kube-system\cf1\highlight2 
\par \cf0\highlight0    66  sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0    67  sudo ./cluster/kubectl.sh apply -f https://raw.githubusercontent.com/Click2Cloud-Centaurus/Documentation/main/test-yamls/test_pods.yaml\cf1\highlight2 
\par \cf0\highlight0    68  sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0    69  ./cluster/kubectl.sh version\cf1\highlight2 
\par \cf0\highlight0    70  sudo ./cluster/kubectl.sh --version\cf1\highlight2 
\par \cf0\highlight0    71  sudo ./cluster/kubectl.sh version\cf1\highlight2 
\par \cf0\highlight0    72  sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0    73  sudo ./cluster/kubectl.sh describe pod netpod1 -n default\cf1\highlight2 
\par \cf0\highlight0    74  sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0    75  sudo ./cluster/kubectl.sh get vpc -Ao wide\cf1\highlight2 
\par \cf0\highlight0    76  sudo ./cluster/kubectl.sh getnet -Ao wide\cf1\highlight2 
\par \cf0\highlight0    77  sudo ./cluster/kubectl.sh get net -Ao wide\cf1\highlight2 
\par \cf0\highlight0    78  sudo ./cluster/kubectl.sh get subnets -Ao wide\cf1\highlight2 
\par \cf0\highlight0    79  history\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get bouncers -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE   NAME                                          VPC    NET    IP             MAC                 DROPLET   STATUS        CREATETIME                   PROVISIONDELAY\cf1\highlight2 
\par \cf0\highlight0 default     net0-b-ab61a092-5b45-4f9f-82e7-69e23bc0135e   vpc0   net0   172.31.19.33   06:c5:e6:0f:54:9e   arktos    Provisioned   2021-12-07T08:47:55.141759   1.05588\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get dividers -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE   NAME                                          VPC    IP             MAC                 DROPLET   STATUS        CREATETIME                   PROVISIONDELAY\cf1\highlight2 
\par \cf0\highlight0 default     vpc0-d-c2c16f11-1758-44fe-8dd1-bac7096437ad   vpc0   172.31.19.33   06:c5:e6:0f:54:9e   arktos    Provisioned   2021-12-07T08:47:35.278403   0.240979\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh apply -f https://raw.githubusercontent.com/Click2Cloud-Centaurus/Documentation/main/test-yamls/test_pods.yaml\cf1\highlight2 
\par \cf0\highlight0 pod/netpod1 unchanged\cf1\highlight2 
\par \cf0\highlight0 pod/netpod2 unchanged\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh delete -f https://raw.githubusercontent.com/Click2Cloud-Centaurus/Documentation/main/test-yamls/test_pods.yaml\cf1\highlight2 
\par \cf0\highlight0 pod "netpod1" deleted\cf1\highlight2 
\par \cf0\highlight0 pod "netpod2" deleted\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh apply -f https://raw.githubusercontent.com/Click2Cloud-Centaurus/Documentation/main/test-yamls/test_pods.yaml\cf1\highlight2 
\par \cf0\highlight0 pod/netpod1 created\cf1\highlight2 
\par \cf0\highlight0 pod/netpod2 created\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS              RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-c5jrs                5433553103926506877   1/1     Running             0          34m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-tkx5m   4252175553422586693   1/1     Running             0          34m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod1                           7700285248518461656   0/1     ContainerCreating   0          11s   <none>         arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod2                           5209992066268822030   0/1     ContainerCreating   0          11s   <none>         arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-dgwpx         1606520876123553529   3/3     Running             0          34m   20.0.0.2       arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   virtlet-g97z2                     2296479715811486546   0/3     Init:0/1            0          34m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh describe pod virtlet-g97z2 -n kube-system\cf1\highlight2 
\par \cf0\highlight0 Name:           virtlet-g97z2\cf1\highlight2 
\par \cf0\highlight0 Namespace:      kube-system\cf1\highlight2 
\par \cf0\highlight0 Tenant:         system\cf1\highlight2 
\par \cf0\highlight0 Priority:       0\cf1\highlight2 
\par \cf0\highlight0 Node:           arktos/172.31.19.33\cf1\highlight2 
\par \cf0\highlight0 Start Time:     Tue, 07 Dec 2021 08:46:51 +0000\cf1\highlight2 
\par \cf0\highlight0 Labels:         controller-revision-hash=5899cf49f7\cf1\highlight2 
\par \cf0\highlight0                 pod-template-generation=1\cf1\highlight2 
\par \cf0\highlight0                 runtime=virtlet\cf1\highlight2 
\par \cf0\highlight0 Annotations:    apparmorlibvirtname: apparmorlibvirtvalue\cf1\highlight2 
\par \cf0\highlight0                 apparmorvirtletname: apparmorvirtletvalue\cf1\highlight2 
\par \cf0\highlight0                 apparmorvmsname: apparmorvmsvalue\cf1\highlight2 
\par \cf0\highlight0                 kopf.zalando.org/last-handled-configuration:\cf1\highlight2 
\par \cf0\highlight0                   \{"spec":\{"volumes":[\{"name":"dev","hostPath":\{"path":"/dev","type":""\}\},\{"name":"cgroup","hostPath":\{"path":"/sys/fs/cgroup","type":""\}\},\{...\cf1\highlight2 
\par \cf0\highlight0 Status:         Pending\cf1\highlight2 
\par \cf0\highlight0 IP:             172.31.19.33\cf1\highlight2 
\par \cf0\highlight0 Controlled By:  DaemonSet/virtlet\cf1\highlight2 
\par \cf0\highlight0 Init Containers:\cf1\highlight2 
\par \cf0\highlight0   prepare-node:\cf1\highlight2 
\par \cf0\highlight0     Container ID:\cf1\highlight2 
\par \cf0\highlight0     Image:         arktosstaging/vmruntime:0.5.4\cf1\highlight2 
\par \cf0\highlight0     Image ID:\cf1\highlight2 
\par \cf0\highlight0     Port:          <none>\cf1\highlight2 
\par \cf0\highlight0     Host Port:     <none>\cf1\highlight2 
\par \cf0\highlight0     Command:\cf1\highlight2 
\par \cf0\highlight0       /prepare-node.sh\cf1\highlight2 
\par \cf0\highlight0     State:          Waiting\cf1\highlight2 
\par \cf0\highlight0       Reason:       PodInitializing\cf1\highlight2 
\par \cf0\highlight0     Ready:          False\cf1\highlight2 
\par \cf0\highlight0     Restart Count:  0\cf1\highlight2 
\par \cf0\highlight0     Environment:\cf1\highlight2 
\par \cf0\highlight0       KUBE_NODE_NAME:                   (v1:spec.nodeName)\cf1\highlight2 
\par \cf0\highlight0       VIRTLET_DISABLE_KVM:             <set to the key 'disable_kvm' of config map 'virtlet-config'>               Optional: true\cf1\highlight2 
\par \cf0\highlight0       VIRTLET_SRIOV_SUPPORT:           <set to the key 'sriov_support' of config map 'virtlet-config'>             Optional: true\cf1\highlight2 
\par \cf0\highlight0       VIRTLET_DOWNLOAD_PROTOCOL:       <set to the key 'download_protocol' of config map 'virtlet-config'>         Optional: true\cf1\highlight2 
\par \cf0\highlight0       VIRTLET_LOGLEVEL:                <set to the key 'loglevel' of config map 'virtlet-config'>                  Optional: true\cf1\highlight2 
\par \cf0\highlight0       VIRTLET_CALICO_SUBNET:           <set to the key 'calico-subnet' of config map 'virtlet-config'>             Optional: true\cf1\highlight2 
\par \cf0\highlight0       IMAGE_REGEXP_TRANSLATION:        <set to the key 'image_regexp_translation' of config map 'virtlet-config'>  Optional: true\cf1\highlight2 
\par \cf0\highlight0       VIRTLET_RAW_DEVICES:             <set to the key 'raw_devices' of config map 'virtlet-config'>               Optional: true\cf1\highlight2 
\par \cf0\highlight0       VIRTLET_DISABLE_LOGGING:         <set to the key 'disable_logging' of config map 'virtlet-config'>           Optional: true\cf1\highlight2 
\par \cf0\highlight0       VIRTLET_CPU_MODEL:               <set to the key 'cpu-model' of config map 'virtlet-config'>                 Optional: true\cf1\highlight2 
\par \cf0\highlight0       KUBELET_ROOT_DIR:                <set to the key 'kubelet_root_dir' of config map 'virtlet-config'>          Optional: true\cf1\highlight2 
\par \cf0\highlight0       VIRTLET_IMAGE_TRANSLATIONS_DIR:  /etc/virtlet/images\cf1\highlight2 
\par \cf0\highlight0     Mounts:\cf1\highlight2 
\par \cf0\highlight0       /dev from dev (rw)\cf1\highlight2 
\par \cf0\highlight0       /host-var-lib from var-lib (rw)\cf1\highlight2 
\par \cf0\highlight0       /hostlog from log (rw)\cf1\highlight2 
\par \cf0\highlight0       /kubelet-volume-plugins from k8s-flexvolume-plugins-dir (rw)\cf1\highlight2 
\par \cf0\highlight0       /run from run (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/lib/virtlet from virtlet (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/run/docker.sock from dockersock (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/run/secrets/kubernetes.io/serviceaccount from virtlet-token-8nrdd (ro)\cf1\highlight2 
\par \cf0\highlight0 Containers:\cf1\highlight2 
\par \cf0\highlight0   libvirt:\cf1\highlight2 
\par \cf0\highlight0     Container ID:\cf1\highlight2 
\par \cf0\highlight0     Image:         arktosstaging/vmruntime:0.5.4\cf1\highlight2 
\par \cf0\highlight0     Image ID:\cf1\highlight2 
\par \cf0\highlight0     Port:          <none>\cf1\highlight2 
\par \cf0\highlight0     Host Port:     <none>\cf1\highlight2 
\par \cf0\highlight0     Command:\cf1\highlight2 
\par \cf0\highlight0       /libvirt.sh\cf1\highlight2 
\par \cf0\highlight0     State:          Waiting\cf1\highlight2 
\par \cf0\highlight0       Reason:       PodInitializing\cf1\highlight2 
\par \cf0\highlight0     Ready:          False\cf1\highlight2 
\par \cf0\highlight0     Restart Count:  0\cf1\highlight2 
\par \cf0\highlight0     Readiness:      exec [/bin/sh -c socat - UNIX:/var/run/libvirt/libvirt-sock-ro </dev/null] delay=0s timeout=1s period=10s #success=1 #failure=3\cf1\highlight2 
\par \cf0\highlight0     Environment:    <none>\cf1\highlight2 
\par \cf0\highlight0     Mounts:\cf1\highlight2 
\par \cf0\highlight0       /boot from boot (ro)\cf1\highlight2 
\par \cf0\highlight0       /dev from dev (rw)\cf1\highlight2 
\par \cf0\highlight0       /etc/libvirt/qemu from qemu (rw)\cf1\highlight2 
\par \cf0\highlight0       /lib/modules from modules (ro)\cf1\highlight2 
\par \cf0\highlight0       /run from run (rw)\cf1\highlight2 
\par \cf0\highlight0       /sys/fs/cgroup from cgroup (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/lib/libvirt from libvirt (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/lib/virtlet from virtlet (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/log/libvirt from libvirt-log (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/log/vms from vms-log (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/run/libvirt from libvirt-sockets (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/run/secrets/kubernetes.io/serviceaccount from virtlet-token-8nrdd (ro)\cf1\highlight2 
\par \cf0\highlight0   virtlet:\cf1\highlight2 
\par \cf0\highlight0     Container ID:\cf1\highlight2 
\par \cf0\highlight0     Image:          arktosstaging/vmruntime:0.5.4\cf1\highlight2 
\par \cf0\highlight0     Image ID:\cf1\highlight2 
\par \cf0\highlight0     Port:           <none>\cf1\highlight2 
\par \cf0\highlight0     Host Port:      <none>\cf1\highlight2 
\par \cf0\highlight0     State:          Waiting\cf1\highlight2 
\par \cf0\highlight0       Reason:       PodInitializing\cf1\highlight2 
\par \cf0\highlight0     Ready:          False\cf1\highlight2 
\par \cf0\highlight0     Restart Count:  0\cf1\highlight2 
\par \cf0\highlight0     Readiness:      exec [/bin/sh -c socat - UNIX:/run/virtlet.sock </dev/null] delay=0s timeout=1s period=10s #success=1 #failure=3\cf1\highlight2 
\par \cf0\highlight0     Environment:    <none>\cf1\highlight2 
\par \cf0\highlight0     Mounts:\cf1\highlight2 
\par \cf0\highlight0       /boot from boot (ro)\cf1\highlight2 
\par \cf0\highlight0       /dev from dev (rw)\cf1\highlight2 
\par \cf0\highlight0       /etc/libvirt/qemu from qemu (rw)\cf1\highlight2 
\par \cf0\highlight0       /lib/modules from modules (ro)\cf1\highlight2 
\par \cf0\highlight0       /run from run (rw)\cf1\highlight2 
\par \cf0\highlight0       /sys/fs/cgroup from cgroup (rw)\cf1\highlight2 
\par \cf0\highlight0       /usr/libexec/kubernetes/kubelet-plugins/volume/exec from k8s-flexvolume-plugins-dir (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/lib/kubelet/pods from k8s-pods-dir (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/lib/libvirt from libvirt (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/lib/virtlet from virtlet (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/log/libvirt from libvirt-log (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/log/pods from pods-log (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/log/vms from vms-log (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/run/libvirt from libvirt-sockets (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/run/netns from netns-dir (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/run/secrets/kubernetes.io/serviceaccount from virtlet-token-8nrdd (ro)\cf1\highlight2 
\par \cf0\highlight0   vms:\cf1\highlight2 
\par \cf0\highlight0     Container ID:\cf1\highlight2 
\par \cf0\highlight0     Image:         arktosstaging/vmruntime:0.5.4\cf1\highlight2 
\par \cf0\highlight0     Image ID:\cf1\highlight2 
\par \cf0\highlight0     Port:          <none>\cf1\highlight2 
\par \cf0\highlight0     Host Port:     <none>\cf1\highlight2 
\par \cf0\highlight0     Command:\cf1\highlight2 
\par \cf0\highlight0       /vms.sh\cf1\highlight2 
\par \cf0\highlight0     State:          Waiting\cf1\highlight2 
\par \cf0\highlight0       Reason:       PodInitializing\cf1\highlight2 
\par \cf0\highlight0     Ready:          False\cf1\highlight2 
\par \cf0\highlight0     Restart Count:  0\cf1\highlight2 
\par \cf0\highlight0     Environment:    <none>\cf1\highlight2 
\par \cf0\highlight0     Mounts:\cf1\highlight2 
\par \cf0\highlight0       /dev from dev (rw)\cf1\highlight2 
\par \cf0\highlight0       /lib/modules from modules (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/lib/kubelet/pods from k8s-pods-dir (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/lib/libvirt from libvirt (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/lib/virtlet from virtlet (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/log/vms from vms-log (rw)\cf1\highlight2 
\par \cf0\highlight0       /var/run/secrets/kubernetes.io/serviceaccount from virtlet-token-8nrdd (ro)\cf1\highlight2 
\par \cf0\highlight0 Conditions:\cf1\highlight2 
\par \cf0\highlight0   Type              Status\cf1\highlight2 
\par \cf0\highlight0   Initialized       False\cf1\highlight2 
\par \cf0\highlight0   ContainersReady   False\cf1\highlight2 
\par \cf0\highlight0   Ready             False\cf1\highlight2 
\par \cf0\highlight0   PodScheduled      True\cf1\highlight2 
\par \cf0\highlight0 Volumes:\cf1\highlight2 
\par \cf0\highlight0   dev:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /dev\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   cgroup:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /sys/fs/cgroup\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   modules:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /lib/modules\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   boot:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /boot\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   run:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /run\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   dockersock:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /var/run/docker.sock\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   virtlet:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /var/lib/virtlet\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   libvirt:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /var/lib/libvirt\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   log:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /var/log\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   k8s-flexvolume-plugins-dir:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   k8s-pods-dir:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /var/lib/kubelet/pods\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   var-lib:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /var/lib\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   vms-log:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /var/log/virtlet/vms\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   libvirt-log:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /var/log/libvirt\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   libvirt-sockets:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /var/run/libvirt\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   pods-log:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /var/log/pods\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   netns-dir:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /var/run/netns\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   qemu:\cf1\highlight2 
\par \cf0\highlight0     Type:          HostPath (bare host directory volume)\cf1\highlight2 
\par \cf0\highlight0     Path:          /etc/libvirt/qemu\cf1\highlight2 
\par \cf0\highlight0     HostPathType:\cf1\highlight2 
\par \cf0\highlight0   virtlet-token-8nrdd:\cf1\highlight2 
\par \cf0\highlight0     Type:        Secret (a volume populated by a Secret)\cf1\highlight2 
\par \cf0\highlight0     SecretName:  virtlet-token-8nrdd\cf1\highlight2 
\par \cf0\highlight0     Optional:    false\cf1\highlight2 
\par \cf0\highlight0 QoS Class:       BestEffort\cf1\highlight2 
\par \cf0\highlight0 Node-Selectors:  <none>\cf1\highlight2 
\par \cf0\highlight0 Tolerations:     node.kubernetes.io/disk-pressure:NoSchedule\cf1\highlight2 
\par \cf0\highlight0                  node.kubernetes.io/memory-pressure:NoSchedule\cf1\highlight2 
\par \cf0\highlight0                  node.kubernetes.io/network-unavailable:NoSchedule\cf1\highlight2 
\par \cf0\highlight0                  node.kubernetes.io/not-ready:NoExecute\cf1\highlight2 
\par \cf0\highlight0                  node.kubernetes.io/pid-pressure:NoSchedule\cf1\highlight2 
\par \cf0\highlight0                  node.kubernetes.io/unreachable:NoExecute\cf1\highlight2 
\par \cf0\highlight0                  node.kubernetes.io/unschedulable:NoSchedule\cf1\highlight2 
\par \cf0\highlight0 Events:          <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS              RESTARTS   AGE    IP             NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-c5jrs                5433553103926506877   1/1     Running             0          36m    172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-tkx5m   4252175553422586693   1/1     Running             0          36m    172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod1                           7700285248518461656   0/1     ContainerCreating   0          102s   <none>         arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod2                           5209992066268822030   0/1     ContainerCreating   0          102s   <none>         arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-dgwpx         1606520876123553529   3/3     Running             0          36m    20.0.0.2       arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   virtlet-g97z2                     2296479715811486546   0/3     Init:0/1            0          36m    172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh describe pod netpod1 -n default\cf1\highlight2 
\par \cf0\highlight0 Name:         netpod1\cf1\highlight2 
\par \cf0\highlight0 Namespace:    default\cf1\highlight2 
\par \cf0\highlight0 Tenant:       system\cf1\highlight2 
\par \cf0\highlight0 Priority:     0\cf1\highlight2 
\par \cf0\highlight0 Node:         arktos/172.31.19.33\cf1\highlight2 
\par \cf0\highlight0 Start Time:   Tue, 07 Dec 2021 09:21:34 +0000\cf1\highlight2 
\par \cf0\highlight0 Labels:       podkey=netpodkey1\cf1\highlight2 
\par \cf0\highlight0 Annotations:  kopf.zalando.org/last-handled-configuration:\cf1\highlight2 
\par \cf0\highlight0                 \{"spec":\{"volumes":[\{"name":"default-token-cx9sf","secret":\{"secretName":"default-token-cx9sf","defaultMode":420\}\}],"containers":[\{"name":...\cf1\highlight2 
\par \cf0\highlight0               kubectl.kubernetes.io/last-applied-configuration:\cf1\highlight2 
\par \cf0\highlight0                 \{"apiVersion":"v1","kind":"Pod","metadata":\{"annotations":\{\},"labels":\{"podkey":"netpodkey1"\},"name":"netpod1","namespace":"default","tena...\cf1\highlight2 
\par \cf0\highlight0 Status:       Pending\cf1\highlight2 
\par \cf0\highlight0 IP:\cf1\highlight2 
\par \cf0\highlight0 Containers:\cf1\highlight2 
\par \cf0\highlight0   netctr:\cf1\highlight2 
\par \cf0\highlight0     Container ID:\cf1\highlight2 
\par \cf0\highlight0     Image:         yuvalif/fedora-tcpdump\cf1\highlight2 
\par \cf0\highlight0     Image ID:\cf1\highlight2 
\par \cf0\highlight0     Port:          <none>\cf1\highlight2 
\par \cf0\highlight0     Host Port:     <none>\cf1\highlight2 
\par \cf0\highlight0     Command:\cf1\highlight2 
\par \cf0\highlight0       tail\cf1\highlight2 
\par \cf0\highlight0       -f\cf1\highlight2 
\par \cf0\highlight0       /dev/null\cf1\highlight2 
\par \cf0\highlight0     State:          Waiting\cf1\highlight2 
\par \cf0\highlight0       Reason:       ContainerCreating\cf1\highlight2 
\par \cf0\highlight0     Ready:          False\cf1\highlight2 
\par \cf0\highlight0     Restart Count:  0\cf1\highlight2 
\par \cf0\highlight0     Environment:    <none>\cf1\highlight2 
\par \cf0\highlight0     Mounts:\cf1\highlight2 
\par \cf0\highlight0       /var/run/secrets/kubernetes.io/serviceaccount from default-token-cx9sf (ro)\cf1\highlight2 
\par \cf0\highlight0 Conditions:\cf1\highlight2 
\par \cf0\highlight0   Type              Status\cf1\highlight2 
\par \cf0\highlight0   Initialized       True\cf1\highlight2 
\par \cf0\highlight0   ContainersReady   False\cf1\highlight2 
\par \cf0\highlight0   Ready             False\cf1\highlight2 
\par \cf0\highlight0   PodScheduled      True\cf1\highlight2 
\par \cf0\highlight0 Volumes:\cf1\highlight2 
\par \cf0\highlight0   default-token-cx9sf:\cf1\highlight2 
\par \cf0\highlight0     Type:        Secret (a volume populated by a Secret)\cf1\highlight2 
\par \cf0\highlight0     SecretName:  default-token-cx9sf\cf1\highlight2 
\par \cf0\highlight0     Optional:    false\cf1\highlight2 
\par \cf0\highlight0 QoS Class:       BestEffort\cf1\highlight2 
\par \cf0\highlight0 Node-Selectors:  <none>\cf1\highlight2 
\par \cf0\highlight0 Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\cf1\highlight2 
\par \cf0\highlight0                  node.kubernetes.io/unreachable:NoExecute for 300s\cf1\highlight2 
\par \cf0\highlight0 Events:          <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ ./get events\cf1\highlight2 
\par \cf0\highlight0 .git/         CHANGELOG/    api/          cmd/          logo/         plugin/       third_party/\cf1\highlight2 
\par \cf0\highlight0 .github/      Godeps/       build/        docs/         perf-tests/   staging/      translations/\cf1\highlight2 
\par \cf0\highlight0 .make/        _output/      cluster/      hack/         pkg/          test/         vendor/\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ ./cluster/kubectl.sh get events\cf1\highlight2 
\par \cf0\highlight0 ./cluster/../cluster/../cluster/gce/../../cluster/gce/../../cluster/common.sh: line 30: /root/go/src/k8s.io/arktos/hack/lib/util.sh: Permission denied\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get events\cf1\highlight2 
\par \cf0\highlight0 LAST SEEN   TYPE      REASON                           OBJECT                                                       MESSAGE\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    Starting                         node/arktos                                                  Starting kube-proxy.\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    Starting                         node/arktos                                                  Starting kubelet.\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    VmRuntimeReady is not ready      node/arktos                                                  Node arktos status is now: VmRuntimeReady is not ready\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    ContainerRuntimeReady is ready   node/arktos                                                  Node arktos status is now: ContainerRuntimeReady is ready\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    NodeHasSufficientMemory          node/arktos                                                  Node arktos status is now: NodeHasSufficientMemory\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    NodeHasNoDiskPressure            node/arktos                                                  Node arktos status is now: NodeHasNoDiskPressure\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    NodeHasSufficientPID             node/arktos                                                  Node arktos status is now: NodeHasSufficientPID\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    NodeAllocatableEnforced          node/arktos                                                  Updated Node Allocatable limit across pods\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    NodeReady                        node/arktos                                                  Node arktos status is now: NodeReady\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    RegisteredNode                   node/arktos                                                  Node arktos event: Registered Node arktos in Controller\cf1\highlight2 
\par \cf0\highlight0 45s         Warning   ImageGCFailed                    node/arktos                                                  rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/virtlet.sock: connect: no such file or directory"\cf1\highlight2 
\par \cf0\highlight0 39m         Error     Logging                          subnet/net0                                                  Handler 'net_opr_on_net_init' failed temporarily: Temporary Error: Task: NetCreate Net: net0 No droplets available.\cf1\highlight2 
\par \cf0\highlight0 5m45s       Normal    Logging                          endpoint/netpod2-default-system-eth0                         Updating is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 40m         Error     Logging                          subnet/net0                                                  Handler 'net_opr_on_net_init' failed temporarily: Temporary Error: Task: NetCreate Net: net0 No droplets available.\cf1\highlight2 
\par \cf0\highlight0 17m         Normal    Logging                          pod/netpod2                                                  Handler 'builtins_on_pod' succeeded.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          divider/vpc0-d-c2c16f11-1758-44fe-8dd1-bac7096437ad          Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 6m9s        Normal    Logging                          endpoint/netpod1-default-system-eth0                         Deletion is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          vpc/vpc0                                                     Handler 'vpc_opr_on_vpc_provisioned' succeeded.\cf1\highlight2 
\par \cf0\highlight0 17m         Normal    Logging                          pod/netpod2                                                  Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 17m         Normal    Logging                          endpoint/netpod1-default-system-eth0                         Handler 'endpoint_opr_on_endpoint_provisioned' succeeded.\cf1\highlight2 
\par \cf0\highlight0 6m9s        Normal    Logging                          pod/netpod2                                                  Deletion is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 5m45s       Normal    Logging                          endpoint/netpod2-default-system-eth0                         Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 40m         Error     Logging                          pod/mizar-daemon-c5jrs                                       Handler 'builtins_on_pod' failed temporarily: Temporary Error: Droplet not yet created.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          subnet/net0                                                  Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          bouncer/net0-b-ab61a092-5b45-4f9f-82e7-69e23bc0135e          Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          vpc/vpc0                                                     Updating is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 40m         Error     Logging                          endpoints/kubernetes                                         Handler 'services_opr_on_endpoints' failed temporarily: Temporary Error: Task: k8sEndpointsUpdate Endpoint: kubernetes-default bouncers not yet provisioned.\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    Logging                          endpoint/kubernetes-default                                  Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          endpoint/virtlet-g97z2-kube-system-system-eth0               Handler 'endpoint_opr_on_endpoint_init' succeeded.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          pod/mizar-daemon-c5jrs                                       Handler 'builtins_on_pod' succeeded.\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    Logging                          endpoint/kube-dns-kube-system                                Handler 'endpoint_opr_on_endpoint_init' succeeded.\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    Logging                          endpoint/kubernetes-default                                  Updating is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 40m         Error     Logging                          pod/mizar-daemon-c5jrs                                       Handler 'builtins_on_pod' failed temporarily: Temporary Error: Droplet not yet created.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          droplet/arktos                                               Updating is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          subnet/net0                                                  Handler 'net_opr_on_net_init' succeeded.\cf1\highlight2 
\par \cf0\highlight0 40m         Error     Logging                          vpc/vpc0                                                     Handler 'vpc_opr_on_vpc_init' failed temporarily: Temporary Error: Task: VpcCreate VPC: vpc0 No droplets available.\cf1\highlight2 
\par \cf0\highlight0 40m         Error     Logging                          vpc/vpc0                                                     Handler 'vpc_opr_on_vpc_init' failed temporarily: Temporary Error: Task: VpcCreate VPC: vpc0 No droplets available.\cf1\highlight2 
\par \cf0\highlight0 6m9s        Normal    Logging                          pod/netpod1                                                  Deletion is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 5m46s       Normal    Logging                          pod/netpod1                                                  Handler 'builtins_on_pod' succeeded.\cf1\highlight2 
\par \cf0\highlight0 5m46s       Normal    Logging                          pod/netpod2                                                  Handler 'builtins_on_pod' succeeded.\cf1\highlight2 
\par \cf0\highlight0 40m         Error     Logging                          node/arktos                                                  Handler 'droplet_opr_on_node' failed temporarily: Temporary Error: Daemon not ready.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          endpoint/virtlet-g97z2-kube-system-system-eth0               Handler 'endpoint_opr_on_endpoint_provisioned' succeeded.\cf1\highlight2 
\par \cf0\highlight0 17m         Normal    Logging                          endpoint/netpod1-default-system-eth0                         Updating is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 17m         Normal    Logging                          pod/netpod1                                                  Handler 'builtins_on_pod' succeeded.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          subnet/net0                                                  Handler 'net_opr_on_net_provisioned' succeeded.\cf1\highlight2 
\par \cf0\highlight0 40m         Error     Logging                          node/arktos                                                  Handler 'droplet_opr_on_node' failed temporarily: Temporary Error: Daemon not ready.\cf1\highlight2 
\par \cf0\highlight0 17m         Normal    Logging                          endpoint/netpod1-default-system-eth0                         Handler 'endpoint_opr_on_endpoint_init' succeeded.\cf1\highlight2 
\par \cf0\highlight0 5m45s       Normal    Logging                          endpoint/netpod1-default-system-eth0                         Updating is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          vpc/vpc0                                                     Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 6m9s        Normal    Logging                          endpoint/netpod1-default-system-eth0                         Handler 'endpoint_opr_on_endpoint_delete' succeeded.\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    Logging                          namespace/kube-node-lease                                    Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 39m         Error     Logging                          endpoints/kubernetes                                         Handler 'services_opr_on_endpoints' failed temporarily: Temporary Error: Task: k8sEndpointsUpdate Endpoint: kubernetes-default bouncers not yet provisioned.\cf1\highlight2 
\par \cf0\highlight0 40m         Error     Logging                          pod/mizar-operator-6b78d7ffc4-tkx5m                          Handler 'builtins_on_pod' failed temporarily: Temporary Error: Droplet not yet created.\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    Logging                          endpoint/kube-dns-kube-system                                Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    Logging                          namespace/default                                            Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          node/arktos                                                  Handler 'droplet_opr_on_node' succeeded.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          node/arktos                                                  Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          endpoint/virtlet-g97z2-kube-system-system-eth0               Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 17m         Normal    Logging                          endpoint/netpod2-default-system-eth0                         Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 5m45s       Normal    Logging                          endpoint/netpod1-default-system-eth0                         Handler 'endpoint_opr_on_endpoint_init' succeeded.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          divider/vpc0-d-c2c16f11-1758-44fe-8dd1-bac7096437ad          Handler 'divider_opr_on_divider_provisioned' succeeded.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          droplet/arktos                                               Handler 'droplet_opr_on_droplet_provisioned' succeeded.\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    Logging                          namespace/kube-public                                        Handler 'builtins_on_namespace' succeeded.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          divider/vpc0-d-c2c16f11-1758-44fe-8dd1-bac7096437ad          Handler 'divider_opr_on_divider_init' succeeded.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          endpoint/arktos-default--hostep                              Handler 'endpoint_opr_on_endpoint_init' succeeded.\cf1\highlight2 
\par \cf0\highlight0 17m         Normal    Logging                          endpoint/netpod2-default-system-eth0                         Handler 'endpoint_opr_on_endpoint_provisioned' succeeded.\cf1\highlight2 
\par \cf0\highlight0 40m         Error     Logging                          endpoints/kubernetes                                         Handler 'services_opr_on_endpoints' failed temporarily: Temporary Error: Task: k8sEndpointsUpdate Endpoint: kubernetes-default bouncers not yet provisioned.\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    Logging                          endpoint/kube-dns-kube-system                                Updating is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          divider/vpc0-d-c2c16f11-1758-44fe-8dd1-bac7096437ad          Updating is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 5m46s       Normal    Logging                          pod/netpod1                                                  Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          endpoint/kube-dns-554c5866fc-dgwpx-kube-system-system-eth0   Updating is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 40m         Error     Logging                          subnet/net0                                                  Handler 'net_opr_on_net_init' failed temporarily: Temporary Error: Task: NetCreate Net: net0 No droplets available.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          pod/mizar-operator-6b78d7ffc4-tkx5m                          Handler 'builtins_on_pod' succeeded.\cf1\highlight2 
\par \cf0\highlight0 17m         Normal    Logging                          endpoint/netpod1-default-system-eth0                         Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 5m45s       Normal    Logging                          endpoint/netpod2-default-system-eth0                         Handler 'endpoint_opr_on_endpoint_provisioned' succeeded.\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    Logging                          namespace/kube-public                                        Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          pod/mizar-daemon-c5jrs                                       Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    Logging                          service/kubernetes                                           Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          endpoint/kube-dns-554c5866fc-dgwpx-kube-system-system-eth0   Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 6m9s        Normal    Logging                          endpoint/netpod2-default-system-eth0                         Handler 'endpoint_opr_on_endpoint_delete' succeeded.\cf1\highlight2 
\par \cf0\highlight0 17m         Normal    Logging                          endpoint/netpod2-default-system-eth0                         Updating is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          droplet/arktos                                               Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          endpoints/kubernetes                                         Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    Logging                          service/kubernetes                                           Handler 'services_opr_on_services' succeeded.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          endpoint/kube-dns-554c5866fc-dgwpx-kube-system-system-eth0   Handler 'endpoint_opr_on_endpoint_init' succeeded.\cf1\highlight2 
\par \cf0\highlight0 40m         Error     Logging                          pod/mizar-operator-6b78d7ffc4-tkx5m                          Handler 'builtins_on_pod' failed temporarily: Temporary Error: Droplet not yet created.\cf1\highlight2 
\par \cf0\highlight0 6m9s        Normal    Logging                          pod/netpod1                                                  Handler 'builtins_on_pod_delete' succeeded.\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    Logging                          endpoint/kube-dns-kube-system                                Handler 'endpoint_opr_on_endpoint_provisioned' succeeded.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          endpoint/arktos-default--hostep                              Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          endpoint/virtlet-g97z2-kube-system-system-eth0               Updating is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          bouncer/net0-b-ab61a092-5b45-4f9f-82e7-69e23bc0135e          Updating is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    Logging                          namespace/default                                            Handler 'builtins_on_namespace' succeeded.\cf1\highlight2 
\par \cf0\highlight0 5m45s       Normal    Logging                          endpoint/netpod2-default-system-eth0                         Handler 'endpoint_opr_on_endpoint_init' succeeded.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          endpoint/arktos-default--hostep                              Updating is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    Logging                          namespace/kube-system                                        Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 5m45s       Normal    Logging                          endpoint/netpod1-default-system-eth0                         Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 5m45s       Normal    Logging                          endpoint/netpod1-default-system-eth0                         Handler 'endpoint_opr_on_endpoint_provisioned' succeeded.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          endpoint/arktos-default--hostep                              Handler 'endpoint_opr_on_endpoint_provisioned' succeeded.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          subnet/net0                                                  Updating is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          bouncer/net0-b-ab61a092-5b45-4f9f-82e7-69e23bc0135e          Handler 'bouncer_opr_on_bouncer_init' succeeded.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          pod/mizar-operator-6b78d7ffc4-tkx5m                          Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 17m         Normal    Logging                          endpoint/netpod2-default-system-eth0                         Handler 'endpoint_opr_on_endpoint_init' succeeded.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          endpoints/kubernetes                                         Handler 'services_opr_on_endpoints' succeeded.\cf1\highlight2 
\par \cf0\highlight0 39m         Error     Logging                          pod/mizar-operator-6b78d7ffc4-tkx5m                          Handler 'builtins_on_pod' failed temporarily: Temporary Error: Droplet not yet created.\cf1\highlight2 
\par \cf0\highlight0 6m9s        Normal    Logging                          endpoint/netpod2-default-system-eth0                         Deletion is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          bouncer/net0-b-ab61a092-5b45-4f9f-82e7-69e23bc0135e          Handler 'bouncer_opr_on_bouncer_provisioned' succeeded.\cf1\highlight2 
\par \cf0\highlight0 5m46s       Normal    Logging                          pod/netpod2                                                  Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 17m         Normal    Logging                          pod/netpod1                                                  Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    Logging                          namespace/kube-node-lease                                    Handler 'builtins_on_namespace' succeeded.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          endpoint/kube-dns-554c5866fc-dgwpx-kube-system-system-eth0   Handler 'endpoint_opr_on_endpoint_provisioned' succeeded.\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    Logging                          endpoint/kubernetes-default                                  Handler 'endpoint_opr_on_endpoint_provisioned' succeeded.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          vpc/vpc0                                                     Handler 'vpc_opr_on_vpc_init' succeeded.\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    Logging                          endpoint/kubernetes-default                                  Handler 'endpoint_opr_on_endpoint_init' succeeded.\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    Logging                          namespace/kube-system                                        Handler 'builtins_on_namespace' succeeded.\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Logging                          droplet/arktos                                               Handler 'droplet_opr_on_droplet_init' succeeded.\cf1\highlight2 
\par \cf0\highlight0 6m9s        Normal    Logging                          pod/netpod2                                                  Handler 'builtins_on_pod_delete' succeeded.\cf1\highlight2 
\par \cf0\highlight0 <unknown>   Normal    Scheduled                        pod/mizar-daemon-c5jrs                                       Successfully assigned default/mizar-daemon-c5jrs to arktos\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    Pulled                           pod/mizar-daemon-c5jrs                                       Container image "mizarnet/mizar:0.9" already present on machine\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    Created                          pod/mizar-daemon-c5jrs                                       Created container node-init\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    Started                          pod/mizar-daemon-c5jrs                                       Started container node-init\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Pulled                           pod/mizar-daemon-c5jrs                                       Container image "mizarnet/dropletd:0.9" already present on machine\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Created                          pod/mizar-daemon-c5jrs                                       Created container mizar-daemon\cf1\highlight2 
\par \cf0\highlight0 39m         Normal    Started                          pod/mizar-daemon-c5jrs                                       Started container mizar-daemon\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    SuccessfulCreate                 daemonset/mizar-daemon                                       Created pod: mizar-daemon-c5jrs\cf1\highlight2 
\par \cf0\highlight0 <unknown>   Normal    Scheduled                        pod/mizar-operator-6b78d7ffc4-tkx5m                          Successfully assigned default/mizar-operator-6b78d7ffc4-tkx5m to arktos\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    Pulled                           pod/mizar-operator-6b78d7ffc4-tkx5m                          Container image "mizarnet/endpointopr:0.9" already present on machine\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    Created                          pod/mizar-operator-6b78d7ffc4-tkx5m                          Created container mizar-operator\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    Started                          pod/mizar-operator-6b78d7ffc4-tkx5m                          Started container mizar-operator\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    SuccessfulCreate                 replicaset/mizar-operator-6b78d7ffc4                         Created pod: mizar-operator-6b78d7ffc4-tkx5m\cf1\highlight2 
\par \cf0\highlight0 40m         Normal    ScalingReplicaSet                deployment/mizar-operator                                    Scaled up replica set mizar-operator-6b78d7ffc4 to 1\cf1\highlight2 
\par \cf0\highlight0 <unknown>   Normal    Scheduled                        pod/netpod1                                                  Successfully assigned default/netpod1 to arktos\cf1\highlight2 
\par \cf0\highlight0 7m46s       Warning   FailedCreatePodSandBox           pod/netpod1                                                  Failed create pod sandbox: failed to get DNS IP address: network system/default not found\cf1\highlight2 
\par \cf0\highlight0 <unknown>   Normal    Scheduled                        pod/netpod1                                                  Successfully assigned default/netpod1 to arktos\cf1\highlight2 
\par \cf0\highlight0 33s         Warning   FailedCreatePodSandBox           pod/netpod1                                                  Failed create pod sandbox: failed to get DNS IP address: network system/default not found\cf1\highlight2 
\par \cf0\highlight0 <unknown>   Normal    Scheduled                        pod/netpod2                                                  Successfully assigned default/netpod2 to arktos\cf1\highlight2 
\par \cf0\highlight0 7m47s       Warning   FailedCreatePodSandBox           pod/netpod2                                                  Failed create pod sandbox: failed to get DNS IP address: network system/default not found\cf1\highlight2 
\par \cf0\highlight0 <unknown>   Normal    Scheduled                        pod/netpod2                                                  Successfully assigned default/netpod2 to arktos\cf1\highlight2 
\par \cf0\highlight0 32s         Warning   FailedCreatePodSandBox           pod/netpod2                                                  Failed create pod sandbox: failed to get DNS IP address: network system/default not found\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get vpc bouncers\cf1\highlight2 
\par \cf0\highlight0 Error from server (NotFound): vpcs.mizar.com "bouncers" not found\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get bouncers -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE   NAME                                          VPC    NET    IP             MAC                 DROPLET   STATUS        CREATETIME                   PROVISIONDELAY\cf1\highlight2 
\par \cf0\highlight0 default     net0-b-ab61a092-5b45-4f9f-82e7-69e23bc0135e   vpc0   net0   172.31.19.33   06:c5:e6:0f:54:9e   arktos    Provisioned   2021-12-07T08:47:55.141759   1.05588\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get dividers -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE   NAME                                          VPC    IP             MAC                 DROPLET   STATUS        CREATETIME                   PROVISIONDELAY\cf1\highlight2 
\par \cf0\highlight0 default     vpc0-d-c2c16f11-1758-44fe-8dd1-bac7096437ad   vpc0   172.31.19.33   06:c5:e6:0f:54:9e   arktos    Provisioned   2021-12-07T08:47:35.278403   0.240979\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh delete -f https://raw.githubusercontent.com/Click2Cloud-Centaurus/Documentation/main/test-yamls/test_pods.yaml\cf1\highlight2 
\par \cf0\highlight0 pod "netpod1" deleted\cf1\highlight2 
\par \cf0\highlight0 pod "netpod2" deleted\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ cat /etc/resolv.conf\cf1\highlight2 
\par \cf0\highlight0 # This file is managed by man:systemd-resolved(8). Do not edit.\cf1\highlight2 
\par \cf0\highlight0 #\cf1\highlight2 
\par \cf0\highlight0 # This is a dynamic resolv.conf file for connecting local clients to the\cf1\highlight2 
\par \cf0\highlight0 # internal DNS stub resolver of systemd-resolved. This file lists all\cf1\highlight2 
\par \cf0\highlight0 # configured search domains.\cf1\highlight2 
\par \cf0\highlight0 #\cf1\highlight2 
\par \cf0\highlight0 # Run "systemd-resolve --status" to see details about the uplink DNS servers\cf1\highlight2 
\par \cf0\highlight0 # currently in use.\cf1\highlight2 
\par \cf0\highlight0 #\cf1\highlight2 
\par \cf0\highlight0 # Third party programs must not access this file directly, but only through the\cf1\highlight2 
\par \cf0\highlight0 # symlink at /etc/resolv.conf. To manage man:resolv.conf(5) in a different way,\cf1\highlight2 
\par \cf0\highlight0 # replace this symlink by a static file or a different symlink.\cf1\highlight2 
\par \cf0\highlight0 #\cf1\highlight2 
\par \cf0\highlight0 # See man:systemd-resolved.service(8) for details about the supported modes of\cf1\highlight2 
\par \cf0\highlight0 # operation for /etc/resolv.conf.\cf1\highlight2 
\par 
\par \cf0\highlight0 nameserver 127.0.0.53\cf1\highlight2 
\par \cf0\highlight0 options edns0\cf1\highlight2 
\par \cf0\highlight0 search us-east-2.compute.internal\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo wget  https://raw.githubusercontent.com/Click2Cloud-Centaurus/Documentation/main/test-yamls/test_pods.yaml\cf1\highlight2 
\par \cf0\highlight0 --2021-12-07 09:30:05--  https://raw.githubusercontent.com/Click2Cloud-Centaurus/Documentation/main/test-yamls/test_pods.yaml\cf1\highlight2 
\par \cf0\highlight0 Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\cf1\highlight2 
\par \cf0\highlight0 Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\cf1\highlight2 
\par \cf0\highlight0 HTTP request sent, awaiting response... 200 OK\cf1\highlight2 
\par \cf0\highlight0 Length: 499 [text/plain]\cf1\highlight2 
\par \cf0\highlight0\f1 Saving to: \lquote test_pods.yaml\rquote\cf1\highlight2\f0 
\par 
\par \cf0\highlight0 test_pods.yaml                     100%[=============================================================>]     499  --.-KB/s    in 0s\cf1\highlight2 
\par 
\par \cf0\highlight0\f1 2021-12-07 09:30:05 (26.3 MB/s) - \lquote test_pods.yaml\rquote  saved [499/499]\cf1\highlight2\f0 
\par 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ ls\cf1\highlight2 
\par \cf7\highlight0 BUILD.bazel\cf0       \cf7 Makefile\cf0                   \cf7 WORKSPACE\cf0             \cf8 build\cf0                go.mod      \cf8 perf-tests\cf0         \cf8 staging\cf0          \cf8 vendor\cf1\highlight2 
\par \cf8\highlight0 CHANGELOG\cf0         \cf7 Makefile.generated_files\cf0   \cf8 _output\cf0               \cf8 cluster\cf0              go.sum      \cf8 pkg\cf0                \cf8 test\cf1\highlight2 
\par \cf0\highlight0 CONTRIBUTING.md  OWNERS                    \cf8 api\cf0                   \cf8 cmd\cf0                  \cf8 hack\cf0         \cf8 plugin\cf0             test_pods.yaml\cf1\highlight2 
\par \cf8\highlight0 Godeps\cf0            README.md                 aws_build_version    code-of-conduct.md  \cf8 logo\cf0         pod.yaml          \cf8 third_party\cf1\highlight2 
\par \cf0\highlight0 LICENSE          SUPPORT.md                aws_kube_params.inc  \cf8 docs\cf0                 mkdocs.yml  readthedocs.yaml  \cf8 translations\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ cat test_pods.yaml\cf1\highlight2 
\par \cf0\highlight0 apiVersion: v1\cf1\highlight2 
\par \cf0\highlight0 kind: Pod\cf1\highlight2 
\par \cf0\highlight0 metadata:\cf1\highlight2 
\par \cf0\highlight0   name: netpod1\cf1\highlight2 
\par \cf0\highlight0   labels:\cf1\highlight2 
\par \cf0\highlight0     podkey: netpodkey1\cf1\highlight2 
\par \cf0\highlight0 spec:\cf1\highlight2 
\par \cf0\highlight0   restartPolicy: OnFailure\cf1\highlight2 
\par \cf0\highlight0   terminationGracePeriodSeconds: 2\cf1\highlight2 
\par \cf0\highlight0   containers:\cf1\highlight2 
\par \cf0\highlight0     - name: netctr\cf1\highlight2 
\par \cf0\highlight0       image: yuvalif/fedora-tcpdump\cf1\highlight2 
\par \cf0\highlight0       command: ["tail", "-f", "/dev/null"]\cf1\highlight2 
\par \cf0\highlight0 ---\cf1\highlight2 
\par \cf0\highlight0 apiVersion: v1\cf1\highlight2 
\par \cf0\highlight0 kind: Pod\cf1\highlight2 
\par \cf0\highlight0 metadata:\cf1\highlight2 
\par \cf0\highlight0   name: netpod2\cf1\highlight2 
\par \cf0\highlight0 spec:\cf1\highlight2 
\par \cf0\highlight0   restartPolicy: OnFailure\cf1\highlight2 
\par \cf0\highlight0   terminationGracePeriodSeconds: 2\cf1\highlight2 
\par \cf0\highlight0   containers:\cf1\highlight2 
\par \cf0\highlight0     - name: netctr\cf1\highlight2 
\par \cf0\highlight0       image: yuvalif/fedora-tcpdump\cf1\highlight2 
\par \cf0\highlight0       command: ["tail", "-f", "/dev/null"]\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ ./cluster/kubectl.sh  apply -f test_pods.yaml\cf1\highlight2 
\par \cf0\highlight0 ./cluster/../cluster/../cluster/gce/../../cluster/gce/../../cluster/common.sh: line 30: /root/go/src/k8s.io/arktos/hack/lib/util.sh: Permission denied\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh  apply -f test_pods.yaml\cf1\highlight2 
\par \cf0\highlight0 pod/netpod1 created\cf1\highlight2 
\par \cf0\highlight0 pod/netpod2 created\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 'NAMESPACE     NAME                              HASHKEY               READY   STATUS              RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-c5jrs                5433553103926506877   1/1     Running             0          44m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-tkx5m   4252175553422586693   1/1     Running             0          44m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod1                           8032595380941552862   0/1     ContainerCreating   0          14s   <none>         arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod2                           2590598396426926653   0/1     ContainerCreating   0          14s   <none>         arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-dgwpx         1606520876123553529   3/3     Running             0          44m   20.0.0.2       arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   virtlet-g97z2                     2296479715811486546   0/3     Init:0/1            0          44m   172.31.19.33   arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ watch sudo ./cluster/kubectl.sh  apply -f test_pods.yaml\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ watch sudo ./cluster/kubectl.sh  get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh delete pod virtlet-g97z2\cf1\highlight2 
\par \cf0\highlight0 Error from server (NotFound): pods "virtlet-g97z2" not found\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh delete pod virtlet-g97z2 -n kube-system\cf1\highlight2 
\par \cf0\highlight0 pod "virtlet-g97z2" deleted\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ watch sudo ./cluster/kubectl.sh  get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh  get events\cf1\highlight2 
\par \cf0\highlight0 LAST SEEN   TYPE      REASON                           OBJECT                                                       MESSAGE\cf1\highlight2 
\par \cf0\highlight0 47m         Normal    Starting                         node/arktos                                                  Starting kube-proxy.\cf1\highlight2 
\par \cf0\highlight0 47m         Normal    Starting                         node/arktos                                                  Starting kubelet.\cf1\highlight2 
\par \cf0\highlight0 47m         Normal    VmRuntimeReady is not ready      node/arktos                                                  Node arktos status is now: VmRuntimeReady is not ready\cf1\highlight2 
\par \cf0\highlight0 47m         Normal    ContainerRuntimeReady is ready   node/arktos                                                  Node arktos status is now: ContainerRuntimeReady is ready\cf1\highlight2 
\par \cf0\highlight0 47m         Normal    NodeHasSufficientMemory          node/arktos                                                  Node arktos status is now: NodeHasSufficientMemory\cf1\highlight2 
\par \cf0\highlight0 47m         Normal    NodeHasNoDiskPressure            node/arktos                                                  Node arktos status is now: NodeHasNoDiskPressure\cf1\highlight2 
\par \cf0\highlight0 47m         Normal    NodeHasSufficientPID             node/arktos                                                  Node arktos status is now: NodeHasSufficientPID\cf1\highlight2 
\par \cf0\highlight0 47m         Normal    NodeAllocatableEnforced          node/arktos                                                  Updated Node Allocatable limit across pods\cf1\highlight2 
\par \cf0\highlight0 47m         Normal    NodeReady                        node/arktos                                                  Node arktos status is now: NodeReady\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    RegisteredNode                   node/arktos                                                  Node arktos event: Registered Node arktos in Controller\cf1\highlight2 
\par \cf0\highlight0 2m8s        Warning   ImageGCFailed                    node/arktos                                                  rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /run/virtlet.sock: connect: no such file or directory"\cf1\highlight2 
\par \cf0\highlight0 2m41s       Normal    Logging                          endpoint/netpod2-default-system-eth0                         Updating is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 46m         Error     Logging                          subnet/net0                                                  Handler 'net_opr_on_net_init' failed temporarily: Temporary Error: Task: NetCreate Net: net0 No droplets available.\cf1\highlight2 
\par \cf0\highlight0 5m9s        Normal    Logging                          endpoint/netpod1-default-system-eth0                         Deletion is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 12m         Normal    Logging                          endpoint/netpod2-default-system-eth0                         Updating is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 46m         Error     Logging                          subnet/net0                                                  Handler 'net_opr_on_net_init' failed temporarily: Temporary Error: Task: NetCreate Net: net0 No droplets available.\cf1\highlight2 
\par \cf0\highlight0 24m         Normal    Logging                          pod/netpod2                                                  Handler 'builtins_on_pod' succeeded.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          divider/vpc0-d-c2c16f11-1758-44fe-8dd1-bac7096437ad          Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 5m9s        Normal    Logging                          endpoint/netpod1-default-system-eth0                         Handler 'endpoint_opr_on_endpoint_delete' succeeded.\cf1\highlight2 
\par \cf0\highlight0 12m         Normal    Logging                          endpoint/netpod1-default-system-eth0                         Deletion is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          vpc/vpc0                                                     Handler 'vpc_opr_on_vpc_provisioned' succeeded.\cf1\highlight2 
\par \cf0\highlight0 24m         Normal    Logging                          pod/netpod2                                                  Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 2m42s       Normal    Logging                          pod/netpod1                                                  Handler 'builtins_on_pod' succeeded.\cf1\highlight2 
\par \cf0\highlight0 24m         Normal    Logging                          endpoint/netpod1-default-system-eth0                         Handler 'endpoint_opr_on_endpoint_provisioned' succeeded.\cf1\highlight2 
\par \cf0\highlight0 12m         Normal    Logging                          pod/netpod2                                                  Deletion is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 12m         Normal    Logging                          endpoint/netpod2-default-system-eth0                         Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 46m         Error     Logging                          pod/mizar-daemon-c5jrs                                       Handler 'builtins_on_pod' failed temporarily: Temporary Error: Droplet not yet created.\cf1\highlight2 
\par \cf0\highlight0 5m9s        Normal    Logging                          endpoint/netpod2-default-system-eth0                         Deletion is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 45m         Normal    Logging                          subnet/net0                                                  Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 45m         Normal    Logging                          bouncer/net0-b-ab61a092-5b45-4f9f-82e7-69e23bc0135e          Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          vpc/vpc0                                                     Updating is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 5m9s        Normal    Logging                          pod/netpod1                                                  Deletion is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 46m         Error     Logging                          endpoints/kubernetes                                         Handler 'services_opr_on_endpoints' failed temporarily: Temporary Error: Task: k8sEndpointsUpdate Endpoint: kubernetes-default bouncers not yet provisioned.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          endpoint/kubernetes-default                                  Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 45m         Normal    Logging                          endpoint/virtlet-g97z2-kube-system-system-eth0               Handler 'endpoint_opr_on_endpoint_init' succeeded.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          pod/mizar-daemon-c5jrs                                       Handler 'builtins_on_pod' succeeded.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          endpoint/kube-dns-kube-system                                Handler 'endpoint_opr_on_endpoint_init' succeeded.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          endpoint/kubernetes-default                                  Updating is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 46m         Error     Logging                          pod/mizar-daemon-c5jrs                                       Handler 'builtins_on_pod' failed temporarily: Temporary Error: Droplet not yet created.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          droplet/arktos                                               Updating is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 45m         Normal    Logging                          subnet/net0                                                  Handler 'net_opr_on_net_init' succeeded.\cf1\highlight2 
\par \cf0\highlight0 46m         Error     Logging                          vpc/vpc0                                                     Handler 'vpc_opr_on_vpc_init' failed temporarily: Temporary Error: Task: VpcCreate VPC: vpc0 No droplets available.\cf1\highlight2 
\par \cf0\highlight0 5m9s        Normal    Logging                          pod/netpod2                                                  Deletion is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 46m         Error     Logging                          vpc/vpc0                                                     Handler 'vpc_opr_on_vpc_init' failed temporarily: Temporary Error: Task: VpcCreate VPC: vpc0 No droplets available.\cf1\highlight2 
\par \cf0\highlight0 12m         Normal    Logging                          pod/netpod1                                                  Deletion is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 2m41s       Normal    Logging                          endpoint/netpod1-default-system-eth0                         Updating is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 12m         Normal    Logging                          pod/netpod1                                                  Handler 'builtins_on_pod' succeeded.\cf1\highlight2 
\par \cf0\highlight0 12m         Normal    Logging                          pod/netpod2                                                  Handler 'builtins_on_pod' succeeded.\cf1\highlight2 
\par \cf0\highlight0 46m         Error     Logging                          node/arktos                                                  Handler 'droplet_opr_on_node' failed temporarily: Temporary Error: Daemon not ready.\cf1\highlight2 
\par \cf0\highlight0 2m41s       Normal    Logging                          endpoint/netpod1-default-system-eth0                         Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 45m         Normal    Logging                          endpoint/virtlet-g97z2-kube-system-system-eth0               Handler 'endpoint_opr_on_endpoint_provisioned' succeeded.\cf1\highlight2 
\par \cf0\highlight0 24m         Normal    Logging                          endpoint/netpod1-default-system-eth0                         Updating is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 24m         Normal    Logging                          pod/netpod1                                                  Handler 'builtins_on_pod' succeeded.\cf1\highlight2 
\par \cf0\highlight0 45m         Normal    Logging                          subnet/net0                                                  Handler 'net_opr_on_net_provisioned' succeeded.\cf1\highlight2 
\par \cf0\highlight0 46s         Normal    Logging                          endpoint/virtlet-g97z2-kube-system-system-eth0               Handler 'endpoint_opr_on_endpoint_delete' succeeded.\cf1\highlight2 
\par \cf0\highlight0 46m         Error     Logging                          node/arktos                                                  Handler 'droplet_opr_on_node' failed temporarily: Temporary Error: Daemon not ready.\cf1\highlight2 
\par \cf0\highlight0 24m         Normal    Logging                          endpoint/netpod1-default-system-eth0                         Handler 'endpoint_opr_on_endpoint_init' succeeded.\cf1\highlight2 
\par \cf0\highlight0 12m         Normal    Logging                          endpoint/netpod1-default-system-eth0                         Updating is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 5m9s        Normal    Logging                          pod/netpod2                                                  Handler 'builtins_on_pod_delete' succeeded.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          vpc/vpc0                                                     Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 12m         Normal    Logging                          endpoint/netpod1-default-system-eth0                         Handler 'endpoint_opr_on_endpoint_delete' succeeded.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          namespace/kube-node-lease                                    Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 46m         Error     Logging                          endpoints/kubernetes                                         Handler 'services_opr_on_endpoints' failed temporarily: Temporary Error: Task: k8sEndpointsUpdate Endpoint: kubernetes-default bouncers not yet provisioned.\cf1\highlight2 
\par \cf0\highlight0 46m         Error     Logging                          pod/mizar-operator-6b78d7ffc4-tkx5m                          Handler 'builtins_on_pod' failed temporarily: Temporary Error: Droplet not yet created.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          endpoint/kube-dns-kube-system                                Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 2m42s       Normal    Logging                          pod/netpod1                                                  Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          namespace/default                                            Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          node/arktos                                                  Handler 'droplet_opr_on_node' succeeded.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          node/arktos                                                  Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 45m         Normal    Logging                          endpoint/virtlet-g97z2-kube-system-system-eth0               Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 5m9s        Normal    Logging                          endpoint/netpod2-default-system-eth0                         Handler 'endpoint_opr_on_endpoint_delete' succeeded.\cf1\highlight2 
\par \cf0\highlight0 24m         Normal    Logging                          endpoint/netpod2-default-system-eth0                         Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 12m         Normal    Logging                          endpoint/netpod1-default-system-eth0                         Handler 'endpoint_opr_on_endpoint_init' succeeded.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          divider/vpc0-d-c2c16f11-1758-44fe-8dd1-bac7096437ad          Handler 'divider_opr_on_divider_provisioned' succeeded.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          droplet/arktos                                               Handler 'droplet_opr_on_droplet_provisioned' succeeded.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          namespace/kube-public                                        Handler 'builtins_on_namespace' succeeded.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          divider/vpc0-d-c2c16f11-1758-44fe-8dd1-bac7096437ad          Handler 'divider_opr_on_divider_init' succeeded.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          endpoint/arktos-default--hostep                              Handler 'endpoint_opr_on_endpoint_init' succeeded.\cf1\highlight2 
\par \cf0\highlight0 24m         Normal    Logging                          endpoint/netpod2-default-system-eth0                         Handler 'endpoint_opr_on_endpoint_provisioned' succeeded.\cf1\highlight2 
\par \cf0\highlight0 46m         Error     Logging                          endpoints/kubernetes                                         Handler 'services_opr_on_endpoints' failed temporarily: Temporary Error: Task: k8sEndpointsUpdate Endpoint: kubernetes-default bouncers not yet provisioned.\cf1\highlight2 
\par \cf0\highlight0 2m41s       Normal    Logging                          endpoint/netpod2-default-system-eth0                         Handler 'endpoint_opr_on_endpoint_init' succeeded.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          endpoint/kube-dns-kube-system                                Updating is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          divider/vpc0-d-c2c16f11-1758-44fe-8dd1-bac7096437ad          Updating is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 12m         Normal    Logging                          pod/netpod1                                                  Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          endpoint/kube-dns-554c5866fc-dgwpx-kube-system-system-eth0   Updating is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 46m         Error     Logging                          subnet/net0                                                  Handler 'net_opr_on_net_init' failed temporarily: Temporary Error: Task: NetCreate Net: net0 No droplets available.\cf1\highlight2 
\par \cf0\highlight0 45m         Normal    Logging                          pod/mizar-operator-6b78d7ffc4-tkx5m                          Handler 'builtins_on_pod' succeeded.\cf1\highlight2 
\par \cf0\highlight0 2m41s       Normal    Logging                          endpoint/netpod2-default-system-eth0                         Handler 'endpoint_opr_on_endpoint_provisioned' succeeded.\cf1\highlight2 
\par \cf0\highlight0 2m41s       Normal    Logging                          endpoint/netpod1-default-system-eth0                         Handler 'endpoint_opr_on_endpoint_init' succeeded.\cf1\highlight2 
\par \cf0\highlight0 24m         Normal    Logging                          endpoint/netpod1-default-system-eth0                         Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 12m         Normal    Logging                          endpoint/netpod2-default-system-eth0                         Handler 'endpoint_opr_on_endpoint_provisioned' succeeded.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          namespace/kube-public                                        Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          pod/mizar-daemon-c5jrs                                       Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          service/kubernetes                                           Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          endpoint/kube-dns-554c5866fc-dgwpx-kube-system-system-eth0   Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 12m         Normal    Logging                          endpoint/netpod2-default-system-eth0                         Handler 'endpoint_opr_on_endpoint_delete' succeeded.\cf1\highlight2 
\par \cf0\highlight0 24m         Normal    Logging                          endpoint/netpod2-default-system-eth0                         Updating is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          droplet/arktos                                               Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 45m         Normal    Logging                          endpoints/kubernetes                                         Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          service/kubernetes                                           Handler 'services_opr_on_services' succeeded.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          endpoint/kube-dns-554c5866fc-dgwpx-kube-system-system-eth0   Handler 'endpoint_opr_on_endpoint_init' succeeded.\cf1\highlight2 
\par \cf0\highlight0 46m         Error     Logging                          pod/mizar-operator-6b78d7ffc4-tkx5m                          Handler 'builtins_on_pod' failed temporarily: Temporary Error: Droplet not yet created.\cf1\highlight2 
\par \cf0\highlight0 12m         Normal    Logging                          pod/netpod1                                                  Handler 'builtins_on_pod_delete' succeeded.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          endpoint/kube-dns-kube-system                                Handler 'endpoint_opr_on_endpoint_provisioned' succeeded.\cf1\highlight2 
\par \cf0\highlight0 2m42s       Normal    Logging                          pod/netpod2                                                  Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          endpoint/arktos-default--hostep                              Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 45m         Normal    Logging                          endpoint/virtlet-g97z2-kube-system-system-eth0               Updating is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 45m         Normal    Logging                          bouncer/net0-b-ab61a092-5b45-4f9f-82e7-69e23bc0135e          Updating is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          namespace/default                                            Handler 'builtins_on_namespace' succeeded.\cf1\highlight2 
\par \cf0\highlight0 12m         Normal    Logging                          endpoint/netpod2-default-system-eth0                         Handler 'endpoint_opr_on_endpoint_init' succeeded.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          endpoint/arktos-default--hostep                              Updating is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          namespace/kube-system                                        Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 12m         Normal    Logging                          endpoint/netpod1-default-system-eth0                         Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 12m         Normal    Logging                          endpoint/netpod1-default-system-eth0                         Handler 'endpoint_opr_on_endpoint_provisioned' succeeded.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          endpoint/arktos-default--hostep                              Handler 'endpoint_opr_on_endpoint_provisioned' succeeded.\cf1\highlight2 
\par \cf0\highlight0 45m         Normal    Logging                          subnet/net0                                                  Updating is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 45m         Normal    Logging                          bouncer/net0-b-ab61a092-5b45-4f9f-82e7-69e23bc0135e          Handler 'bouncer_opr_on_bouncer_init' succeeded.\cf1\highlight2 
\par \cf0\highlight0 2m41s       Normal    Logging                          endpoint/netpod2-default-system-eth0                         Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 45m         Normal    Logging                          pod/mizar-operator-6b78d7ffc4-tkx5m                          Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 24m         Normal    Logging                          endpoint/netpod2-default-system-eth0                         Handler 'endpoint_opr_on_endpoint_init' succeeded.\cf1\highlight2 
\par \cf0\highlight0 45m         Normal    Logging                          endpoints/kubernetes                                         Handler 'services_opr_on_endpoints' succeeded.\cf1\highlight2 
\par \cf0\highlight0 46m         Error     Logging                          pod/mizar-operator-6b78d7ffc4-tkx5m                          Handler 'builtins_on_pod' failed temporarily: Temporary Error: Droplet not yet created.\cf1\highlight2 
\par \cf0\highlight0 12m         Normal    Logging                          endpoint/netpod2-default-system-eth0                         Deletion is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 45m         Normal    Logging                          bouncer/net0-b-ab61a092-5b45-4f9f-82e7-69e23bc0135e          Handler 'bouncer_opr_on_bouncer_provisioned' succeeded.\cf1\highlight2 
\par \cf0\highlight0 12m         Normal    Logging                          pod/netpod2                                                  Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 46s         Normal    Logging                          endpoint/virtlet-g97z2-kube-system-system-eth0               Deletion is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 2m42s       Normal    Logging                          pod/netpod2                                                  Handler 'builtins_on_pod' succeeded.\cf1\highlight2 
\par \cf0\highlight0 24m         Normal    Logging                          pod/netpod1                                                  Creation is processed: 1 succeeded; 0 failed.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          namespace/kube-node-lease                                    Handler 'builtins_on_namespace' succeeded.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          endpoint/kube-dns-554c5866fc-dgwpx-kube-system-system-eth0   Handler 'endpoint_opr_on_endpoint_provisioned' succeeded.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          endpoint/kubernetes-default                                  Handler 'endpoint_opr_on_endpoint_provisioned' succeeded.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          vpc/vpc0                                                     Handler 'vpc_opr_on_vpc_init' succeeded.\cf1\highlight2 
\par \cf0\highlight0 2m41s       Normal    Logging                          endpoint/netpod1-default-system-eth0                         Handler 'endpoint_opr_on_endpoint_provisioned' succeeded.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          endpoint/kubernetes-default                                  Handler 'endpoint_opr_on_endpoint_init' succeeded.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          namespace/kube-system                                        Handler 'builtins_on_namespace' succeeded.\cf1\highlight2 
\par \cf0\highlight0 5m10s       Normal    Logging                          pod/netpod1                                                  Handler 'builtins_on_pod_delete' succeeded.\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Logging                          droplet/arktos                                               Handler 'droplet_opr_on_droplet_init' succeeded.\cf1\highlight2 
\par \cf0\highlight0 12m         Normal    Logging                          pod/netpod2                                                  Handler 'builtins_on_pod_delete' succeeded.\cf1\highlight2 
\par \cf0\highlight0 <unknown>   Normal    Scheduled                        pod/mizar-daemon-c5jrs                                       Successfully assigned default/mizar-daemon-c5jrs to arktos\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Pulled                           pod/mizar-daemon-c5jrs                                       Container image "mizarnet/mizar:0.9" already present on machine\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Created                          pod/mizar-daemon-c5jrs                                       Created container node-init\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Started                          pod/mizar-daemon-c5jrs                                       Started container node-init\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Pulled                           pod/mizar-daemon-c5jrs                                       Container image "mizarnet/dropletd:0.9" already present on machine\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Created                          pod/mizar-daemon-c5jrs                                       Created container mizar-daemon\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Started                          pod/mizar-daemon-c5jrs                                       Started container mizar-daemon\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    SuccessfulCreate                 daemonset/mizar-daemon                                       Created pod: mizar-daemon-c5jrs\cf1\highlight2 
\par \cf0\highlight0 <unknown>   Normal    Scheduled                        pod/mizar-operator-6b78d7ffc4-tkx5m                          Successfully assigned default/mizar-operator-6b78d7ffc4-tkx5m to arktos\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Pulled                           pod/mizar-operator-6b78d7ffc4-tkx5m                          Container image "mizarnet/endpointopr:0.9" already present on machine\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Created                          pod/mizar-operator-6b78d7ffc4-tkx5m                          Created container mizar-operator\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    Started                          pod/mizar-operator-6b78d7ffc4-tkx5m                          Started container mizar-operator\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    SuccessfulCreate                 replicaset/mizar-operator-6b78d7ffc4                         Created pod: mizar-operator-6b78d7ffc4-tkx5m\cf1\highlight2 
\par \cf0\highlight0 46m         Normal    ScalingReplicaSet                deployment/mizar-operator                                    Scaled up replica set mizar-operator-6b78d7ffc4 to 1\cf1\highlight2 
\par \cf0\highlight0 <unknown>   Normal    Scheduled                        pod/netpod1                                                  Successfully assigned default/netpod1 to arktos\cf1\highlight2 
\par \cf0\highlight0 14m         Warning   FailedCreatePodSandBox           pod/netpod1                                                  Failed create pod sandbox: failed to get DNS IP address: network system/default not found\cf1\highlight2 
\par \cf0\highlight0 <unknown>   Normal    Scheduled                        pod/netpod1                                                  Successfully assigned default/netpod1 to arktos\cf1\highlight2 
\par \cf0\highlight0 6m56s       Warning   FailedCreatePodSandBox           pod/netpod1                                                  Failed create pod sandbox: failed to get DNS IP address: network system/default not found\cf1\highlight2 
\par \cf0\highlight0 <unknown>   Normal    Scheduled                        pod/netpod1                                                  Successfully assigned default/netpod1 to arktos\cf1\highlight2 
\par \cf0\highlight0 10s         Warning   FailedCreatePodSandBox           pod/netpod1                                                  Failed create pod sandbox: failed to get DNS IP address: network system/default not found\cf1\highlight2 
\par \cf0\highlight0 <unknown>   Normal    Scheduled                        pod/netpod2                                                  Successfully assigned default/netpod2 to arktos\cf1\highlight2 
\par \cf0\highlight0 14m         Warning   FailedCreatePodSandBox           pod/netpod2                                                  Failed create pod sandbox: failed to get DNS IP address: network system/default not found\cf1\highlight2 
\par \cf0\highlight0 <unknown>   Normal    Scheduled                        pod/netpod2                                                  Successfully assigned default/netpod2 to arktos\cf1\highlight2 
\par \cf0\highlight0 6m55s       Warning   FailedCreatePodSandBox           pod/netpod2                                                  Failed create pod sandbox: failed to get DNS IP address: network system/default not found\cf1\highlight2 
\par \cf0\highlight0 <unknown>   Normal    Scheduled                        pod/netpod2                                                  Successfully assigned default/netpod2 to arktos\cf1\highlight2 
\par \cf0\highlight0 13s         Warning   FailedCreatePodSandBox           pod/netpod2                                                  Failed create pod sandbox: failed to get DNS IP address: network system/default not found\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo ./cluster/kubectl.sh  get net\cf1\highlight2 
\par \cf0\highlight0 NAME      TYPE    VPC                      PHASE   DNS\cf1\highlight2 
\par \cf0\highlight0 default   mizar   system-default-network\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo vim ./hack/arktos-\cf1\highlight2 
\par \cf0\highlight0 arktos-apiserver-partition.sh  arktos-onebox.rc               arktos-up.sh\cf1\highlight2 
\par \cf0\highlight0 arktos-cni.rc                  arktos-up-scale-out-poc.sh     arktos-worker-up.sh\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo vim ./hack/arktos-up.sh\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo cat /tmp/arktos-network-controller.log\cf1\highlight2 
\par \cf0\highlight0 F1207 08:46:33.387499     472 network-controller.go:62] --kube-apiserver-ip must be the valid ip address of kube-apiserver.\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ ip a\cf1\highlight2 
\par \cf0\highlight0 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\cf1\highlight2 
\par \cf0\highlight0     inet 127.0.0.1/8 scope host lo\cf1\highlight2 
\par \cf0\highlight0        valid_lft forever preferred_lft forever\cf1\highlight2 
\par \cf0\highlight0     inet6 ::1/128 scope host\cf1\highlight2 
\par \cf0\highlight0        valid_lft forever preferred_lft forever\cf1\highlight2 
\par \cf0\highlight0 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9000 xdpgeneric/id:125 qdisc mq state UP group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/ether 06:c5:e6:0f:54:9e brd ff:ff:ff:ff:ff:ff\cf1\highlight2 
\par \cf0\highlight0     inet 172.31.19.33/20 brd 172.31.31.255 scope global dynamic eth0\cf1\highlight2 
\par \cf0\highlight0        valid_lft 3509sec preferred_lft 3509sec\cf1\highlight2 
\par \cf0\highlight0     inet6 fe80::4c5:e6ff:fe0f:549e/64 scope link\cf1\highlight2 
\par \cf0\highlight0        valid_lft forever preferred_lft forever\cf1\highlight2 
\par \cf0\highlight0 3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default\cf1\highlight2 
\par \cf0\highlight0     link/ether 02:42:df:32:10:f2 brd ff:ff:ff:ff:ff:ff\cf1\highlight2 
\par \cf0\highlight0     inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0\cf1\highlight2 
\par \cf0\highlight0        valid_lft forever preferred_lft forever\cf1\highlight2 
\par \cf0\highlight0 4: veth-6bb914a6@if5: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9000 xdpgeneric/id:129 qdisc noqueue state UP group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/ether d2:3d:74:15:1d:77 brd ff:ff:ff:ff:ff:ff link-netnsid 0\cf1\highlight2 
\par \cf0\highlight0     inet6 fe80::d03d:74ff:fe15:1d77/64 scope link\cf1\highlight2 
\par \cf0\highlight0        valid_lft forever preferred_lft forever\cf1\highlight2 
\par \cf0\highlight0 6: veth-hostep@eth-hostep: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9000 xdpgeneric/id:145 qdisc noqueue state UP group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/ether 46:37:a8:25:c3:d2 brd ff:ff:ff:ff:ff:ff\cf1\highlight2 
\par \cf0\highlight0     inet6 fe80::4437:a8ff:fe25:c3d2/64 scope link\cf1\highlight2 
\par \cf0\highlight0        valid_lft forever preferred_lft forever\cf1\highlight2 
\par \cf0\highlight0 7: eth-hostep@veth-hostep: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/ether ea:61:28:ed:07:65 brd ff:ff:ff:ff:ff:ff\cf1\highlight2 
\par \cf0\highlight0     inet 172.31.19.33/32 scope global eth-hostep\cf1\highlight2 
\par \cf0\highlight0        valid_lft forever preferred_lft forever\cf1\highlight2 
\par \cf0\highlight0     inet6 fe80::e861:28ff:feed:765/64 scope link\cf1\highlight2 
\par \cf0\highlight0        valid_lft forever preferred_lft forever\cf1\highlight2 
\par \cf0\highlight0 8: veth-cf848e77@eth-cf848e77: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/ether da:a7:fc:81:c4:d2 brd ff:ff:ff:ff:ff:ff\cf1\highlight2 
\par \cf0\highlight0 9: eth-cf848e77@veth-cf848e77: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/ether 76:97:ca:9c:6f:31 brd ff:ff:ff:ff:ff:ff\cf1\highlight2 
\par \cf0\highlight0 18: veth-3f51c38f@eth-3f51c38f: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/ether 76:0e:09:31:af:1a brd ff:ff:ff:ff:ff:ff\cf1\highlight2 
\par \cf0\highlight0 19: eth-3f51c38f@veth-3f51c38f: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/ether 6e:80:77:63:f7:ce brd ff:ff:ff:ff:ff:ff\cf1\highlight2 
\par \cf0\highlight0 20: veth-996cbd4d@eth-996cbd4d: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/ether b2:c5:86:0e:0f:ef brd ff:ff:ff:ff:ff:ff\cf1\highlight2 
\par \cf0\highlight0 21: eth-996cbd4d@veth-996cbd4d: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/ether 7a:9b:7b:72:23:fa brd ff:ff:ff:ff:ff:ff\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ ip address\cf1\highlight2 
\par \cf0\highlight0 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\cf1\highlight2 
\par \cf0\highlight0     inet 127.0.0.1/8 scope host lo\cf1\highlight2 
\par \cf0\highlight0        valid_lft forever preferred_lft forever\cf1\highlight2 
\par \cf0\highlight0     inet6 ::1/128 scope host\cf1\highlight2 
\par \cf0\highlight0        valid_lft forever preferred_lft forever\cf1\highlight2 
\par \cf0\highlight0 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9000 xdpgeneric/id:125 qdisc mq state UP group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/ether 06:c5:e6:0f:54:9e brd ff:ff:ff:ff:ff:ff\cf1\highlight2 
\par \cf0\highlight0     inet 172.31.19.33/20 brd 172.31.31.255 scope global dynamic eth0\cf1\highlight2 
\par \cf0\highlight0        valid_lft 3494sec preferred_lft 3494sec\cf1\highlight2 
\par \cf0\highlight0     inet6 fe80::4c5:e6ff:fe0f:549e/64 scope link\cf1\highlight2 
\par \cf0\highlight0        valid_lft forever preferred_lft forever\cf1\highlight2 
\par \cf0\highlight0 3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default\cf1\highlight2 
\par \cf0\highlight0     link/ether 02:42:df:32:10:f2 brd ff:ff:ff:ff:ff:ff\cf1\highlight2 
\par \cf0\highlight0     inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0\cf1\highlight2 
\par \cf0\highlight0        valid_lft forever preferred_lft forever\cf1\highlight2 
\par \cf0\highlight0 4: veth-6bb914a6@if5: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9000 xdpgeneric/id:129 qdisc noqueue state UP group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/ether d2:3d:74:15:1d:77 brd ff:ff:ff:ff:ff:ff link-netnsid 0\cf1\highlight2 
\par \cf0\highlight0     inet6 fe80::d03d:74ff:fe15:1d77/64 scope link\cf1\highlight2 
\par \cf0\highlight0        valid_lft forever preferred_lft forever\cf1\highlight2 
\par \cf0\highlight0 6: veth-hostep@eth-hostep: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9000 xdpgeneric/id:145 qdisc noqueue state UP group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/ether 46:37:a8:25:c3:d2 brd ff:ff:ff:ff:ff:ff\cf1\highlight2 
\par \cf0\highlight0     inet6 fe80::4437:a8ff:fe25:c3d2/64 scope link\cf1\highlight2 
\par \cf0\highlight0        valid_lft forever preferred_lft forever\cf1\highlight2 
\par \cf0\highlight0 7: eth-hostep@veth-hostep: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/ether ea:61:28:ed:07:65 brd ff:ff:ff:ff:ff:ff\cf1\highlight2 
\par \cf0\highlight0     inet 172.31.19.33/32 scope global eth-hostep\cf1\highlight2 
\par \cf0\highlight0        valid_lft forever preferred_lft forever\cf1\highlight2 
\par \cf0\highlight0     inet6 fe80::e861:28ff:feed:765/64 scope link\cf1\highlight2 
\par \cf0\highlight0        valid_lft forever preferred_lft forever\cf1\highlight2 
\par \cf0\highlight0 8: veth-cf848e77@eth-cf848e77: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/ether da:a7:fc:81:c4:d2 brd ff:ff:ff:ff:ff:ff\cf1\highlight2 
\par \cf0\highlight0 9: eth-cf848e77@veth-cf848e77: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/ether 76:97:ca:9c:6f:31 brd ff:ff:ff:ff:ff:ff\cf1\highlight2 
\par \cf0\highlight0 18: veth-3f51c38f@eth-3f51c38f: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/ether 76:0e:09:31:af:1a brd ff:ff:ff:ff:ff:ff\cf1\highlight2 
\par \cf0\highlight0 19: eth-3f51c38f@veth-3f51c38f: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/ether 6e:80:77:63:f7:ce brd ff:ff:ff:ff:ff:ff\cf1\highlight2 
\par \cf0\highlight0 20: veth-996cbd4d@eth-996cbd4d: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/ether b2:c5:86:0e:0f:ef brd ff:ff:ff:ff:ff:ff\cf1\highlight2 
\par \cf0\highlight0 21: eth-996cbd4d@veth-996cbd4d: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/ether 7a:9b:7b:72:23:fa brd ff:ff:ff:ff:ff:ff\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ $(hostname -i)\cf1\highlight2 
\par \cf0\highlight0 172.31.19.33: command not found\cf1\highlight2 
\par \cf0\highlight0 ubuntu@arktos:/root/go/src/k8s.io/arktos$ sudo init 6\cf1\highlight2 
\par \cf0\highlight0 '\cf1\highlight2 
\par \cf9\highlight0 Remote side unexpectedly closed network connection\cf1\highlight2 
\par 
\par \cf10\highlight0\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\cf1\highlight2 
\par 
\par \cf9\highlight0 Session stopped\cf1\highlight2 
\par \cf0\highlight0     - Press \cf5 <return>\cf0  to exit tab\cf1\highlight2 
\par \cf0\highlight0     - Press \cf5 R\cf0  to restart session\cf1\highlight2 
\par \cf0\highlight0     - Press \cf5 S\cf0  to save terminal output to file\cf1\highlight2 
\par 
\par \cf9\highlight0 Network error: Connection refused\cf1\highlight2 
\par 
\par \cf10\highlight0\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\cf1\highlight2 
\par 
\par \cf9\highlight0 Session stopped\cf1\highlight2 
\par \cf0\highlight0     - Press \cf5 <return>\cf0  to exit tab\cf1\highlight2 
\par \cf0\highlight0     - Press \cf5 R\cf0  to restart session\cf1\highlight2 
\par \cf0\highlight0     - Press \cf5 S\cf0  to save terminal output to file\cf1\highlight2 
\par \cf0\highlight0 Authenticating with public key "Imported-Openssh-Key: C:\\\\Users\\\\prajwal.akhuj\\\\Downloads\\\\prajwal-arktos-key.pem"\cf1\highlight2 
\par \cf0\highlight0     \u9484?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9488?\cf1\highlight2 
\par \cf0\highlight0     \u9474?                 \cf3\f1\bullet  MobaXterm Personal Edition v21.4 \bullet\cf0\f0                  \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?               \cf4 (SSH client, X server and network tools)\cf0                \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?                                                                      \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474? \u8594? SSH session to \cf5 ubuntu@18.191.204.227                     \cf0           \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?\f1    \bullet  Direct SSH      :  \cf3\f0 v\cf0                                              \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?\f1    \bullet  SSH compression :  \cf3\f0 v\cf0                                              \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?\f1    \bullet  SSH-browser     :  \cf3\f0 v\cf0                                              \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?\f1    \bullet  X11-forwarding  :  \cf3\f0 v\cf0   (remote display is forwarded through SSH)  \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474?                                                                      \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9474? \u8594? For more info, ctrl+click on \cf6\ul help\cf0\ulnone  or visit our \cf6\ul website\cf0\ulnone .            \u9474?\cf1\highlight2 
\par \cf0\highlight0     \u9492?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9472?\u9496?\cf1\highlight2 
\par 
\par \cf0\highlight0 Welcome to Ubuntu 18.04.6 LTS (GNU/Linux 5.6.0-rc2 x86_64)\cf1\highlight2 
\par 
\par \cf0\highlight0  * Documentation:  https://help.ubuntu.com\cf1\highlight2 
\par \cf0\highlight0  * Management:     https://landscape.canonical.com\cf1\highlight2 
\par \cf0\highlight0  * Support:        https://ubuntu.com/advantage\cf1\highlight2 
\par 
\par \cf0\highlight0   System information as of Tue Dec  7 09:43:22 UTC 2021\cf1\highlight2 
\par 
\par \cf0\highlight0   System load:  0.56                Processes:              208\cf1\highlight2 
\par \cf0\highlight0   Usage of /:   14.1% of 116.27GB   Users logged in:        1\cf1\highlight2 
\par \cf0\highlight0   Memory usage: 1%                  IP address for eth0:    172.31.19.33\cf1\highlight2 
\par \cf0\highlight0   Swap usage:   0%                  IP address for docker0: 172.17.0.1\cf1\highlight2 
\par 
\par 
\par \cf0\highlight0 32 updates can be applied immediately.\cf1\highlight2 
\par \cf0\highlight0 25 of these updates are standard security updates.\cf1\highlight2 
\par \cf0\highlight0 To see these additional updates run: apt list --upgradable\cf1\highlight2 
\par 
\par \cf0\highlight0 New release '20.04.3 LTS' available.\cf1\highlight2 
\par \cf0\highlight0 Run 'do-release-upgrade' to upgrade to it.\cf1\highlight2 
\par 
\par 
\par \cf0\highlight0 Last login: Tue Dec  7 09:42:39 2021 from 114.143.207.106\cf1\highlight2 
\par \cf0\highlight0 ubuntu@aws-arktos:~$ hostname -i\cf1\highlight2 
\par \cf0\highlight0 172.31.19.33 172.17.0.1 fe80::4c5:e6ff:fe0f:549e\cf1\highlight2 
\par \cf0\highlight0 ubuntu@aws-arktos:~$ vi /etc/hosts\cf1\highlight2 
\par \cf0\highlight0 ubuntu@aws-arktos:~$ ip a\cf1\highlight2 
\par \cf0\highlight0 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\cf1\highlight2 
\par \cf0\highlight0     inet 127.0.0.1/8 scope host lo\cf1\highlight2 
\par \cf0\highlight0        valid_lft forever preferred_lft forever\cf1\highlight2 
\par \cf0\highlight0     inet6 ::1/128 scope host\cf1\highlight2 
\par \cf0\highlight0        valid_lft forever preferred_lft forever\cf1\highlight2 
\par \cf0\highlight0 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9000 xdpgeneric/id:45 qdisc mq state UP group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/ether 06:c5:e6:0f:54:9e brd ff:ff:ff:ff:ff:ff\cf1\highlight2 
\par \cf0\highlight0     inet 172.31.19.33/20 brd 172.31.31.255 scope global dynamic eth0\cf1\highlight2 
\par \cf0\highlight0        valid_lft 3432sec preferred_lft 3432sec\cf1\highlight2 
\par \cf0\highlight0     inet6 fe80::4c5:e6ff:fe0f:549e/64 scope link\cf1\highlight2 
\par \cf0\highlight0        valid_lft forever preferred_lft forever\cf1\highlight2 
\par \cf0\highlight0 3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default\cf1\highlight2 
\par \cf0\highlight0     link/ether 02:42:dd:3c:6c:97 brd ff:ff:ff:ff:ff:ff\cf1\highlight2 
\par \cf0\highlight0     inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0\cf1\highlight2 
\par \cf0\highlight0        valid_lft forever preferred_lft forever\cf1\highlight2 
\par \cf0\highlight0 4: veth-ffd05d78@eth-ffd05d78: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/ether ea:00:83:eb:d0:ad brd ff:ff:ff:ff:ff:ff\cf1\highlight2 
\par \cf0\highlight0 5: eth-ffd05d78@veth-ffd05d78: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/ether 9e:04:23:9c:6e:4a brd ff:ff:ff:ff:ff:ff\cf1\highlight2 
\par \cf0\highlight0 6: veth-408301e1@if7: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9000 xdpgeneric/id:49 qdisc noqueue state UP group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/ether ea:d0:0a:40:0f:c0 brd ff:ff:ff:ff:ff:ff link-netnsid 0\cf1\highlight2 
\par \cf0\highlight0     inet6 fe80::e8d0:aff:fe40:fc0/64 scope link\cf1\highlight2 
\par \cf0\highlight0        valid_lft forever preferred_lft forever\cf1\highlight2 
\par \cf0\highlight0 8: veth-hostep@eth-hostep: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9000 xdpgeneric/id:65 qdisc noqueue state UP group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/ether 52:0a:fd:6f:f9:3e brd ff:ff:ff:ff:ff:ff\cf1\highlight2 
\par \cf0\highlight0     inet6 fe80::500a:fdff:fe6f:f93e/64 scope link\cf1\highlight2 
\par \cf0\highlight0        valid_lft forever preferred_lft forever\cf1\highlight2 
\par \cf0\highlight0 9: eth-hostep@veth-hostep: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/ether 7e:db:39:77:98:dd brd ff:ff:ff:ff:ff:ff\cf1\highlight2 
\par \cf0\highlight0     inet 172.31.19.33/32 scope global eth-hostep\cf1\highlight2 
\par \cf0\highlight0        valid_lft forever preferred_lft forever\cf1\highlight2 
\par \cf0\highlight0     inet6 fe80::7cdb:39ff:fe77:98dd/64 scope link\cf1\highlight2 
\par \cf0\highlight0        valid_lft forever preferred_lft forever\cf1\highlight2 
\par \cf0\highlight0 ubuntu@aws-arktos:~$ cat\cf1\highlight2 
\par \cf0\highlight0 ^C\cf1\highlight2 
\par \cf0\highlight0 ubuntu@aws-arktos:~$ cat /tmp/arktos-network-controller.log\cf1\highlight2 
\par \cf0\highlight0 F1207 09:43:31.868325    5221 network-controller.go:62] --kube-apiserver-ip must be the valid ip address of kube-apiserver.\cf1\highlight2 
\par \cf0\highlight0 ubuntu@aws-arktos:~$ ip a\cf1\highlight2 
\par \cf0\highlight0 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\cf1\highlight2 
\par \cf0\highlight0     inet 127.0.0.1/8 scope host lo\cf1\highlight2 
\par \cf0\highlight0        valid_lft forever preferred_lft forever\cf1\highlight2 
\par \cf0\highlight0     inet6 ::1/128 scope host\cf1\highlight2 
\par \cf0\highlight0        valid_lft forever preferred_lft forever\cf1\highlight2 
\par \cf0\highlight0 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9000 xdpgeneric/id:45 qdisc mq state UP group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/ether 06:c5:e6:0f:54:9e brd ff:ff:ff:ff:ff:ff\cf1\highlight2 
\par \cf0\highlight0     inet 172.31.19.33/20 brd 172.31.31.255 scope global dynamic eth0\cf1\highlight2 
\par \cf0\highlight0        valid_lft 3377sec preferred_lft 3377sec\cf1\highlight2 
\par \cf0\highlight0     inet6 fe80::4c5:e6ff:fe0f:549e/64 scope link\cf1\highlight2 
\par \cf0\highlight0        valid_lft forever preferred_lft forever\cf1\highlight2 
\par \cf0\highlight0 3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default\cf1\highlight2 
\par \cf0\highlight0     link/ether 02:42:dd:3c:6c:97 brd ff:ff:ff:ff:ff:ff\cf1\highlight2 
\par \cf0\highlight0     inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0\cf1\highlight2 
\par \cf0\highlight0        valid_lft forever preferred_lft forever\cf1\highlight2 
\par \cf0\highlight0 4: veth-ffd05d78@eth-ffd05d78: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/ether ea:00:83:eb:d0:ad brd ff:ff:ff:ff:ff:ff\cf1\highlight2 
\par \cf0\highlight0 5: eth-ffd05d78@veth-ffd05d78: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/ether 9e:04:23:9c:6e:4a brd ff:ff:ff:ff:ff:ff\cf1\highlight2 
\par \cf0\highlight0 6: veth-408301e1@if7: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9000 xdpgeneric/id:49 qdisc noqueue state UP group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/ether ea:d0:0a:40:0f:c0 brd ff:ff:ff:ff:ff:ff link-netnsid 0\cf1\highlight2 
\par \cf0\highlight0     inet6 fe80::e8d0:aff:fe40:fc0/64 scope link\cf1\highlight2 
\par \cf0\highlight0        valid_lft forever preferred_lft forever\cf1\highlight2 
\par \cf0\highlight0 8: veth-hostep@eth-hostep: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9000 xdpgeneric/id:65 qdisc noqueue state UP group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/ether 52:0a:fd:6f:f9:3e brd ff:ff:ff:ff:ff:ff\cf1\highlight2 
\par \cf0\highlight0     inet6 fe80::500a:fdff:fe6f:f93e/64 scope link\cf1\highlight2 
\par \cf0\highlight0        valid_lft forever preferred_lft forever\cf1\highlight2 
\par \cf0\highlight0 9: eth-hostep@veth-hostep: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000\cf1\highlight2 
\par \cf0\highlight0     link/ether 7e:db:39:77:98:dd brd ff:ff:ff:ff:ff:ff\cf1\highlight2 
\par \cf0\highlight0     inet 172.31.19.33/32 scope global eth-hostep\cf1\highlight2 
\par \cf0\highlight0        valid_lft forever preferred_lft forever\cf1\highlight2 
\par \cf0\highlight0     inet6 fe80::7cdb:39ff:fe77:98dd/64 scope link\cf1\highlight2 
\par \cf0\highlight0        valid_lft forever preferred_lft forever\cf1\highlight2 
\par \cf0\highlight0 ubuntu@aws-arktos:~$ hostname -i\cf1\highlight2 
\par \cf0\highlight0 172.31.19.33 172.17.0.1 172.31.19.33 fe80::4c5:e6ff:fe0f:549e fe80::e8d0:aff:fe40:fc0 fe80::500a:fdff:fe6f:f93e fe80::7cdb:39ff:fe77:98dd\cf1\highlight2 
\par \cf0\highlight0 ubuntu@aws-arktos:~$ sudo -i\cf1\highlight2 
\par \cf0\highlight0 root@aws-arktos:~/go/src/k8s.io/arktos# ./cluster/kubectl.sh version\cf1\highlight2 
\par \cf0\highlight0 Client Version: version.Info\{Major:"", Minor:"", GitVersion:"v0.9.0", GitCommit:"$Format:%H$", GitTreeState:"", BuildDate:"2021-12-07T05:45:43Z", GoVersion:"go1.13.9", Compiler:"gc", Platform:"linux/amd64"\}\cf1\highlight2 
\par \cf0\highlight0 Server Version: version.Info\{Major:"", Minor:"", GitVersion:"v0.9.0", GitCommit:"$Format:%H$", GitTreeState:"", BuildDate:"2021-12-07T05:45:43Z", GoVersion:"go1.13.9", Compiler:"gc", Platform:"linux/amd64"\}\cf1\highlight2 
\par \cf0\highlight0 root@aws-arktos:~/go/src/k8s.io/arktos# ./cluster/kubectl.sh version sort\cf1\highlight2 
\par \cf0\highlight0 Client Version: version.Info\{Major:"", Minor:"", GitVersion:"v0.9.0", GitCommit:"$Format:%H$", GitTreeState:"", BuildDate:"2021-12-07T05:45:43Z", GoVersion:"go1.13.9", Compiler:"gc", Platform:"linux/amd64"\}\cf1\highlight2 
\par \cf0\highlight0 Server Version: version.Info\{Major:"", Minor:"", GitVersion:"v0.9.0", GitCommit:"$Format:%H$", GitTreeState:"", BuildDate:"2021-12-07T05:45:43Z", GoVersion:"go1.13.9", Compiler:"gc", Platform:"linux/amd64"\}\cf1\highlight2 
\par \cf0\highlight0 root@aws-arktos:~/go/src/k8s.io/arktos# ./cluster/kubectl.sh version --short\cf1\highlight2 
\par \cf0\highlight0 Client Version: v0.9.0\cf1\highlight2 
\par \cf0\highlight0 Server Version: v0.9.0\cf1\highlight2 
\par \cf0\highlight0 root@aws-arktos:~/go/src/k8s.io/arktos# hostname -i\cf1\highlight2 
\par \cf0\highlight0 172.31.19.33 172.17.0.1 172.31.19.33 fe80::4c5:e6ff:fe0f:549e fe80::e8d0:aff:fe40:fc0 fe80::500a:fdff:fe6f:f93e fe80::7cdb:39ff:fe77:98dd\cf1\highlight2 
\par \cf0\highlight0 root@aws-arktos:~/go/src/k8s.io/arktos# cat /tmp/arktos-network-controller.log\cf1\highlight2 
\par \cf0\highlight0 F1207 09:43:31.868325    5221 network-controller.go:62] --kube-apiserver-ip must be the valid ip address of kube-apiserver.\cf1\highlight2 
\par \cf0\highlight0 root@aws-arktos:~/go/src/k8s.io/arktos# ./cluster/\cf1\highlight2 
\par \cf0\highlight0 addons/                    get-kube-local.sh          kube-util.sh               log-dump/                  test-e2e.sh\cf1\highlight2 
\par \cf0\highlight0 aws/                       get-kube.sh                kubeadm.sh                 pre-existing/              test-network.sh\cf1\highlight2 
\par \cf0\highlight0 clientbin.sh               images/                    kubectl.sh                 proxy-down.sh              test-smoke.sh\cf1\highlight2 
\par \cf0\highlight0 common.sh                  juju/                      kubemark/                  proxy-up.sh                update-storage-objects.sh\cf1\highlight2 
\par \cf0\highlight0 gce/                       kube-down.sh               kubernetes-anywhere/       restore-from-backup.sh     validate-cluster.sh\cf1\highlight2 
\par \cf0\highlight0 get-kube-binaries.sh       kube-up.sh                 lib/                       skeleton/\cf1\highlight2 
\par \cf0\highlight0 root@aws-arktos:~/go/src/k8s.io/arktos# ./cluster/\cf1\highlight2 
\par \cf0\highlight0 addons/                    get-kube-local.sh          kube-util.sh               log-dump/                  test-e2e.sh\cf1\highlight2 
\par \cf0\highlight0 aws/                       get-kube.sh                kubeadm.sh                 pre-existing/              test-network.sh\cf1\highlight2 
\par \cf0\highlight0 clientbin.sh               images/                    kubectl.sh                 proxy-down.sh              test-smoke.sh\cf1\highlight2 
\par \cf0\highlight0 common.sh                  juju/                      kubemark/                  proxy-up.sh                update-storage-objects.sh\cf1\highlight2 
\par \cf0\highlight0 gce/                       kube-down.sh               kubernetes-anywhere/       restore-from-backup.sh     validate-cluster.sh\cf1\highlight2 
\par \cf0\highlight0 get-kube-binaries.sh       kube-up.sh                 lib/                       skeleton/\cf1\highlight2 
\par \cf0\highlight0 root@aws-arktos:~/go/src/k8s.io/arktos# ./cluster/\cf1\highlight2 
\par \cf0\highlight0 addons/                    get-kube-local.sh          kube-util.sh               log-dump/                  test-e2e.sh\cf1\highlight2 
\par \cf0\highlight0 aws/                       get-kube.sh                kubeadm.sh                 pre-existing/              test-network.sh\cf1\highlight2 
\par \cf0\highlight0 clientbin.sh               images/                    kubectl.sh                 proxy-down.sh              test-smoke.sh\cf1\highlight2 
\par \cf0\highlight0 common.sh                  juju/                      kubemark/                  proxy-up.sh                update-storage-objects.sh\cf1\highlight2 
\par \cf0\highlight0 gce/                       kube-down.sh               kubernetes-anywhere/       restore-from-backup.sh     validate-cluster.sh\cf1\highlight2 
\par \cf0\highlight0 get-kube-binaries.sh       kube-up.sh                 lib/                       skeleton/\cf1\highlight2 
\par \cf0\highlight0 root@aws-arktos:~/go/src/k8s.io/arktos# ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS     RESTARTS   AGE     IP             NODE         NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-mrlht                6336367786187412136   1/1     Running    0          9m39s   172.31.19.33   aws-arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-srqzn   4103094056190627943   1/1     Running    0          9m39s   172.31.19.33   aws-arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-5dp84         2024609569699318849   3/3     Running    0          9m39s   20.0.0.22      aws-arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   virtlet-qbjdc                     1793340137653883433   0/3     Init:0/1   0          9m39s   172.31.19.33   aws-arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@aws-arktos:~/go/src/k8s.io/arktos# history\cf1\highlight2 
\par \cf0\highlight0     1  vi /etc/hostname\cf1\highlight2 
\par \cf0\highlight0     2  init 6\cf1\highlight2 
\par \cf0\highlight0     3  uname -a\cf1\highlight2 
\par \cf0\highlight0     4  wget https://raw.githubusercontent.com/CentaurusInfra/mizar/dev-next/kernelupdate.sh\cf1\highlight2 
\par \cf0\highlight0     5  sudo bash kernelupdate.sh\cf1\highlight2 
\par \cf0\highlight0     6  swapoff -a\cf1\highlight2 
\par \cf0\highlight0     7  ufw disable\cf1\highlight2 
\par \cf0\highlight0     8  apt-get update -y\cf1\highlight2 
\par \cf0\highlight0     9  apt-get install build-essential -y\cf1\highlight2 
\par \cf0\highlight0    10  git clone https://github.com/Click2Cloud-Centaurus/arktos.git ~/go/src/k8s.io/arktos\cf1\highlight2 
\par \cf0\highlight0    11  git checkout default-cni-mizar\cf1\highlight2 
\par \cf0\highlight0    12  git branch\cf1\highlight2 
\par \cf0\highlight0    13  ls\cf1\highlight2 
\par \cf0\highlight0    14  git clone https://github.com/Click2Cloud-Centaurus/arktos.git ~/go/src/k8s.io/arktos -b default-cni-mizar\cf1\highlight2 
\par \cf0\highlight0    15  rm -rf /root/go/src/k8s.io/arktos\cf1\highlight2 
\par \cf0\highlight0    16  git clone https://github.com/Click2Cloud-Centaurus/arktos.git ~/go/src/k8s.io/arktos -b default-cni-mizar\cf1\highlight2 
\par \cf0\highlight0    17  git checkout\cf1\highlight2 
\par \cf0\highlight0    18  git branch\cf1\highlight2 
\par \cf0\highlight0    19  sudo bash $HOME/go/src/k8s.io/arktos/hack/setup-dev-node.sh\cf1\highlight2 
\par \cf0\highlight0    20  export PATH=$PATH:/usr/local/go/bin\cf1\highlight2 
\par \cf0\highlight0    21  vi /etc/hostname\cf1\highlight2 
\par \cf0\highlight0    22  echo export PATH=$PATH:/usr/local/go/bin\\ >> ~/.profile\cf1\highlight2 
\par \cf0\highlight0    23  echo cd \\$HOME/go/src/k8s.io/arktos >> ~/.profile\cf1\highlight2 
\par \cf0\highlight0    24  source ~/.profile\cf1\highlight2 
\par \cf0\highlight0    25  systemctl status docker\cf1\highlight2 
\par \cf0\highlight0    26  systemctl statuscontainerd\cf1\highlight2 
\par \cf0\highlight0    27  systemctl status containerd\cf1\highlight2 
\par \cf0\highlight0    28  CNIPLUGIN=mizar ./hack/arktos-up.sh\cf1\highlight2 
\par \cf0\highlight0    29  CNIPLUGIN=mizar ./hack/arktos-up.sh -O\cf1\highlight2 
\par \cf0\highlight0    30  ./cluster/kubectl.sh get nodes\cf1\highlight2 
\par \cf0\highlight0    31  ./cluster/kubectl.sh get nodes -Ao wide\cf1\highlight2 
\par \cf0\highlight0    32  sudo su\cf1\highlight2 
\par \cf0\highlight0    33  ls\cf1\highlight2 
\par \cf0\highlight0    34  history\cf1\highlight2 
\par \cf0\highlight0    35  sudo su\cf1\highlight2 
\par \cf0\highlight0    36  su ubuntu\cf1\highlight2 
\par \cf0\highlight0    37  su ubunut\cf1\highlight2 
\par \cf0\highlight0    38  su ubuntu\cf1\highlight2 
\par \cf0\highlight0    39  ./cluster/kubectl.sh version\cf1\highlight2 
\par \cf0\highlight0    40  ./cluster/kubectl.sh version sort\cf1\highlight2 
\par \cf0\highlight0    41  ./cluster/kubectl.sh version --short\cf1\highlight2 
\par \cf0\highlight0    42  hostname -i\cf1\highlight2 
\par \cf0\highlight0    43  cat /tmp/arktos-network-controller.log\cf1\highlight2 
\par \cf0\highlight0    44  ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0    45  history\cf1\highlight2 
\par \cf0\highlight0 root@aws-arktos:~/go/src/k8s.io/arktos# cat /tmp/arktos-network-controller.log\cf1\highlight2 
\par \cf0\highlight0 F1207 09:43:31.868325    5221 network-controller.go:62] --kube-apiserver-ip must be the valid ip address of kube-apiserver.\cf1\highlight2 
\par \cf0\highlight0 root@aws-arktos:~/go/src/k8s.io/arktos# ./cluster/kubectl.sh apply -f https://raw.githubusercontent.com/Click2Cloud-Centaurus/Documentation/main/test-yamls/test_pods.yaml\cf1\highlight2 
\par \cf0\highlight0 pod/netpod1 created\cf1\highlight2 
\par \cf0\highlight0 pod/netpod2 created\cf1\highlight2 
\par \cf0\highlight0 root@aws-arktos:~/go/src/k8s.io/arktos# ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS              RESTARTS   AGE   IP             NODE         NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-mrlht                6336367786187412136   1/1     Running             0          23m   172.31.19.33   aws-arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-srqzn   4103094056190627943   1/1     Running             0          23m   172.31.19.33   aws-arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod1                           4245008424301658704   0/1     ContainerCreating   0          25s   <none>         aws-arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod2                           7416432928224090983   0/1     ContainerCreating   0          25s   <none>         aws-arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-5dp84         2024609569699318849   3/3     Running             0          23m   20.0.0.22      aws-arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   virtlet-qbjdc                     1793340137653883433   0/3     Init:0/1            0          23m   172.31.19.33   aws-arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@aws-arktos:~/go/src/k8s.io/arktos# ./cluster/kubectl.sh delete pod virtlet-qbjdc\cf1\highlight2 
\par \cf0\highlight0 Error from server (NotFound): pods "virtlet-qbjdc" not found\cf1\highlight2 
\par \cf0\highlight0 root@aws-arktos:~/go/src/k8s.io/arktos# ./cluster/kubectl.sh delete pod virtlet-qbjdc -n kube-system\cf1\highlight2 
\par \cf0\highlight0 pod "virtlet-qbjdc" deleted\cf1\highlight2 
\par \cf0\highlight0 root@aws-arktos:~/go/src/k8s.io/arktos# ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0 NAMESPACE     NAME                              HASHKEY               READY   STATUS              RESTARTS   AGE   IP             NODE         NOMINATED NODE   READINESS GATES\cf1\highlight2 
\par \cf0\highlight0 default       mizar-daemon-mrlht                6336367786187412136   1/1     Running             0          24m   172.31.19.33   aws-arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       mizar-operator-6b78d7ffc4-srqzn   4103094056190627943   1/1     Running             0          24m   172.31.19.33   aws-arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod1                           4245008424301658704   0/1     ContainerCreating   0          80s   <none>         aws-arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 default       netpod2                           7416432928224090983   0/1     ContainerCreating   0          80s   <none>         aws-arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 kube-system   kube-dns-554c5866fc-5dp84         2024609569699318849   3/3     Running             0          24m   20.0.0.22      aws-arktos   <none>           <none>\cf1\highlight2 
\par \cf0\highlight0 root@aws-arktos:~/go/src/k8s.io/arktos# history\cf1\highlight2 
\par \cf0\highlight0     1  vi /etc/hostname\cf1\highlight2 
\par \cf0\highlight0     2  init 6\cf1\highlight2 
\par \cf0\highlight0     3  uname -a\cf1\highlight2 
\par \cf0\highlight0     4  wget https://raw.githubusercontent.com/CentaurusInfra/mizar/dev-next/kernelupdate.sh\cf1\highlight2 
\par \cf0\highlight0     5  sudo bash kernelupdate.sh\cf1\highlight2 
\par \cf0\highlight0     6  swapoff -a\cf1\highlight2 
\par \cf0\highlight0     7  ufw disable\cf1\highlight2 
\par \cf0\highlight0     8  apt-get update -y\cf1\highlight2 
\par \cf0\highlight0     9  apt-get install build-essential -y\cf1\highlight2 
\par \cf0\highlight0    10  git clone https://github.com/Click2Cloud-Centaurus/arktos.git ~/go/src/k8s.io/arktos\cf1\highlight2 
\par \cf0\highlight0    11  git checkout default-cni-mizar\cf1\highlight2 
\par \cf0\highlight0    12  git branch\cf1\highlight2 
\par \cf0\highlight0    13  ls\cf1\highlight2 
\par \cf0\highlight0    14  git clone https://github.com/Click2Cloud-Centaurus/arktos.git ~/go/src/k8s.io/arktos -b default-cni-mizar\cf1\highlight2 
\par \cf0\highlight0    15  rm -rf /root/go/src/k8s.io/arktos\cf1\highlight2 
\par \cf0\highlight0    16  git clone https://github.com/Click2Cloud-Centaurus/arktos.git ~/go/src/k8s.io/arktos -b default-cni-mizar\cf1\highlight2 
\par \cf0\highlight0    17  git checkout\cf1\highlight2 
\par \cf0\highlight0    18  git branch\cf1\highlight2 
\par \cf0\highlight0    19  sudo bash $HOME/go/src/k8s.io/arktos/hack/setup-dev-node.sh\cf1\highlight2 
\par \cf0\highlight0    20  export PATH=$PATH:/usr/local/go/bin\cf1\highlight2 
\par \cf0\highlight0    21  vi /etc/hostname\cf1\highlight2 
\par \cf0\highlight0    22  echo export PATH=$PATH:/usr/local/go/bin\\ >> ~/.profile\cf1\highlight2 
\par \cf0\highlight0    23  echo cd \\$HOME/go/src/k8s.io/arktos >> ~/.profile\cf1\highlight2 
\par \cf0\highlight0    24  source ~/.profile\cf1\highlight2 
\par \cf0\highlight0    25  systemctl status docker\cf1\highlight2 
\par \cf0\highlight0    26  systemctl statuscontainerd\cf1\highlight2 
\par \cf0\highlight0    27  systemctl status containerd\cf1\highlight2 
\par \cf0\highlight0    28  CNIPLUGIN=mizar ./hack/arktos-up.sh\cf1\highlight2 
\par \cf0\highlight0    29  CNIPLUGIN=mizar ./hack/arktos-up.sh -O\cf1\highlight2 
\par \cf0\highlight0    30  ./cluster/kubectl.sh get nodes\cf1\highlight2 
\par \cf0\highlight0    31  ./cluster/kubectl.sh get nodes -Ao wide\cf1\highlight2 
\par \cf0\highlight0    32  sudo su\cf1\highlight2 
\par \cf0\highlight0    33  ls\cf1\highlight2 
\par \cf0\highlight0    34  history\cf1\highlight2 
\par \cf0\highlight0    35  sudo su\cf1\highlight2 
\par \cf0\highlight0    36  su ubuntu\cf1\highlight2 
\par \cf0\highlight0    37  su ubunut\cf1\highlight2 
\par \cf0\highlight0    38  su ubuntu\cf1\highlight2 
\par \cf0\highlight0    39  ./cluster/kubectl.sh version\cf1\highlight2 
\par \cf0\highlight0    40  ./cluster/kubectl.sh version sort\cf1\highlight2 
\par \cf0\highlight0    41  ./cluster/kubectl.sh version --short\cf1\highlight2 
\par \cf0\highlight0    42  hostname -i\cf1\highlight2 
\par \cf0\highlight0    43  cat /tmp/arktos-network-controller.log\cf1\highlight2 
\par \cf0\highlight0    44  ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0    45  history\cf1\highlight2 
\par \cf0\highlight0    46  cat /tmp/arktos-network-controller.log\cf1\highlight2 
\par \cf0\highlight0    47  ./cluster/kubectl.sh apply -f https://raw.githubusercontent.com/Click2Cloud-Centaurus/Documentation/main/test-yamls/test_pods.yaml\cf1\highlight2 
\par \cf0\highlight0    48  ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0    49  ./cluster/kubectl.sh delete pod virtlet-qbjdc\cf1\highlight2 
\par \cf0\highlight0    50  ./cluster/kubectl.sh delete pod virtlet-qbjdc -n kube-system\cf1\highlight2 
\par \cf0\highlight0    51  ./cluster/kubectl.sh get pods -Ao wide\cf1\highlight2 
\par \cf0\highlight0    52  history\cf1\highlight2 
\par \cf0\highlight0 root@aws-arktos:~/go/src/k8s.io/arktos# hostname -i\cf1\highlight2 
\par \cf0\highlight0 172.31.19.33 172.17.0.1 172.31.19.33 fe80::4c5:e6ff:fe0f:549e fe80::e8d0:aff:fe40:fc0 fe80::500a:fdff:fe6f:f93e fe80::7cdb:39ff:fe77:98dd\cf1\highlight2 
\par \cf0\highlight0 root@aws-arktos:~/go/src/k8s.io/arktos#\cf1\highlight2 
\par \pard\cf0\highlight0\f2 
\par }
 